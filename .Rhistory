fix_border_issues(part = "body")
## ================= ##
## ASSUMPTION CHECKS ##
## ================= ##
# (1) Outliers → Checked via a Box Q-Q Plot of BART pump scores with those +/-3SD marked. This allowed further analysis of any identified scores as to whether they were data-entry issues or just sample variance
# ~ ~ ~ #
# TABOO #
# ~ ~ ~ #
# Create a dataframe of outliers based on their offensiveness scores for the taboo condition only - this reuses the taboo outliers dataframe created in the bart assumptions
offense.outliers.t.label <- df.outliers.taboo %>%
group_by(Session.Type) %>%
mutate(outlier = abs(Offense - median(Offense)) > 2 * IQR(Offense)) %>%
filter(outlier)
# Plot and label the outliers on a boxplot for the taboo offensiveness scores
# Note: The colour scheme for the sessions matches the same colour scheme used in the counterbalancing spreadsheet used during data collection
offense.outliers <- ggplot(df.outliers.taboo, aes(Session.Type, Offense, fill=Session.Type)) +
#scale_fill_manual(name="Session Type", values = alpha(c("skyblue1", "palegreen1", "salmon1"),0.5), guide= "none") +
stat_boxplot(geom = "errorbar", width = 0.15, linetype="solid") +
geom_boxplot(notch=F, notchwidth = 0.8, linetype = "solid", outlier.colour="#000000",
outlier.size=2, outlier.shape=1) +
geom_boxplot(notch=F, notchwidth = 0.95, outlier.shape = NA, coef = 0) +
scale_y_continuous(limits = c(1,9), expand = c(0, 0), breaks = seq(1,9,by = 1)) +
geom_text(data=offense.outliers.t.label, aes(label=PPT.ID), hjust = -0.15, vjust = 0.1) +
theme_classic()
# ~ ~ ~ ~ ~ #
# NON-TABOO #
# ~ ~ ~ ~ ~ #
# Create a dataframe of outliers based on their offensiveness scores for the non-taboo condition only - this reuses the non-taboo outliers dataframe created in the bart assumptions
offense.outliers.nt.label <- df.outliers.nt %>%
group_by(Session.Type) %>%
mutate(outlier = abs(Offense - median(Offense)) > 2 * IQR(Offense)) %>%
filter(outlier)
# Plot and label the outliers on a boxplot for the taboo offensiveness scores
# Note: The colour scheme for the sessions matches the same colour scheme used in the counterbalancing spreadsheet used during data collection
offense.outliers.nt <- ggplot(df.outliers.nt, aes(Session.Type, Offense, fill=Session.Type)) +
#scale_fill_manual(name="Session Type", values = alpha(c("skyblue1", "palegreen1", "salmon1"),0.5), guide= "none") +
stat_boxplot(geom = "errorbar", width = 0.15, linetype="solid") +
geom_boxplot(notch=F, notchwidth = 0.8, linetype = "solid", outlier.colour="#000000",
outlier.size=2, outlier.shape=1) +
geom_boxplot(notch=F, notchwidth = 0.95, outlier.shape = NA, coef = 0) +
scale_y_continuous(limits = c(1,6), expand = c(0, 0), breaks = seq(1,6, by = 1)) +
geom_text(data=offense.outliers.nt.label, aes(label=PPT.ID), hjust = -0.15, vjust = 0.1) +
theme_classic()
# ==================== #
# NORMALITY & VARIANCE #
# ==================== #
# Create an anova model per session type to assess normality and variance assumptions
anova.model.offense <- df.final.offense_clean %>% group_by(Word) %>% do(model = lm(Offense ~ Session.Type, data = .))
# (2) Homogeneity of Variance (Leven's Test → wants to be non-sig)
leveneTest(anova.model.offense[[2]][[1]]) # Non-Taboo
leveneTest(anova.model.offense[[2]][[2]]) # Taboo
# (3) Normal Distribution (Shaprio Wilk → wants to be non-sig)
# Get residuals from ANOVA model
NT.Offense.residuals <- residuals(object = anova.model.offense[[2]][[1]]) # Non-Taboo
T.Offense.residuals <- residuals(object = anova.model.offense[[2]][[2]])  # Taboo
# Check normality via Shapiro Wilk tests for the residuals of each word type
shapiro.test(x = NT.Offense.residuals) # Non-Taboo
shapiro.test(x = T.Offense.residuals) # Taboo
# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  #
# CLT applies and Donaldson (1968) says ANOVA is robust to non-normal  #
# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  #
# (4) Mauchley's test of Sphericity needed as there are three levels to session type → needs to be greater than 0.05
mauch.anova.offense <- anova_test(data = df.final.offense_clean, dv = Offense, wid = PPT.ID, within = c(Word, Session.Type), between = Order)
mauch.anova.offense$`Mauchly's Test for Sphericity`
# (5) Independence
# Note: this is naturally assumed based on the fact testing sessions were a minimum of 24 hours apart, and each VAS scale was reset to a mid point before participants made a rating, and the exact value of each rating previously made was not displayed to participants.
# (6) Interval Data
# Met as Offensiveness was measured via an integer and thus, interval data
## ====================== ##
## 2x2 ORDER EFFECT ANOVA ##
## ====================== ##
# I think R would do this automaticall,y but just to be safe, I'm changing the PPT ID to be a factor.
df.final.offense_clean$PPT.ID <- as.factor(df.final.offense_clean$PPT.ID)
# Run ANOVA - this is a repeated measures anova hence the need for Error()
Offense.Anova <- df.final.offense_clean %>% group_by(Session.Type) %>% do(model = aov(Offense ~ Word * Order + Error(PPT.ID/(Word)), data = .))
# -------------
# Output ANOVAs
# -------------
# Supervised
offense.anova.order.rs <- summary(Offense.Anova[[2]][[1]])
offense.anova.order.rs
# Text
offense.anova.order.ti <- summary(Offense.Anova[[2]][[2]])
offense.anova.order.ti
# Video
offense.anova.order.vi <- summary(Offense.Anova[[2]][[3]])
offense.anova.order.vi
# ----------------------------------------------------------------------------------------------
# Calculate Effect Size (based on Lakens [https://bit.ly/3vxfktI] -> eta2 = F * dfn/[F*dfn+dfd])
# ----------------------------------------------------------------------------------------------
# Store F statistics
# Order
offense.order.rs.f <- offense.anova.order.rs$`Error: PPT.ID:Word`[[1]][[4]][[2]]
offense.order.ti.f <- offense.anova.order.ti$`Error: PPT.ID:Word`[[1]][[4]][[2]]
offense.order.vi.f <- offense.anova.order.vi$`Error: PPT.ID:Word`[[1]][[4]][[2]]
# Word
offense.word.rs.f <- offense.anova.order.rs$`Error: PPT.ID:Word`[[1]][[4]][[1]]
offense.word.ti.f <- offense.anova.order.ti$`Error: PPT.ID:Word`[[1]][[4]][[1]]
offense.word.vi.f <- offense.anova.order.vi$`Error: PPT.ID:Word`[[1]][[4]][[1]]
# Store Degrees of freedom
# Order
offense.order.rs.dfn <- offense.anova.order.rs$`Error: PPT.ID:Word`[[1]][[1]][[2]]
offense.order.ti.dfn <- offense.anova.order.ti$`Error: PPT.ID:Word`[[1]][[1]][[2]]
offense.order.vi.dfn <- offense.anova.order.vi$`Error: PPT.ID:Word`[[1]][[1]][[2]]
# Word
offense.word.rs.dfn <- offense.anova.order.rs$`Error: PPT.ID:Word`[[1]][[1]][[1]]
offense.word.ti.dfn <- offense.anova.order.ti$`Error: PPT.ID:Word`[[1]][[1]][[1]]
offense.word.vi.dfn <- offense.anova.order.vi$`Error: PPT.ID:Word`[[1]][[1]][[1]]
# Residuals
offense.anova.rs.dfd <- offense.anova.order.rs$`Error: PPT.ID:Word`[[1]][[1]][[3]]
offense.anova.ti.dfd <- offense.anova.order.ti$`Error: PPT.ID:Word`[[1]][[1]][[3]]
offense.anova.vi.dfd <- offense.anova.order.vi$`Error: PPT.ID:Word`[[1]][[1]][[3]]
# ~ ~ ~
# Order
# ~ ~ ~
# Supervised
offense.order.rs.eta <- offense.order.rs.f * offense.order.rs.dfn/(offense.order.rs.f * offense.order.rs.dfn + offense.anova.rs.dfd)
offense.order.rs.eta
# Text
offense.order.ti.eta <- offense.order.ti.f * offense.order.ti.dfn/(offense.order.ti.f * offense.order.ti.dfn + offense.anova.ti.dfd)
offense.order.ti.eta
# Video
offense.order.vi.eta <- offense.order.vi.f * offense.order.vi.dfn/(offense.order.vi.f * offense.order.vi.dfn + offense.anova.vi.dfd)
offense.order.vi.eta
# ~ ~ ~
# Word
# ~ ~ ~
# Supervised
offense.word.rs.eta <- offense.word.rs.f * offense.word.rs.dfn/(offense.word.rs.f * offense.word.rs.dfn + offense.anova.rs.dfd)
offense.word.rs.eta
# Text
offense.word.ti.eta <- offense.word.ti.f * offense.word.ti.dfn/(offense.word.ti.f * offense.word.ti.dfn + offense.anova.ti.dfd)
offense.word.ti.eta
# Video
offense.word.vi.eta <- offense.word.vi.f * offense.word.vi.dfn/(offense.word.vi.f * offense.word.vi.dfn + offense.anova.vi.dfd)
offense.word.vi.eta
## ========================== ##
## 1x3 BETWEEN SESSION ANCOVA ##
## ========================== ##
# Create data frame
df.ancova <- data.frame()
# Insert data from the cleaned dataframe we set up ealier
df.ancova <- cbind.data.frame(df.final.offense_clean$PPT.ID[df.final.offense_clean$Word=="Taboo"],
df.final.offense_clean$Offense[df.final.offense_clean$Word=="Taboo"],
df.final.offense_clean$Order[df.final.offense_clean$Word=="Taboo"],
df.final.offense_clean$Session.Type[df.final.offense_clean$Word=="Taboo"],
df.final.offense_clean$Offense[df.final.offense_clean$Word=="Non-Taboo"])
# Rename header names
names(df.ancova) <- c("PPT.ID", "Offense", "Order", "Session.Type", "Flat")
# Create the ANCOA, this is repeated measures in which we say that we want to factor in ppt's score for Flat, and their vocalisation order, with the Error fixed for PPT.ID within which we're manipulating the Session ID (i.e., for each PPT.ID there are difference sessiosn.type)
Offense.Ancova <- summary(aov(Offense ~ Session.Type + Order + Flat + Error(PPT.ID/(Session.Type)), data = df.ancova))
# Output the ANCOVA
Offense.Ancova
# Store F statistic
offense.session.out.f <- Offense.Ancova$`Error: PPT.ID:Session.Type`[[1]][[4]][[1]]
# Store Degrees of freedom
offense.session.out.dfn <- Offense.Ancova$`Error: PPT.ID:Session.Type`[[1]][[1]][[1]]
offense.session.out.dfd <- Offense.Ancova$`Error: PPT.ID:Session.Type`[[1]][[1]][[4]]
# Calculate Effect Size (based on Lakens [https://bit.ly/3vxfktI] -> eta2 = F * dfn/[F*dfn+dfd])
offense.session.eta <- offense.session.out.f * offense.session.out.dfn/(offense.session.out.f * offense.session.out.dfn + offense.session.out.dfd)
offense.session.eta
## -------------- ##
## POST HOC TESTS ##
## -------------- ##
# No need as no significant Word * Session interaction :-(
Offense.Anova
Offense.Anova$Session.Type
Offense.Anova$model
offense.anova.order.rs
library(eulerr)
install.packages(eulerr)
install.packages("eulerr")
install.packages("pwr")
install.packages("plotly")
options(scipen = 999)
library(ggplot2)
# Set a seed number so that the randomisation won't change with each update of the textbook (if this isn't done, then there is a .95^144 chance that the below example will not work)
set.seed(1)
participants_per_condition = 20
heights = 10:21 * 7.5
shoe_sizes = 4:15
maths_scores <- data.frame(
height =rep(heights, each = length(heights)* participants_per_condition),
shoe = rep(shoe_sizes, length(shoe_sizes) * participants_per_condition),
maths  = rnorm(participants_per_condition * length(shoe_sizes) * length(heights), mean = 75, sd = 15)
)
## Analyses
library(tidyverse)
maths_scores %>%
group_by(height, shoe) %>%
summarise(
score = mean(maths),
sd   = sd(maths),
se   = sd(maths)/length(maths),
sig  = "no",
color="red"
) -> maths_summarised
for(height in heights){
for(shoe_size in shoe_sizes){
exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]
con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]
this_t.test <- t.test(exp_group_scores, con_group_scores)
if(this_t.test$p.value < .05){
maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = "yes"
maths_summarised$color[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = "blue"
}
}
}
# setting yes and no colors
yes_no_colors <- c("red", "#005599")
yes_no_colors <- setNames(yes_no_colors, c("no", "yes"))
#| label: fig-fwer-uncorrected-prop-false-pos
#| fig-cap: Visualisation of an analysis on whether height and shoe size predict mathematic scores. Note that all data is randomly generated. The participants have been grouped based on heights and show sizes and then compared to all other participants using t-tests to see if their maths score is significantly higher or lower than all other participants. Groups represented by the blue dots performed significantly worse or better when compared to all other participants. Groups in red dots did not perform significantly better or worse compared to all other participants. Note that some higher groups did not perform significantly better than all others because they had a wider range of scores within their group, thus increasing their standard error.
library(plotly)
plot_ly(
data = maths_summarised,
x=~height,
y=~shoe,
z=~score,
color=~sig,
error_z = list(array=~se),
type = "scatter3d",
mode = "markers",
size = 1,
colors = yes_no_colors
)
# reset the "sig" column
maths_summarised$sig = "no"
comparisons = length(heights) * length(shoe_size)
for(height in heights){
for(shoe_size in shoe_sizes){
exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]
con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]
this_t.test <- t.test(exp_group_scores, con_group_scores)
# here is where the new alpha threshold is applied
if(this_t.test$p.value < .05/comparisons){
maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = "yes"
}
}
}
#| label: fig-fwer-bonferroni-corrected-prop-false-pos
#| fig-cap: Same visualisation as above, but this time the alpha threshold has a bonferroni correction.
plot_ly(
data = maths_summarised,
x=~height,
y=~shoe,
z=~score,
color=~sig,
error_z = list(array=~se),
type = "scatter3d",
mode = "markers",
size = 1,
colors = yes_no_colors
)
# reset the "sig" column
maths_summarised$sig = "no"
sidak_alpha =  1 - (1 - .05)^(1/144)
comparisons = length(heights) * length(shoe_size)
for(height in heights){
for(shoe_size in shoe_sizes){
exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]
con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]
this_t.test <- t.test(exp_group_scores, con_group_scores)
# here is where the new alpha threshold is applied
if(this_t.test$p.value < sidak_alpha){
maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = "yes"
}
}
}
#| label: fig-fwer-Šidák-corrected-prop-false-pos
#| fig-cap: Same visualisation as above, but this time the alpha threshold has a Šidák correction.
plot_ly(
data = maths_summarised,
x=~height,
y=~shoe,
z=~score,
color=~sig,
error_z = list(array=~se),
type = "scatter3d",
mode = "markers",
size = 1,
colors = yes_no_colors
)
bonferroni_comb <- data.frame(
description = c(
"Both tests are false-positives",
"Only the first test is a false-positive",
"Only the second test is a false-positive",
"Neither test is a false-positive"
),
first.test = c(.025,.025,.975,.975),
second.test = c(.025,.975,.025,.975)
)
bonferroni_comb$likelihood = bonferroni_comb$first.test * bonferroni_comb$second.test
knitr::kable(bonferroni_comb)
sum(bonferroni_comb$likelihood)
ggplot() +
geom_vline(xintercept = 1) +
geom_hline(yintercept = 1) +
xlim(0,1) +
ylim(0,1) +
xlab("first result p-value") +
ylab("second result p-value") +
geom_rect(
mapping = aes(
xmin = 0,
xmax = .05,
ymin = 0,
ymax = .05
),
fill = "red",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0,
xmax = .05,
ymin = .05,
ymax = 1
),
fill = "green",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0.05,
xmax = 1,
ymin = 0,
ymax = .05
),
fill = "blue",
alpha = .5
)   +
geom_rect(
mapping = aes(
xmin = .05,
xmax = 1,
ymin = 0.05,
ymax = 1
),
fill = "orange",
alpha = .5
)
bonferroni_holm_comb <- data.frame(
description = c(
"Both are false positives, but only the first test is <.025",
"Both are false positives, but only the second test is <.025",
"Both are false positives and both < .025",
"Only the first test is a false-positive",
"Only the second test is a false-positive",
"Neither test is a false-positive"
),
first.test = c(
.025, # likelihood of 0 to .025 as it is the ONLY test that it < .025
.025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025
.025, # likelihood of 0 to .025 as both tests are < .025
.025, # likelihood of 0 to .025 as it is the ONLY false-positive
.95,  # likelihood of 0 to .95 as the other test is significant
.975  # likelihood of 0 to .975 as the other test is not significant
),
second.test = c(
.025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025
.025, # likelihood of 0 to .025 as it is the ONLY test that it < .025
.025, # likelihood of 0 to .025 as both tests are <.025
.95,  # likelihood of 0 to .025 as it is the ONLY false-positive
.025, # likelihood of 0 to .95 as the other test is significant
.975  # likelihood of 0 to .975 as the other test is not significant
)
)
bonferroni_holm_comb$likelihood = bonferroni_holm_comb$first.test * bonferroni_holm_comb$second.test
# knitr::kable(bonferroni_holm_comb)
sum(bonferroni_holm_comb$likelihood)
ggplot() +
geom_vline(xintercept = 1) +
geom_hline(yintercept = 1) +
xlim(0,1) +
ylim(0,1) +
xlab("First result p-value") +
ylab("Second result p-value") +
geom_rect(
mapping = aes(
xmin = .0,
xmax = .025,
ymin = 0,
ymax = .025
),
fill = "blue",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0,
xmax = .025,
ymin = .025,
ymax = .05
),
fill = "red",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = .025,
xmax = .05,
ymin = 0,
ymax = .025
),
fill = "green",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0.05,
xmax = 1,
ymin = 0,
ymax = .025
),
fill = "purple",
alpha = .5
)   +
geom_rect(
mapping = aes(
xmin = 0.0,
xmax = .025,
ymin = 0.05,
ymax = 1
),
fill = "brown",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0.025,
xmax = 1,
ymin = 0.025,
ymax = 1
),
fill = "black",
alpha = .5
)
ggplot() +
geom_vline(xintercept = 1) +
geom_hline(yintercept = 1) +
xlim(0,1) +
ylim(0,1) +
xlab("First result p-value") +
ylab("Second result p-value") +
geom_rect(
mapping = aes(
xmin = .0,
xmax = .025,
ymin = 0,
ymax = .025
),
fill = "blue",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0,
xmax = .025,
ymin = .025,
ymax = .05
),
fill = "red",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = .025,
xmax = .05,
ymin = 0,
ymax = .025
),
fill = "green",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0.05,
xmax = 1,
ymin = 0,
ymax = .025
),
fill = "purple",
alpha = .5
)   +
geom_rect(
mapping = aes(
xmin = 0.0,
xmax = .025,
ymin = 0.05,
ymax = 1
),
fill = "brown",
alpha = .5
) +
geom_rect(
mapping = aes(
xmin = 0.025,
xmax = 1,
ymin = 0.025,
ymax = 1
),
fill = "black",
alpha = .5
)
install.packages(c("askpass", "credentials", "DescTools", "deSolve", "dplyr", "Matrix", "MatrixModels", "rematch", "StanHeaders", "yulab.utils"))
install.packages(c("DescTools", "deSolve", "Matrix", "MatrixModels", "StanHeaders"))
install.packages(c("DescTools", "deSolve", "Matrix", "MatrixModels", "StanHeaders"))
>>>>>>> Stashed changes
library(plotly)
detach("package:plotly", unload = TRUE)
