cor.test(summary_data$sociality, summary_data$aq)
plot(summary_data$aq, summary_data$sociality)
plot(lm(sociality ~ aq, summary_data))
plot(summary_data$aq, summary_data$sociality)
plot(summary_data[summary_data$aq<31,]$aq, summary_data$sociality[summary_data$aq<31,])
plot(summary_data[summary_data$aq<31]$aq, summary_data$sociality[summary_data$aq<31])
plot(summary_data$aq[summary_data$aq<31], summary_data$sociality[summary_data$aq<31])
cor.test(summary_data$aq[summary_data$aq<31], summary_data$sociality[summary_data$aq<31])
plot(summary_data$aq[summary_data$aq<31], summary_data$sociality[summary_data$aq<31])
View(summary_data)
summary_data$RFug_sociality = summary_data$Soc.RFuG.con.10-summary_data$Non.RFuG.con.10
summary(lm(RFug_sociality ~ aq * sociality, data = summary_data))
summary_data$mpfc_sociality = summary_data$Soc.mPFC50.HarvOx.con-summary_data$Non.mPFC50.HarvOx.con
summary(lm(mpfc_sociality ~ aq * sociality, data = summary_data))
summary_data$RFug_sociality = summary_data$Soc.RFuG.con.10-summary_data$Non.RFuG.con.10
summary_data$RFug_sociality = summary_data$Soc.RFuG.con.10-summary_data$Non.RFuG.con.10
summary_data$mpfc_sociality = summary_data$Soc.mPFC50.HarvOx.con-summary_data$Non.mPFC50.HarvOx.con
summary_data$lamy_sociality = summary_data$Soc.LAmy50.HarvOx.con - summary_data$Non.RAmy50.HarvOx.con
summary_data$ramy_sociality = summary_data$Soc.RAmy50.HarvOx.con-summary_data$Non.RAmy50.HarvOx.con
summary_data$lvs_sociality = summary_data$Soc.LVS50.HarvOx.con-summary_data$Non.LVS50.HarvOx.con
summary_data$rvs_sociality = summary_data$Soc.RVS50.HarvOx.con-summary_data$Non.RVS50.HarvOx.con
summary(lm(RFug_sociality ~ aq * sociality, data = summary_data))
summary(lm(mpfc_sociality ~ aq * sociality, data = summary_data))
summary(lm(lamy_sociality ~ aq * sociality, data = summary_data))
summary(lm(ramy_sociality ~ aq * sociality, data = summary_data))
summary(lm(lvs_sociality ~ aq * sociality, data = summary_data))
summary(lm(rvs_sociality ~ aq * sociality, data = summary_data))
cor.test(summary_data$sociality, summary_data$aq)
cor.test(summary_data$sociality, summary_data$aq)
cor.test(summary_data_review$sociality, summary_data_review$aq)
summary_data_review$sociality = summary_data_review$social- summary_data_review$nonsocial
cor.test(summary_data$sociality, summary_data$aq)
cor.test(summary_data_review$sociality, summary_data_review$aq)
aov(valence ~ sociality * aq + Error(ID/times))
aov(valence ~ sociality * aq + Error(ID/times), data = summary_data)
aov(valence ~ sociality * aq + Error(ID/times), data = moderation_df)
aov(valence ~ sociality * aq + Error(ID/sociality), data = moderation_df)
summary(aov(valence ~ sociality * aq + Error(ID/sociality), data = moderation_df))
summary(aov(valence ~ sociality * aq + Error(ID/sociality), data = moderation_df))
cor.test(summary_data$aq, summary_data$sociality)
?boot
boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
library("boot")
boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
this_boot = boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
this_boot$t0
library("boot")
this_boot = boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
this_boot$t0
library("boot")
this_boot = boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
this_boot$t0
library("boot")
this_boot = boot(
summary_data,
statistic = function(summary_data, i) {
cor(summary_data[i, "aq"], summary_data[i, "sociality"], method='pearson')
},
R = 1000
)
this_boot$t0
this_boot
?sample
this_sample = sample(summary_data)
this_sample = sample(summary_data, 10)
this_sample = sample(summary_data, [10,])
this_sample = sample(summary_data)
this_sample = sample_n(summary_data,18)
cor.test(this_sample$aq, this_sample$social)$estimate
social_r_value_vector = rep(NA,1000)
nonsocial_r_value_vector = rep(NA,1000)
for(i in 1:1000){
this_sample = sample_n(summary_data,18)
social_r_value_vector[i] = cor.test(this_sample$aq, this_sample$social)$estimate
nonsocial_r_value_vector[i] = cor.test(this_sample$aq, this_sample$nonsocial)$estimate
}
social_r_value_vector
mean(social_r_value_vector)
sd(social_r_value_vector)
mean(nonsocial_r_value_vector)
sd(nonsocial_r_value_vector)
mean(social_r_value_vector)
sd(social_r_value_vector)
mean(nonsocial_r_value_vector)
sd(nonsocial_r_value_vector)
mean(social_r_value_vector<0)
mean(nonsocial_r_value_vector<0)
social_r_value_vector = rep(NA,1000)
nonsocial_r_value_vector = rep(NA,1000)
sociality_r_value_vector = rep(NA,1000)
for(i in 1:1000){
this_sample = sample_n(summary_data,18)
social_r_value_vector[i] = cor.test(this_sample$aq, this_sample$social)$estimate
nonsocial_r_value_vector[i] = cor.test(this_sample$aq, this_sample$nonsocial)$estimate
sociality_r_value_vector[i] = cor.test(this_sample$aq, this_sample$sociality)$estimate
}
mean(social_r_value_vector<0)
mean(nonsocial_r_value_vector<0)
mean(sociality_r_value_vector<0)
mean(sociality_r_value_vector)
sd(sociality_r_value_vector)
cor.test(summary_data$aq[summary_data$aq<30], summary_data$sociality[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<31], summary_data$sociality[summary_data$aq<31])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$social[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$nonsocial[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$social[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$nonsocial[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$sociality[summary_data$aq<30])
cor.test(summary_data$aq[summary_data$aq<30], summary_data$nonsocial[summary_data$aq<30])
cocor::cocor(
~aq + social | aq + nonsocial  ,
summary_data[summary_data$aq<30],
test = "steiger1980",
return.htest = F,
alternative = "less",
)
cocor::cocor(
~aq + social | aq + nonsocial  ,
summary_data[summary_data$aq<30,],
test = "steiger1980",
return.htest = F,
alternative = "less",
)
mean(summary_data$sociality) - 3 * sd(summary_data$sociality)
mean(summary_data$sociality) + 3 * sd(summary_data$sociality)
max(summary_data$sociality)
min(summary_data$sociality)
mean(summary_data$sociality) - 3 * sd(summary_data$sociality)
mean(summary_data$sociality) + 3 * sd(summary_data$sociality)
max(summary_data$sociality)
min(summary_data$sociality)
cor.test(summary_data$aq[summary_data$aq<30], summary_data$sociality[summary_data$aq<30])
cor.test(summary_data_review$sociality, summary_data_review$aq)
summary(aov(valence ~ sociality * aq + Error(ID/sociality), data = moderation_df))
cor.test(summary_data$aq, summary_data$sociality)
mean(social_r_value_vector<0)
mean(social_r_value_vector<-.1)
mean(social_r_value_vector< 0)
social_r_value_vector = rep(NA,1000)
nonsocial_r_value_vector = rep(NA,1000)
sociality_r_value_vector = rep(NA,1000)
for(i in 1:1000){
this_sample = sample_n(summary_data,18)
social_r_value_vector[i] = cor.test(this_sample$aq, this_sample$social)$estimate
nonsocial_r_value_vector[i] = cor.test(this_sample$aq, this_sample$nonsocial)$estimate
sociality_r_value_vector[i] = cor.test(this_sample$aq, this_sample$sociality)$estimate
}
mean(social_r_value_vector< 0)
mean(social_r_value_vector < -.1)
mean(social_r_value_vector < -.2)
mean(social_r_value_vector < -.3)
mean(social_r_value_vector < -.4)
mean(social_r_value_vector < -.5)
aq.valence.plot = ggplot(aq.valence.plot.data, aes(x=AQ, y=Valence, color=Sociality, shape=Sociality)) +
geom_point(size = 5) +
geom_smooth(
formula = "y~x",
method=lm,
se=TRUE,
fullrange=TRUE,
linewidth = 2
)+
theme_classic() +
theme(text = element_text(size=20)) +
xlab("Autism Quotient") +
ylab("Valence Rating")
# Note brain regions variables are labelled as follows:
# - rFuG is right fusiform gyrus,
# - mPFC is medial orbitofrontal cortex
# - Acc  is ventral striatum (e.g. lAcc is left ventral striatum)
# - Amy  is amygdala (e.g. lAmy is left amygdala)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
aq_df = read.csv("aq_age.csv")
# 0 = Strongly Agree
# 1 = Agree
# 2 = Disagree
# 3 = Strongly Disagree
aq_key = data.frame(
score = c(
"reverse", # "I prefer to do things with others rather than on my own."
"normal",  # "I prefer to do things the same way over and over again."
"reverse", # "If I try to imagine something, I find it very easy to create a picture in my mind."
"normal",  # "I frequently get so strongly absorbed in one thing that I lose sight of other things."
"normal",  # "I often notice small sounds when others do not.",
"normal",  # "I usually notice car number plates or similar strings of information.",
"normal",  # "Other people frequently tell me that what I’ve said is impolite, even though I think it is polite."
"reverse", # "When I’m reading a story, I can easily imagine what the characters might look like."
"normal",  # "I am fascinated by dates."
"reverse", # "In a social group, I can easily keep track of several different people’s conversations."
"reverse", # "I find social situations easy."
"normal",  # "I tend to notice details that others do not.",
"normal",  # "I would rather go to a library than a party.",
"reverse", # "I find making up stories easy.",
"reverse", # "I find myself drawn more strongly to people than to things.",
"normal",  # "I tend to have very strong interests which I get upset about if I can’t pursue.",
"reverse", # "I enjoy social chit-chat.",
"normal",  # "When I talk, it isn’t always easy for others to get a word in edgeways.",
"normal",  # "I am fascinated by numbers.",
"normal",  # "When I’m reading a story, I find it difficult to work out the characters’ intentions.",
"normal",  # "I don’t particularly enjoy reading fiction.",
"normal",  # "I find it hard to make new friends.",
"normal",  # "I notice patterns in things all the time.",
"reverse", # "I would rather go to the theatre than a museum.",
"reverse", # "It does not upset me if my daily routine is disturbed.",
"normal",  # "I frequently find that I don’t know how to keep a conversation going.",
"reverse", # "I find it easy to “read between the lines” when someone is talking to me.",
"reverse", # "I usually concentrate more on the whole picture, rather than the small details."
"reverse", # "I am not very good at remembering phone numbers."
"reverse", # "I don’t usually notice small changes in a situation, or a person’s appearance.",
"reverse", # "I know how to tell if someone listening to me is getting bored.",
"reverse", # "I find it easy to do more than one thing at once.",
"normal",  # "When I talk on the phone, I’m not sure when it’s my turn to speak.",
"reverse", # "I enjoy doing things spontaneously.",
"normal",  # "I am often the last to understand the point of a joke.",
"reverse", # "I find it easy to work out what someone is thinking or feeling just by looking at their face.",
"reverse", # "If there is an interruption, I can switch back to what I was doing very quickly.",
"reverse", # "I am good at social chit-chat.",
"normal",  # "People often tell me that I keep going on and on about the same thing.",
"reverse", # "When I was young, I used to enjoy playing games involving pretending with other children.",
"normal",  # "I like to collect information about categories of things (e.g. types of car, types of bird, types of train, types of plant, etc.).",
"normal",  # "I find it difficult to imagine what it would be like to be someone else.",
"normal",  # "I like to plan any activities I participate in carefully.",
"reverse", # "I enjoy social occasions.",
"normal",  # "I find it difficult to work out people’s intentions.",
"normal",  # "New situations make me anxious.",
"reverse", # "I enjoy meeting new people.",
"reverse", # "I am a good diplomat.",
"reverse", # "I am not very good at remembering people’s date of birth.",
"reverse"  # "I find it very easy to play games with children that involve pretending."
)
)
aq_df$total = 0
for(i in 1:50){
if(aq_key$score[i] == "normal"){
# If the participant agrees then give them a point
# 0 = Strongly Agree
# 1 = Agree
aq_df$total = aq_df$total + ifelse(aq_df[,i+1] < 2, 1,0)
} else if(aq_key$score[i] == "reverse") {
# If the participant disagrees then give them a point
# 2 = Disagree
# 3 = Strongly Disagree
aq_df$total = aq_df$total + ifelse(aq_df[,i+1] > 1, 1,0)
}
}
aq_rating_df <- data.frame(
ID = aq_df$ID,
aq = aq_df$total,
age = aq_df$Age,
gender = aq_df$Gender,
social = NA,
nonsocial = NA
)
library(tidyverse)
library(rprime)
# Converting e-prime to r readable data
eprime_files <- list.files("ratings/", pattern="*.txt", full.names=TRUE)
for(i in 1:length(eprime_files)){
this_file <- read_eprime(eprime_files[i])
# Convert lines from an Eprime file into EprimeFrame objects
wm_frames <- FrameList(this_file)
# Make it a data frame.
experiment_df <- to_data_frame(wm_frames)
# Store as a .csv
write.csv(experiment_df, str_replace(eprime_files[i],".txt",".csv"))
}
# The following key identifies whether each image was social or nonsocial
image_key <- read.csv("imageKey.csv")
trial_outliers = 0
# Load fMRI data
fmri_data = read.csv("fmriData.csv")
# Process each newly created .csv version of the rating data
for(i in 1:length(aq_rating_df$ID)){
# Select a participant
this_id = aq_rating_df$ID[i]
# Load their rating .csv file
this_data <- read.csv(paste("ratings/Rating task-",this_id,"-1.csv", sep=""))
# Create a blank Sociality column to store the sociality of each image
this_data$Sociality = ""
for(j in 1:length(image_key$image)){
# Add the correct sociality for each image to the new Sociality column
this_data$Sociality[this_data$StimImage == image_key$image[j]] = image_key$sociality[j]
}
# Identify outliers more than 3 SDs away from the participant's mean valence rating
val_mean  <- mean(this_data$ValenceUserChoice, na.rm = T)
val_sd    <- sd(this_data$ValenceUserChoice, na.rm = T)
valid_val <- abs(this_data$ValenceUserChoice - val_mean) < 3 * val_sd
if(sum(fmri_data$ID == this_id) == 1){
trial_outliers = trial_outliers + sum(!valid_val, na.rm = T)
}
# Select only valid social trials
social_val    <- this_data$ValenceUserChoice[this_data$Sociality == "Social" & valid_val]
# Select only valid nonsocial trials
nonsocial_val    <- this_data$ValenceUserChoice[this_data$Sociality == "Nonsocial" & valid_val]
aq_rating_df$social[i]    = mean(social_val, na.rm=T)
aq_rating_df$nonsocial[i] = mean(nonsocial_val, na.rm=T)
}
# Store this processed data
write.csv(aq_rating_df,"aq_rating.csv")
# Merge fMRI with rating and AQ data
summary_data = merge(aq_rating_df,fmri_data,by = "ID")
# Age calculations to confirm the above
mean(summary_data$age)
sd(summary_data$age)
min(summary_data$age)
max(summary_data$age)
# AQ calculations to confirm the above
mean(summary_data$aq)
sd(summary_data$aq)
min(summary_data$aq)
max(summary_data$aq)
library(lsr)
# t-test between social vs. nonsocial images
t.test(summary_data$social, summary_data$nonsocial, paired = T)
cohensD(summary_data$social - summary_data$nonsocial)
# one sample t-test of valence of social images compared to a neutral rating of 5
t.test(summary_data$social, mu = 5)
sd(summary_data$social)
cohensD(summary_data$social, mu = 5)
# one sample t-test of valence of nonsocial images compared to a neutral rating of 5
t.test(summary_data$nonsocial, mu = 5)
sd(summary_data$nonsocial)
cohensD(summary_data$nonsocial, mu = 5)
# Look for the max cooks d in all correlations to see if any are above 1
# are there any correlational outliers between AQ and Social Valence ratings?
aq.social.lm <- lm(social ~ aq, summary_data)
max(cooks.distance(aq.social.lm))
# are there any correlational outliers between AQ and Nonsocial Valence ratings?
aq.nonsocial.lm <- lm(nonsocial ~ aq, summary_data)
max(cooks.distance(aq.nonsocial.lm))
# are there any correlational outliers between AQ and right Fusiform Gyrus - medial orbitofrontal cortex connectivity?
aq.rFuG10.mPFC50.con.lm <- lm(rFuG10.mPFC50.con ~ aq, summary_data)
max(cooks.distance(aq.rFuG10.mPFC50.con.lm))
# are there any correlational outliers between AQ and rFuG - left Amygdala connectivity?
aq.rFuG10.LAmy50.con.lm <- lm(rFuG10.LAmy50.con ~ aq, summary_data)
max(cooks.distance(aq.rFuG10.LAmy50.con.lm))
# are there any correlational outliers between AQ and rFuG - right Amygdala connectivity?
aq.rFuG10.RAmy50.con.lm <- lm(rFuG10.RAmy50.con ~ aq, summary_data)
max(cooks.distance(aq.rFuG10.RAmy50.con.lm))
# are there any correlational outliers between AQ and rFuG - left ventral striatum connectivity?
aq.rFuG10.LAcc50.con.lm <- lm(rFuG10.LAcc50.con ~ aq, summary_data)
max(cooks.distance(aq.rFuG10.LAcc50.con.lm))
# are there any correlational outliers between AQ and rFuG - right ventral striatum connectivity?
aq.rFuG10.RAcc50.con.lm <- lm(rFuG10.RAcc50.con ~ aq, summary_data)
max(cooks.distance(aq.rFuG10.RAcc50.con.lm))
cor.test(summary_data$aq,summary_data$social)
cor.test(summary_data$aq,summary_data$nonsocial)
# Steiger test to compare correlations
library(cocor)
cocor::cocor(
~aq + social | aq + nonsocial  ,
summary_data,
test = "steiger1980",
return.htest = F,
alternative = "less",
)
library(ggplot2)
library(ggtext)
blank_background = theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
plot.title = element_markdown()
)
aq.valence.plot.data = data.frame(
AQ = rep(summary_data$aq,2),
Sociality = c(
rep("Social", nrow(summary_data)),
rep("Nonsocial", nrow(summary_data))
),
Valence = c(summary_data$social, summary_data$nonsocial)
)
ggplot(aq.valence.plot.data, aes(x=AQ, y=Valence, color=Sociality, shape=Sociality)) +
geom_point(size = 5) +
geom_smooth(
formula = "y~x",
method=lm,
se=TRUE,
fullrange=TRUE,
linewidth = 2
)+
theme_classic() +
theme(text = element_text(size=20)) +
xlab("Autism Quotient") +
ylab("Valence Rating")
sort(social_r_value_vector)
sort(social_r_value_vector)[1]
sort(social_r_value_vector)[1000]
sort(social_r_value_vector)[25]
sort(social_r_value_vector)[975]
this_boot
sd(social_r_value_vector)
sd(sociality_r_value_vector)
cooks.distance(lm(social ~ aq, summary_data))
max(cooks.distance(lm(social ~ aq, summary_data)))
?cooks.distance
4/(37-2)
4/(37)
cooks.distance(lm(social ~ aq, summary_data)) > .108
cook_outliers = cooks.distance(lm(social ~ aq, summary_data)) > .108
cor.test(summary_data$aq[!cook_outliers], summary_data$sociality[!cook_outliers])
cook_outliers
cook_outliers[§]
cook_outliers[§]
cook_outliers[1]
cook_outliers[1] == F]
cook_outliers[1] == F
cook_outliers = vector(cooks.distance(lm(social ~ aq, summary_data)) > .108)
cook_outliers = as.vector(cooks.distance(lm(social ~ aq, summary_data)) > .108)
cook_outliers
cor.test(summary_data$aq[!cook_outliers], summary_data$sociality[!cook_outliers])
summary_data$aq[!cook_outliers]
summary_data$sociality[!cook_outliers]
summary_data$sociality[
summary_data$sociality
summary_data$sociality = summary_data$social - summary_data$nonsocial
cor.test(summary_data$aq[!cook_outliers], summary_data$sociality[!cook_outliers])
cooks.distance(lm(social ~ aq, data = summary_data))
4/37
as.vector(cooks.distance(lm(social ~ aq, data = summary_data)) > 4/37)
cor.test(summary_data$aq,summary_data$social)
cor.test(summary_data$aq[!cooks_outliers],summary_data$social[!cooks_outliers])
cooks_outliers <- as.vector(cooks.distance(lm(social ~ aq, data = summary_data)) > 4/37)
cor.test(summary_data$aq[!cooks_outliers],summary_data$social[!cooks_outliers])
cooks_outliers
cooks_outliers_nonsocial <- as.vector(cooks.distance(lm(nonsocial ~ aq, data = summary_data)) > 4/37)
cooks_outliers_nonsocial
F*F
cooks_outliers = F+T
F+T
F+T+T
cooks_outliers_nonsocial <- as.vector(cooks.distance(lm(nonsocial ~ aq, data = summary_data)) > 4/37)
cooks_outliers = cooks_outliers_social + cooks_outliers_nonsocial
cooks_outliers_social <- as.vector(cooks.distance(lm(social ~ aq, data = summary_data)) > 4/37)
cooks_outliers_nonsocial <- as.vector(cooks.distance(lm(nonsocial ~ aq, data = summary_data)) > 4/37)
cooks_outliers = cooks_outliers_social + cooks_outliers_nonsocial
cooks_outliers
cooks_not_outliers = cooks_outliers == 0
cor.test(summary_data$aq[cooks_not_outliers],summary_data$social[cooks_not_outliers])
cor.test(summary_data$aq[cooks_not_outliers],summary_data$social[cooks_not_outliers])
cor.test(summary_data$aq[cooks_not_outliers],summary_data$nonsocial[cooks_not_outliers])
cocor::cocor(
~aq + social | aq + nonsocial  ,
summary_data[cooks_not_outliers,],
test = "steiger1980",
return.htest = F,
alternative = "less",
)
cor.test(summary_data$aq[cooks_not_outliers],summary_data$social[cooks_not_outliers])
aq.valence.plot.data = data.frame(
AQ = rep(summary_data$aq[cooks_not_outliers],2),
Sociality = c(
rep("Social", nrow(summary_data[cooks_not_outliers,])),
rep("Nonsocial", nrow(summary_data[cooks_not_outliers,]))
),
Valence = c(summary_data$social[cooks_not_outliers], summary_data$nonsocial[cooks_not_outliers])
)
ggplot(aq.valence.plot.data, aes(x=AQ, y=Valence, color=Sociality, shape=Sociality)) +
geom_point(size = 5) +
geom_smooth(
formula = "y~x",
method=lm,
se=FALSE,
fullrange=TRUE,
linewidth = 2
)+
theme_classic() +
theme(text = element_text(size=20)) +
xlab("Autism Quotient") +
ylab("Valence Rating")
cor.test(summary_data$aq[cooks_not_outliers],summary_data$social[cooks_not_outliers])
cor.test(summary_data$aq[cooks_not_outliers],summary_data$nonsocial[cooks_not_outliers])
cocor::cocor(
~aq + social | aq + nonsocial  ,
summary_data[cooks_not_outliers,],
test = "steiger1980",
return.htest = F,
alternative = "less",
)
774^(1/3)
9.1815^3
