---
title: "T-Tests (R, Python)"
editor: visual
---

{{< include ../overview.qmd >}}

::: callout-note
## T-values and F-Values

As you'll see below, we'll discuss t-values and F-values you can calculate. T-values are from t-tests, in which there are only 2 conditions or 2 groups of participants you are comparing between. F-values are from a variety of tests and are not restricted to comparisons between 2 conditions or 2 groups. For example, you can run ANOVAs to get F-Values on 2 or more conditions. The underlying statistics between calculating T and F-Values is similar when comparing 2 conditions/groups, which is why $t = \sqrt{F}$ and $F = t^2$ .

When describing a GLM approach, this is more aligned with how you would calculate an F-value.
:::

In all general linear models you are trying to compare how much of the variance is explained by a model compared to what's not being explained by a model. In short

$$
\frac{var_{explained}}{var_{unexplained}} = \frac{SS_{explained}}{SS_{unexplained}}
$$

For each type of t-test, the way we calculate this is slightly different:

## One-Sample t-tests

### GLM approach

One sample t-tests try to explain whether variance of data is better explained around one specific value (sample mean) compared to another (previously assumed value). For example, imagine that you wanted to test whether life expectancy is higher than 55 across the world:

-   Your $\mu$ would be 55. This can be thought of as the assumed population mean that we want to use our sample to test.

-   Your $\bar{x}$ would be the sample mean.

Let's visualise these values using gapminder data from 2007:

::: panel-tabset
## R

```{r}
library(ggplot2)
library(gapminder)
gapminder_2007 <- subset(
  gapminder,   # the data set
  year == 2007
)
ggplot(gapminder_2007, aes(x=year,y=lifeExp)) + 
  geom_jitter() + 
  xlab("") + 
  theme(axis.text.x = element_blank()) +
  theme(axis.ticks.x = element_blank()) +
  geom_segment(
    aes(
      x = 2006.6,
      xend = 2007.4,
      y = 55,
      yend = 55,
      color = "Mu"
    )
  ) +
  geom_segment(
    aes(
      x = 2006.6,
      xend = 2007.4,
      y = mean(lifeExp),
      yend = mean(lifeExp),
      color = "Sample Mean"
    )
  )
```

## Python

```{python}
#| eval: false
# load the gapminder module and import the gapminder dataset
from gapminder import gapminder

# import matplotlib
import matplotlib.pyplot as plt

import seaborn as sns

# create a new data frame that only focuses on data from 2007
gapminder_2007 = gapminder.loc[gapminder['year'] == 2007]

# Create the plot
plt.figure(figsize=(8, 4))

# create the scatterplot with some jitter
sns.stripplot(x="year", y='lifeExp', data=gapminder_2007, dodge=True, jitter=0.5)

# add an horizontal line for Mu
plt.axhline(y=55, color='r', linestyle='-', label='Mu')

# add an horizontal line for the mean of 'lifeExp'
plt.axhline(y=gapminder_2007['lifeExp'].mean(), color='g', linestyle='-', label='Sample Mean')

# remove the label on the x-axis
plt.xlabel("")

# remove the tick on the x-axis
plt.xticks([])

# add the legend 
plt.legend()

# show the plot
plt.show()

```

![Scatterplot with Mu and Sample Mean of 'Life expectancy'](TTests_Figure1.png){fig-align="left" width="650"}
:::

We want to create a model that explains any variance around the population mean (mu or $\mu$). The sample mean could be modeled as such:

$$
y = \bar{y} + e
$$

-   Y is the data point value you are trying to predict. Note that for this formula you will **always** have the same predicted outcome (the mean).

-   $\bar{y}$ is mean of all y data points. You are only interested in whether predicting y based on y's mean captures a significant amount of the variance of the y-values around the $\mu$.

-   $e$ is the error, i.e. the residuals that the module do not predict effectively.

If the sample mean is a useful model, then it will explain a large proportion of the variance around the "population" mean (and will also suggested that there is significant reason to reject the population mean). The total variance using sum of squares is thus:

$$
SS_{total} = \sum(x_i-\mu)^2
$$

Which for the above data would give us:

::: panel-tabset
## R

```{r}
sum((gapminder_2007$lifeExp - 55)^2)
```

## Python

```{python}
#| eval: false

# import numpy 
import numpy as np

# calculate the squared dfference
np.sum((gapminder_2007['lifeExp'] - 55) ** 2)
```

```         
41025.157014
```
:::

So your explained variance by this model is any difference between the Mu ($\mu$) and the sample mean ($\bar{x}$). To summarise this using sum of squares, for each data point you subtract the two from each other and square them, as this difference is what we can explain of variance away from the MU:

$$
SS_{explained} = N * (\mu - \bar{x})^2
$$

Which for the above data would give us:

::: panel-tabset
## R

```{r}
length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2
```

## Python

```{python}
#| eval: false

len(gapminder_2007['lifeExp']) *( 55 - gapminder_2007['lifeExp'].mean())**2
```

```         
20473.303823352093
```
:::

Unexplained variance would be the residuals around the sample mean, as this is variance that is not explained by the model. Conveniently, we can calculate the sum of squared around the sample mean quite elegantly:

$$
SS_{unexplained} = \sum(x_i-\bar{x})^2
$$

Which for the above data would give us

::: panel-tabset
## R

```{r}
sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)
```

## Python

```{python}
#| eval: false

np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)
```

```         
20551.853190647882
```
:::

In general linear models we calculate $F$-values as a statistic of how effective a model is at explaining the variance. These $F$-values use degrees of freedom (see [here](../describingData/dispersion.html#degrees-of-freedom) for a reminder) to adjust for

So the F-value should be:

$$
F = \frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \frac{20473.3/(Predictors)}{20551.85/(N-1)} = \frac{20473.3/1}{20551.85/141} 
$$

::: panel-tabset
## R

```{r}
f_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (
  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)
  
)
f_value
```

## Python

```{python}
#| eval: false

# Calculate the sum of squared differences between each value in 'lifeExp' and the mean
ss_between = len(gapminder_2007['lifeExp']) * (55 - gapminder_2007['lifeExp'].mean()) ** 2

# Calculate the sum of squared differences within groups
ss_within = np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)

# Calculate the degrees of freedom for between groups and within groups
df_between = 1
df_within = len(gapminder_2007['lifeExp']) - 1

# Calculate the F-statistic
f_value = (ss_between / df_between) / (ss_within / df_within)

print(f_value)
```

```         
140.46109673488004
```
:::

F-values are squares of t-values, so let's see if this is true here also:

::: panel-tabset
## R

```{r}
sqrt(f_value)

t.test(gapminder_2007$lifeExp, mu=55)
```

## Python

```{python}
#| eval: false

from scipy import stats

# Calculate the square root of the F-value
np.sqrt(f_value)
```

```         
11.851628442323022
```

```{python}
#| eval: false

# Perform a t-test
t_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)

print("t-statistic:", t_statistic)
print("p-value:", p_value)
```

```         
t-statistic: 11.851628442323024
p-value: 6.463174215427706e-23
```
:::

### ANVOA vs. T-test formula

Great. So now that we've highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:

$$
T = \sqrt{F} = \sqrt{\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \sqrt{\frac{N * (\mu - \bar{x})^2/(levelsOfPredictors - 1)}{\sum(x_i-\bar{x})^2/(N-1)}} =
$$

$$
\sqrt{\frac{N * (\mu - \bar{x})^2/(2-1)}{\sigma^2}} = \frac{\sqrt{N * (\mu - \bar{x})^2}}{\sqrt{\sigma^2}} = \frac{\sqrt{(\mu - \bar{x})^2}}{\sigma/\sqrt{N}} = \frac{\mu - \bar{x}}{\sigma/\sqrt{N}}
$$ where:

-   T is the t-value
-   F is the f-value
-   $SS_{exp}$ is the sum of squares of the data explained by the model
-   $SS_{unexp}$ is the sum of squares of the data not explained by the model (i.e. the residuals)
-   $df_{exp}$ is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it's only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it's less intuitive that there are 2 levels).

To confirm, the formula for a one-sample t-test is just:

$$
T = \frac{\mu - \bar{x}}{\sigma/\sqrt{N}}
$$

## Paired samples t-tests

Paired samples t-tests can be approached like 1-sample t-tests, but you first of all need to collapse the data to have a single variable to compare to a $\mu$ of zero. Let's do this for gapminder data, comparing life expectancies between 2002 and 2007:

::: panel-tabset
## R

```{r}
gapminder_2002_2007_life_exp <- gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]
t.test(gapminder_2002_2007_life_exp, mu = 0)

```

## Python

```{python}
#| eval: false

gapminder_2002_2007_life_exp = life_exp_2007.reset_index(drop=True) - life_exp_2002.reset_index(drop=True)
t_statistic, p_value = stats.ttest_1samp(gapminder_2002_2007_life_exp, popmean=0)

print("t-statistic:", t_statistic)
print("p-value:", p_value)
```

```         
t-statistic: 14.664513524875451
p-value: 3.738316746290281e-30
```
:::

The above suggests that life expectancy was significanctly different. Let's see if we get the exact same value when we use a paired t-test in R:

::: panel-tabset
## R

```{r}
t.test(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002], paired=T)
```

## Python

```{python}
#| eval: false

# Filter data for the year 2007
life_exp_2007 = gapminder[gapminder['year'] == 2007]['lifeExp']

# Filter data for the year 2002
life_exp_2002 = gapminder[gapminder['year'] == 2002]['lifeExp']

# Perform a paired t-test
t_statistic, p_value = stats.ttest_rel(life_exp_2007, life_exp_2002)

print("T-statistic:", t_statistic)
print("P-value:", p_value)
```

```         
t-statistic: 14.664513524875451
p-value: 3.738316746290281e-30
```
:::

Looks identical. Let's compare formulas to see why this is ( $\mu$ = 0 , and so it isn't written) :

$$
 t_{paired} = \frac{\bar{x_1} - \bar{x_2}}{\sigma_{pooled}/\sqrt{N}} = \frac{\bar{x_3}}{\sigma_{pooled}/\sqrt{N}}
$$

Where

-   $\bar{x_1}$ is the mean of condition 1

-   $\bar{x_2}$ is the mean of condition 2

-   $\bar{x_3}$ is the mean of the result you get when you subtract condition 2 from condition 1 for each participant, i.e. $mean(x_1-x_2)$.

-   $$
    \sigma_{pooled}  = \sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}} OR \frac{\sum(x_1 - x_2)^2}{N-1} 
    $$ One way effectively gets the average of the standard deviations of condition and 1. The second way gets the standard deviation of the differences between conditions 1 and 2. Both give you the same outcome.

-   $N$ is the number of participants

You can rewrite the above formula to compare $\bar{x_3}$ to $\mu$, as we know $\mu$ is zero, which would make this formula (effectively) identical to the one above for one-sample t-tests: \$\$

$$
\frac{\bar{x_3} - \mu}{\sigma_{pooled}/\sqrt{N}}
$$

## Independent Samples t-tests

### ANOVA approach

T-tests are restricted to comparisons between 2 conditions/groups, so we will restrict the Gapminder data to allow a comparison between 2 continents. To see if life expectancy was different if you are born in Europe compared to the Americas, let's first check what the sum of squares is when you just use the **mean** as the model of life expectancy **across** these contents (so we're **not** separating by continent yet):

::: panel-tabset
## R

```{r}
#| label: fig-mean-life-exp-resid-2-continents
#| fig-cap: "The errors around the mean of life expectancy across Europe and American countries"
gapminder_americas_europe <- subset(
  gapminder_2007,   # the data set
  continent == "Europe" | continent == "Americas"
)

ggplot(
  gapminder_americas_europe, aes(x=rank(lifeExp), y=lifeExp)
) + 
  geom_point() +
  geom_hline(yintercept = mean(gapminder_americas_europe$lifeExp), color="blue") +
  geom_segment(
    aes(
      xend = rank(lifeExp),
      yend = mean(lifeExp),
      color = "resid"
    )
  ) +
  theme(legend.position = "none")

```

## Python

```{python}
#| eval: false

gapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == "Europe") | (gapminder_2007['continent'] == "Americas")]

gapminder_americas_europe["lifeExp_rank"] = gapminder_americas_europe["lifeExp"].rank()

fig, ax = plt.subplots(figsize =(7, 5))

#scatter plot for the dataset
plt.scatter(gapminder_americas_europe["lifeExp_rank"], gapminder_americas_europe["lifeExp"], color='black', s=10)
# only one line may be specified; full height
plt.axhline(y=gapminder_americas_europe["lifeExp"].mean(), color='blue', ls='-')

plt.vlines(x=gapminder_americas_europe["lifeExp_rank"],ymin=gapminder_americas_europe["lifeExp"], ymax=gapminder_americas_europe["lifeExp"].mean(), colors='red', lw=0.5)

# add title on the x-axis
plt.xlabel("rank(lifeExp)")

# add title on the y-axis
plt.ylabel("lifeExp")

plt.show()

```

![*Fig. 3.* The errors around the mean of life expectancy across Europe and American countries.](generalLinearModels-Figure3.png){fig-align="left" width="700" height="500"}
:::

Once we square the errors in the pink lines above, we'll get the **squares:**

::: panel-tabset
## R

```{r}
#| label: fig-mean-life-exp-resid-squared-2-continents
#| fig-cap: "The squared errors around the mean of life expectancy across Europe and American countries"
ggplot(
  gapminder_americas_europe, 
  aes(
    x=rank(lifeExp), 
    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.
    y=(lifeExp-mean(lifeExp))^2
  )
) + 
  geom_point() +
  geom_segment(
    aes(
      xend = rank(lifeExp),
      yend = 0,
      color = "resid"
    )
  ) +
  theme(legend.position = "none")

sum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)

```

## Python

```{python}
#| eval: false
fig, ax = plt.subplots(figsize =(7, 5))

#scatter plot for the dataset
plt.scatter(gapminder_americas_europe["lifeExp_rank"], (gapminder_americas_europe["lifeExp"]-gapminder_americas_europe["lifeExp"].mean())**2, color='black', s=10)
# only one line may be specified; full height

plt.vlines(x=gapminder_americas_europe["lifeExp_rank"],ymin=0, ymax=(gapminder_americas_europe["lifeExp"]-gapminder_americas_europe["lifeExp"].mean())**2, colors='red', lw=0.5)

# add title on the x-axis
plt.xlabel("rank(lifeExp)")

# add title on the y-axis
plt.ylabel("(Life Expectancy - mean(Life Expectancy))^2")
plt.show()

sum((gapminder_americas_europe["lifeExp"]-gapminder_americas_europe["lifeExp"].mean())**2)
```

![The squared errors around the mean of life expectancy across Europe and American countries](generalLinearModels-Figure4.png){fig-align="left" width="700" height="500"}

```         
953.4477649818183
```
:::

And when you add all of these together:

$$
SumOfSquares = \sum(Y_i-\bar{Y})^2 = 953.4478
$$

So if the model we create for a t-test would result in a smaller **sum of squares** then that suggests it's a more precise model for estimating life expectancy than simply using the **mean** as a model. This is because this would mean there's less **error** in this model. Let's model this using a t-test. For this we will need to effect code continent:

::: panel-tabset
## R

```{r}
# create a column to place 1 or -1 for each row dependent on the country
contEffect = NA
contEffect[gapminder_americas_europe$continent == "Europe"] = 1
contEffect[gapminder_americas_europe$continent == "Americas"] = -1
gapminder_americas_europe = cbind(contEffect,gapminder_americas_europe)
rmarkdown::paged_table(head(gapminder_americas_europe))
```

## Python

```{python}
#| eval: false

gapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == "Europe") | (gapminder_2007['continent'] == "Americas")]
gapminder_americas_europe["contEffect"]=0
gapminder_americas_europe["contEffect"].loc[(gapminder_americas_europe['continent'] == "Europe")]=1
gapminder_americas_europe["contEffect"].loc[(gapminder_americas_europe['continent'] == "Americas")]=-1
cols = list(gapminder_americas_europe.columns)
cols = list(gapminder_americas_europe.columns)
cols = cols[len(cols)-1:len(cols):1]+cols[0:-1:1]
gapminder_americas_europe = gapminder_americas_europe[cols]
print(tabulate(gapminder_americas_europe[:6], headers=gapminder_americas_europe.head(), tablefmt="fancy_grid",showindex=False))
```

![Table](generalLinearModels-Table1.png){fig-align="left" height="300"}
:::

Now that we have effect coded the continent, we can create a new model to try to predict an individual's life expectancy based on which continent they are from

$$
Y = intercept + \beta * effectVariable + Error
$$

Which in our case means:

$$
lifeExp = mean(lifeExp) + \beta * contEffect + Error
$$

-   Y being the predicted life expectancy.

-   $\bar{Y}$ being the mean life expectancy regardless of continent. For a t-test this is also the $intercept$.

-   $\beta$ being how much to adjust the prediction based on which continent the person is from

-   $contEffect$ being 1 (Europe) or -1 (Americas) to reflect which continent the participant is from

-   $Error$ being any error in the prediction not captured by the model

To get the $intercept$ and $\beta$ for the above formula let's use linear model functions:

::: panel-tabset
## R

```{r}
#| label: fig-model-life-exp-mean-2-continents
#| fig-cap: "Plot with the errors around the mean of life expectancy across Europe and American countries."

continent_ttest <- lm(lifeExp ~ contEffect, gapminder_americas_europe)

continent_ttest$coefficients[1] 
continent_ttest$coefficients[2]


gapminder_americas_europe$t_fit = continent_ttest$coefficients[1] + # intercept
  continent_ttest$coefficients[2]                       * # gradient
  gapminder_americas_europe$contEffect


ggplot(gapminder_americas_europe, aes(x = contEffect, y = lifeExp)) +
  geom_segment(
    position = "jitter",
    #arrow = arrow(length = unit(0.01, "npc"),ends = "first"),
    aes(
      xend = contEffect,
      yend = t_fit,
      color = "resid"
    )
  ) + 
  geom_segment(aes(
    x = -1.9, 
    xend = -.1, 
    y = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],
    yend = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),
    color = "blue"
  ) + 
  geom_segment(
    aes(
      x = 0.1, 
      xend = 1.9, 
      y = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],
      yend = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]
    ),
    color = "blue"
  ) + 
  geom_segment(
    aes(
      x = - 1.9,
      xend = 1.9,
      y = mean(lifeExp),
      yend = mean(lifeExp)
    ),
    color = "dark green"
  )
```

## Python

```{python}
#| eval: false

from scipy import stats

# convert 'contEffect' to type category
gapminder_americas_europe['contEffect'] = gapminder_americas_europe['contEffect'].astype('category')

# lm 'contEffect' ~ 'lifeExp'
continent_ttest = stats.linregress(gapminder_americas_europe['contEffect'],gapminder_americas_europe['lifeExp'])

# show results of lm
continent_ttest
```

```         
LinregressResult(slope=2.020240000000001, intercept=75.62836, rvalue=0.4832076439158285, pvalue=0.00018637488941351192, stderr=0.502794193121279, intercept_stderr=0.5027941931212789)
```

```{python}
#| eval: false
gapminder_americas_europe["contEffect"]=0
gapminder_americas_europe["contEffect"].loc[(gapminder_americas_europe['continent'] == "Europe")]=1
gapminder_americas_europe["contEffect"].loc[(gapminder_americas_europe['continent'] == "Americas")]=-1

gapminder_americas_europe['t_fit'] = continent_ttest.intercept +continent_ttest.slope * gapminder_americas_europe['contEffect']

gapminder_americas_europe['t_res_square'] = (gapminder_americas_europe['lifeExp'] - gapminder_americas_europe['t_fit'])**2

# calculate 'lifeExp' mean for 'contEffect' ==-1
m1 = gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == -1].mean()

# calculate 'lifeExp' mean for 'contEffect' ==1
m2 = gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == 1].mean()

# repeat lifeExp' mean for 'contEffect' ==-1
m11=np.repeat(m1, len(gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == -1]), axis=0)

# repeat lifeExp' mean for 'contEffect' ==1
m22=np.repeat(m2, len(gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == 1]), axis=0)

# create x coordinates for 'contEffect' ==-1
x1 = np.arange(-1.98, -.05, 0.08)

# create x coordinates for 'contEffect' ==1
x2 = np.arange(0.05, 1.98, 0.065)


fig, ax = plt.subplots(figsize =(10, 7))
ax.set_ylim([60, 85])
plt.axhline(y=gapminder_americas_europe["lifeExp"].mean(), color='green', ls='-')
plt.hlines(y=m1,xmin=-1.99, xmax=-.04, colors='blue', lw=0.5)
plt.hlines(y=m2,xmin=1.99, xmax=.04, colors='blue', lw=0.5)
plt.vlines(x= x1,ymin=gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == -1], ymax=m11, colors='red', lw=0.5)
plt.vlines(x= x2,ymin=gapminder_americas_europe["lifeExp"][gapminder_americas_europe['contEffect'] == 1], ymax=m22, colors='red', lw=0.5)

# add title on the x-axis
plt.xlabel("contEffect")

# add title on the y-axis
plt.ylabel("lifeExp")

plt.show()
```

![Plot with the errors around the mean of life expectancy across Europe and American countries](generalLinearModels-Figure5.png){fig-align="left" width="700" height="500"}
:::

Countries in the americas are dummy coded as -1 and countries in Europe are dummy coded as 1. Note that jittering has been used to help visualise variation within continents, and so all countries in Americas had a $contEffect$ score of -1, even if the jittering above makes it look like participants from Europe had slightly different $contEffect$ values to each other.

So now that we've visualised the predictions and the error, lets summarise these errors with their sum of squares:

::: panel-tabset
## R

```{r}
#temp_summary <- summary(lm(lifeExp ~ contEffect, data = gapminder_americas_europe))
summary(aov(lifeExp ~ contEffect, data = gapminder_americas_europe))

# between
overall_mean <- mean(gapminder_americas_europe$lifeExp)
europe_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1])
america_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1])
ss_between <- 
  sum(gapminder_americas_europe$contEffect == 1) * (europe_mean - overall_mean)^2 +
  sum(gapminder_americas_europe$contEffect == -1) * (america_mean - overall_mean)^2
  
top_half = ss_between

ss_within = (
  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] - europe_mean)^2) + 
  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1] - america_mean)^2)
)
  
bottom_half = (ss_within/(length(gapminder_americas_europe$lifeExp) - 2))

top_half/bottom_half

# Compare with a t-test

t.test(
  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],
  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],
  var.equal = T
)


4.018^2
# look at a t-distribution compared to an f-distribution

```

## Python

```{python}
#| eval: false

import statsmodels.api as sm
from statsmodels.formula.api import ols

model = ols('lifeExp ~ contEffect', data = gapminder_americas_europe).fit()
aov_table = sm.stats.anova_lm(model, typ=2)
aov_table

# between
overall_mean = gapminder_americas_europe['lifeExp'].mean()
europe_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1].mean()

america_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1].mean()

ss_between =(sum(gapminder_americas_europe['contEffect'] == 1) * (europe_mean - overall_mean)**2) + (sum(gapminder_americas_europe['contEffect'] == -1) * (america_mean - overall_mean)**2)

top_half = ss_between

ss_within = (sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1] - europe_mean)**2)+ sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1] - america_mean)**2))

bottom_half = (ss_within/(len(gapminder_americas_europe['lifeExp']) - 2))

top_half/bottom_half

# Compare with a t-test
from scipy.stats import ttest_ind

t_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])
t_test
t_test.statistic **2
```

![Table](generalLinearModels-Table2.png){fig-align="left" height="100"}

```         
Ttest_indResult(statistic=4.018025720342183, pvalue=0.00018637488941352037)

16.144530689331322
```
:::

So the new unexplained variance (AKA residual) is 730.8276, which is smaller than it was when we just used the mean regardless of continent (953.4478; 935.4478 is also the **total variance** around the mean).

The sum of squares for the effect of continent can be calculated by subtracting the residuals from the total sum of squares (SS):

$$
effect_{SS} = total_{SS} - residual_{SS} = 953.4478 - 730.8276 = 222.6202
$$

The $effect_{SS}$ can also be thought of our explained variance! So we can actually calculate how much of the model is explained by the model ($r^2$):

$$
r^2 = \frac{SS_{explained}}{SS_{total}} = \frac{222.6202}{953.4478} = 0.2335
$$

Let's confirm that this is correct by comparing a manual calculation and a function:

```{r}
222.6202/953.4478 # should be  0.2335 
summary(lm(lifeExp ~ contEffect, gapminder_americas_europe))$r.squared
```

Great, the difference reflects a rounding error.

Note that the above formula is similar to the formula for $r$ from [simple regressions](../regressions/simpleRegressions.html#proportion-of-variance-explained). Variance is the square root of the Sum of Squares (or Sum of Squares is the **square** of variance!):

$$
r = \sqrt{\frac{SS_{explained}}{SS_{total}}} = \frac{\sqrt{SS_{explained}}}{\sqrt{SS_{total}}} =\frac{var_{explained}}{var_{total}}
$$

### ANOVA approach vs conventional formula

The main formula for an independent samples t-test is:

$$
t =  \frac{\bar{X_1} - \bar{X_2}}{SE}
$$

However, this is a simplification of the GLM mathematics we've been doing above (although $df$ is the same for the denominator and numerator, so we ignored them as they cancelled out)

$$
t= \sqrt{f} = \sqrt{\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \frac{(\sum{\bar{X_1} - \mu)/(N_1-1) - (\sum{\bar{X_2} - \mu)}/(N_2-1)}}{
((\sum{X_{1i} - \bar{X_1}}) +  (\sum{X_{2i} - \bar{X_2}}) )/(N-1)}= \frac{\sum{\bar{X_1}/N_1 - \sum{\bar{X_2}}/N_2}}{SE} =  \frac{\bar{X_1} - \bar{X_2}}{SE}
$$

## Calculating p-values from t-distributions

Similar to the [normal distribution](../distributions/normal.html#z-scores-and-the-z-distribution), we can calculate how likely it is that you would get a t-value or higher (or lower) by drawing a t-distribution. A t-distribution is like a normal distribution (it's a bell curve), but also takes into account the number of participants using degrees of freedom (N-1). Here are the t-distributions you would get for 101, 21 and 6 participants:

```{r}
curve(dt(x, df=100), from=-4, to=4, col='green')
curve(dt(x, df=20), from=-4, to=4, col='red', add=TRUE)
curve(dt(x, df=5), from=-4, to=4, col='blue', add = TRUE) 

#add legend
legend(-4, .3, legend=c("df=5", "df=20", "df=100"),
       col=c("blue", "red", "green"), lty=1, cex=1.2)
```

We can see that the slopes get steeper the higher the $df$ is (i.e. the more participants you have). This means that the results become more significant even with the same t-value. Not convinced? Let's use a t-value of 2 to illustrate this. In the following figure you can see that the area under the curve for a t-value of 2 or more gets visually smaller the more participants/the higher the $df$ value is:

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt,   args =list(df =5), xlim = c(2,4), geom = "area") + 
  stat_function(fun = dt, args =list(df =5), color = "blue") +
  stat_function(fun = dt, args =list(df =20), color = "red") +
  stat_function(fun = dt, args =list(df =100), color = "green") 
  
```

Remember, area under the curve **IS** the p-value, so the area under the curve for a t-value of 2 or above for each degrees of freedom is:

```{r}
# area under the curve for 2 and above
pt(
  # t-value
  q=2, 
  # degrees of freedom
  df=5, 
  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)
  lower.tail = FALSE
)
pt(
  # t-value
  q=2, 
  # degrees of freedom
  df=20, 
  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)
  lower.tail = FALSE
)
pt(
  # t-value
  q=2, 
  # degrees of freedom
  df=100, 
  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)
  lower.tail = FALSE
)
```

Note that if you wanted a 2-tailed test (i.e. you didn't have an expected direction of your finding) you would double the area under the curve/p-value.

Similar principles apply for an F-distribution, described next.

## Calculating p-values from f-distributions

As F-values are based on the **sum of squares**, which are always positive, they cannot be negative. Also, F distributions can reflect more complex designs than t-distributions, such as comparisons of more than 2 levels. This means that they can have a variety of shapes. First, let's look at some distributions for designs with 2 conditions but differing numbers of participants:

```{r}
ggplot(data.frame(x = c(0, 4)), aes(x)) +
  # stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = "area") + 
  stat_function(fun = df, args =list(df1 =5, df2=1), color = "blue") +
  stat_function(fun = df, args =list(df1 =20, df2=1), color = "red") +
  stat_function(fun = df, args =list(df1 =100, df2=1), color = "green") +
  annotate("label", x = 2, y = .4, label = "df1 = 5, df2=1", colour="blue") +
  annotate("label", x = 2, y = .35, label = "df1 = 20, df2=1", colour="red") +
  annotate("label", x = 2, y = .3, label = "df1 = 100, df2=1", colour="green") +
  xlab("F-Value") +
  ylab("Density")
```

The shape is roughly the same, but shifts slightly as you get more people. What happens if we look at distributions where there are about 100 people but 2 or more conditions (df2 = 2 or more).

```{r}
ggplot(data.frame(x = c(0, 4)), aes(x)) +
  # stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = "area") + 
  stat_function(fun = df, args =list(df1 =100, df2=1), color = "blue") +
  stat_function(fun = df, args =list(df1 =100, df2=2), color = "red") +
  stat_function(fun = df, args =list(df1 =100, df2=3), color = "green") +
  annotate("label", x = 2, y = .4, label  = "df1 = 100, df2=1", colour="blue") +
  annotate("label", x = 2, y = .35, label = "df1 = 100, df2=2", colour="red") +
  annotate("label", x = 2, y = .3, label  = "df1 = 100, df2=3", colour="green") +
  xlab("F-Value") +
  ylab("Density")
```

In all cases, you get your p-value from calculating the area under the curve for that F-value or above.

## GLM approach (optional)

If we were to more strictly conceptualise this as a linear model, then we need to create a model that predicts the life expectancy based on the continent. As continent is a categorical variable, we need to allocate numbers for each continent. This can either be dummy coding (1 for 0 for each continent) or effect coding (1 or -1 for each continent). Let's go through both approaches.

# FINISH HERE

# Effect sizes

## T-tests

To capture how large the effect is, there are slightly different calculations when doing a within subject design (1 or 2 conditions that all participants complete) vs. a between subject design. In all cases these are Cohen $d$ calculations.

### One Sample T-tests

The size of an effect is how big the difference is between the mean and the $\mu$ that you are comparing it to, compared to the standard deviation:

$$
d = \frac{\bar{x} - \mu}{SD(x)}
$$

The general benchmarks for how big or small a Cohen's $d$ value are as follows:

-   .01 and below is very small ([Sawilowsky 2009](http://localhost:7754/describingData/dispersion.html#ref-sawilowsky2009new))

-   .2 and below is small ([Cohen 2013](http://localhost:7754/describingData/dispersion.html#ref-cohen2013statistical))

-   .5 is medium ([Cohen 2013](http://localhost:7754/describingData/dispersion.html#ref-cohen2013statistical))

-   .8 is large ([Cohen 2013](http://localhost:7754/describingData/dispersion.html#ref-cohen2013statistical))

-   1.2 is very large ([Sawilowsky 2009](http://localhost:7754/describingData/dispersion.html#ref-sawilowsky2009new))

-   2 is huge ([Sawilowsky 2009](http://localhost:7754/describingData/dispersion.html#ref-sawilowsky2009new))

### Paired sample T-tests

The size of an effect is how big the difference is between the conditions, compared to the standard deviation:

$$
d = \frac{\bar{x_1} - \bar{x_2}}{SD(x_1-x_2)}
$$

This actually can be thought of the same as one-sample t-tests after you take the step to calculate the difference between each value in $x_1$ and $x_2$ to make $x_3$ to compare to a $\mu$ of 0:

$$
x_3 = x_1 - x_2
$$ $$
\mu = 0
$$ $$
d = \frac{\bar{x_1}-\bar{x_2}}{SD(\bar{x_1}-\bar{x_2})} = \frac{\bar{x_3}}{SD(x_3)} = \frac{\bar{x_3}-\mu}{SD(x_3)}
$$

### Independent Sample T-tests

The formula for this is somewhat similar to that for a paired samples t-test, but because you are comparing between groups of participants you can't collapse the values between conditions because there are no pairings. So, you need to calculate the *pooled* standard deviation.

$$
SD_{pooled} = \sqrt{(SD(x_1)^2 + SD(x_1)^2)/2}
$$

$$
d = \frac{\bar{x_1}-\bar{x_2}}{SD_{pooled}}
$$

## F-tests

The effect sizes for F-tests are based on how much variance is explained by the model, which sometimes is exactly the same as $r^2$. Let's review some of the calculations above to confirm this:\
$$
r^2 = \frac{SS_{explained}}{SS_{total}} = \frac{222.6202}{953.4478} = 0.2335
$$

If we run a linear model on the data we should also confirm the $r^2$ is .2335:

```{r}
summary(lm(lifeExp ~ contEffect, gapminder_americas_europe))
```

We'll use a function to confirm that this is the same for eta squared also:

```{r}
lsr::etaSquared(lm(lifeExp ~ contEffect, gapminder_americas_europe))
```

Boom, you got effect sizes for F-tests like ANOVAs.

If you have understood this page it will set you up for the pages on each type of ANOVA as it uses the same concepts again and again.
