---
title: "Correlations"
format: 
  html:
    code-fold: true
editor: visual
---

Please make sure you've read about variance within the <a href="../describingData/dispersion.html" target="_blank">dispersion section</a> before proceeding with this page.

Correlations capture how much two variables are associated with each other by calculating the proportion of the **total variance** explained by how much the two variables **vary together (explained below).** To understand this, we need to think about how each variable varies **independently, together** and compare the two. We'll use the **gapminder** data to look at how how life expectancy correlated with GDP in 2007:

```{r}
library(gapminder)
library(ggplot2)

# create a new data frame that only focuses on data from 2007
gapminder_2007 <- subset(
  gapminder,   # the data set
  year == 2007     
)

# a reminder of the data frame
rmarkdown::paged_table(head(gapminder_2007))


# Basic scatter plot
ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap, 
    y=lifeExp
  )
) + geom_point() +
  theme(
    text=element_text(
      family="gochi",
      size = 20
    )
  )
```

*Note that in the figure above each dot represents an individual point from our data.Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).*

Generally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables.

## Variance of individual variables

For more insight into variance as a concept, have a look at [dispersion](../distributions/dispersion), but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for **each** of those is:

$$
var_x = \frac{\sum(x_i-\bar{x})^2}{N-1}
$$

$$
var_y = \frac{\sum(y_i-\bar{y})^2}{N-1}
$$

Just a reminder of what each part of the formula is:

$\sum$ is saying to add together everything

$x_i$ refers to each individual's x-score

$y_i$ refers to each individual's y-score

$\bar{x}$ refers to the mean x-score across all participants

$\bar{y}$ refers to the mean y-score across all participants

$N$ refers to the number of participants

$N-1$ is **degrees of freedom**, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the [dispersion](../distributions/dispersion) section).

The $SD$ (Standard deviation; which is just the square root of variance) of how data is distributed around the mean $life$$expectancy$ (per capita) can be visualised as follows within the gapminder $gdp*lifeExpectancy$ in the light blue box:

```{r}
# Basic scatter plot

life_exp_resid <- ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap, 
    y=lifeExp
  )
) + 
  geom_point() +
  geom_hline(
    yintercept = mean(gapminder_2007$lifeExp), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) +
  coord_cartesian(ylim = c(30, 85)) +
  xlab("GDP per capita") +
  ylab("Life Expectancy") +
  geom_segment(
    aes(
      xend = gdpPercap,
      yend = mean(lifeExp),
      color = "resid"
    )
  ) + 
  theme(
    legend.position = "none",
    text=element_text(
      family="gochi",
      size = 20
    )
  )

ggsave("life_exp_resid.png", life_exp_resid)
```

*Note that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the* [degrees of freedom](../describingData/dispersion.html).

Lets look at the variance of GDP per capita:

```{r}
gdp_resid <- ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap, 
    y=lifeExp
  )
) + 
  geom_point() +
  geom_vline(
    xintercept = mean(gapminder_2007$gdpPercap), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) +
  coord_cartesian(ylim = c(30, 85)) +
  xlab("GDP per capita") +
  ylab("Life Expectancy") +
  geom_segment(
    aes(
      xend = mean(gdpPercap),
      yend = lifeExp,
      color = "resid"
    )
  ) + theme(
    legend.position = "none",
    text=element_text(
      family="gochi",
      size = 20
    )
  )
ggsave("gdp_resid.png", gdp_resid)
gdp_resid
```

*Note that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the* [degrees of freedom](../describingData/dispersion.html).

## Total variance

A correlation aims to explain how much of the $total$ $variance$ is explained by the overlapping variance between the x and y axes. So we need to capture the $total$ $variance$ separately for the x and y axes. We do this by multiplying the variance for $x$ by the variance for $y$ (and square rooting to control for the multiplication itself):

$$
totalVariance = \sqrt{\frac{\sum(x_i-\bar{x})^2}{N-1}}*\sqrt{\frac{\sum(y_i-\bar{y})^2}{N-1}}
$$

(Which is the same as:

$$
totalVariance = \frac{\sqrt{\sum(x_i-\bar{x})^2*\sum(y_i-\bar{y})^2}}{N-1}
$$

)

Or, to use the figures above:

+---------------------+-------------------------------------------------------------+-----------+---------------------------------------------------------+
| \$Total\$ \$Var\$ = | sqrt(![](life_exp_resid.png) \\\^2) \$\$ \\frac{}{N-1} \$\$ | \$\*\$    | sqrt(![](gdp_resid.png) \\\^ 2) \$\$ \\frac{}{N-1} \$\$ |
+---------------------+-------------------------------------------------------------+-----------+---------------------------------------------------------+

This is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other.

### Shared variance between $x$ and $y$

An important thing to note, is that variance of a single variable, in this case x:

$$
var_x = \frac{\sum(x_i-\bar{x})^2}{N-1}
$$

could also be written as:

$$
var_x = \frac{\sum(x_i-\bar{x})(x_i-\bar{x})}{N-1}
$$

To look at the amount that x and y vary together, we can adapt a formula for how much $x$ varies (with itself as written above) to now look at how much $x$ varies with $y$:

$$
var_{xy} = \frac{\sum(x_i-\bar{x})(\color{red}{y_i-\bar{y}})}{N-1}
$$

This can be visualised as the residuals from the means multiplied by each other for each data point:

```{r}
shared_resid <- ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap, 
    y=lifeExp
  )
) + 
  geom_point() +
  geom_vline(
    xintercept = mean(gapminder_2007$gdpPercap), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) +
  coord_cartesian(ylim = c(30, 85)) +
  xlab("GDP per capita") +
  ylab("Life Expectancy") +
  geom_segment(
    aes(
      xend = mean(gdpPercap),
      yend = lifeExp,
      color = "GDP residuals"
    )
  ) +
  geom_segment(
    aes(
      xend = gdpPercap,
      yend = mean(lifeExp),
      color = "Life Expectancy Residuals"
    )
  ) +
  geom_hline(
    yintercept = mean(gapminder_2007$lifeExp), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) + theme(
    legend.position = "none",
    text=element_text(
      family="gochi"
    )
  )
ggsave("shared_resid.png", shared_resid) 
shared_resid
```

### Comparing $shared$ $variance$ ($var_{xy}$) to $total$ $variance$

To complete a **Pearson's** **R** correlation we need to compare the amount that x and y vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the **proportion** of $total$ $variance$ is explained by the $shared$ $variance$ ($var_{xy}$).

$$
\frac{var_{xy}}{totalVariance} = \frac{(\sum(x_i-\bar{x})(y_i-\bar{y}))/(N-1)}
                                      {\sqrt{\sum(x_i-\bar{x})^2*\sum(y_i-\bar{y})^2}/(N-1)}
$$

Note that both $var_{xy}$ and $totalVariance$ correct for the degrees of freedom, so the $N-1$s cancel each other out:

$$
r = \frac{var_{xy}}{totalVariance} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}
                                      {\sqrt{\sum(x_i-\bar{x})^2*\sum(y_i-\bar{y})^2}}
$$

Lets apply this to the gapminder data above to calculate $r$:

```{r}
varxy = 
  sum(
    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * 
    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))
   )


totalvar = sqrt(
  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * 
  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)
)
varxy/totalvar
```

If the above calculation is correct, we'll get exactly the same value when using the **cor.test** function:

```{r}
cor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)
```

To visualise what proportion of the variance is captured by $var_{xy}$:

+:------------------------------------------------------------------------:+
| ![](shared_resid.png)                                                    |
+--------------------------------------------------------------------------+
| ------------------------------------------------------------------------ |
+--------------------------------------------------------------------------+
| sqrt(![](life_exp_resid.png) \\\^2                                       |
+--------------------------------------------------------------------------+

A question you might have at this point, is whether the above figure seems consistent with 67.9% of $total$ $variance$ being explained by overlapping variance between $x$ and $y$?

If $x$ and $y$ vary together, then you would expect either:

-   a higher $x$ data point should be associated with a **higher** $y$ data point (positive association)

-   a higher $x$ data point should be associated with a **lower** $y$ data point (negative association)

The bottom half of the figure above doesn't give you that much insight into how consistently $x$ and $y$ are positive or negatively associated with each other, but the top half does;

```{r}
shared_resid
```

If there is a *positive association*, then you would expect there to be consistency in $x$ and $y$ both being above their own respective means, or both being below their respective means consistently, which is what we see above.

If there is a negative association, you would expect $y$ to generally be below its mean when $x$ is above its mean, and vice-versa. Lets visualise this by [transforming](../distributions/transforming.html){target="_blank"}the $life$ $expectancy$ to be inverted by subtracting it from 100. This will make younger people older and older people younger:

```{r}
inverted_resid <- ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap, 
    y=125-lifeExp
  )
) + 
  geom_point() +
  geom_vline(
    xintercept = mean(gapminder_2007$gdpPercap), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) +
  coord_cartesian(ylim = c(30, 85)) +
  xlab("GDP per capita") +
  ylab("Life Expectancy") +
  geom_segment(
    aes(
      xend = mean(gdpPercap),
      yend = 125-lifeExp,
      color = "GDP residuals"
    )
  ) +
  geom_segment(
    aes(
      xend = gdpPercap,
      yend = mean(125-lifeExp),
      color = "Life Expectancy Residuals"
    )
  ) +
  geom_hline(
    yintercept = mean(125-gapminder_2007$lifeExp), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) + theme(
    legend.position = "none",
    text=element_text(
      family="gochi"
    )
  )

inverted_resid
```

The $Pearson's$ $r$ is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there's consistency in the above average $x$ values being associated with below average $y$ values, and vice-versa.

You may have noticed that the data above looks like it's not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman's Rho (AKA Spearman's Rank) instead:

```{r}
# Skewness and kurtosis and their standard errors as implement by SPSS
#
# Reference: pp 451-452 of
# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf
# 
# See also: Suggestion for Using Powerful and Informative Tests of Normality,
# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,
# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321

spssSkewKurtosis=function(x) {
  w=length(x)
  m1=mean(x)
  m2=sum((x-m1)^2)
  m3=sum((x-m1)^3)
  m4=sum((x-m1)^4)
  s1=sd(x)
  skew=w*m3/(w-1)/(w-2)/s1^3
  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )
  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)
  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )

  ## z-scores added by reading-psych
  zskew = skew/sdskew
  zkurtosis = kurtosis/sdkurtosis

  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,
        dimnames=list(c("skew","kurtosis"), c("estimate","se","zScore")))
  return(mat)
}
spssSkewKurtosis(gapminder_2007$gdpPercap)
spssSkewKurtosis(gapminder_2007$lifeExp)
```

As GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 \* their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is *not* normally distributed, and thus we can/should complete a Spearman's Rank/Rho correlation (in the next subsection).

## Spearman's Rank (AKA Spearman's Rho)

**Spearman's Rank** correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because **ranks** are not vulnerable to outlier (i.e. unusually extreme) data points. Let's now turn the gapminder data we've been working with above into ranks and then run a Pearson's correlation on it to confirm this:

```{r}
gapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)
gapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)

```

Lets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:

```{r}

spssSkewKurtosis(gapminder_2007$gdpPercap_rank)
spssSkewKurtosis(gapminder_2007$lifeExp_rank)
```

This has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be **over** sensitive to identifying significant effects (see <a href="../distributions/kurtosis.qmd" target="_blank">kurtosis</a>), i.e. be at a higher risk of **false positives**. This is evidence that using a Spearman's Rank may increase a risk of a false-positive (at least with this data), so <a href="transforming" target="_blank">another transformation of the data</a> may be more appropriate to avoid this problem with kurtosis.

For now, lets focus on how much of the variance in ranks is explained in the overlap in variance of $gdp$ and $life$ $expectancy$ ranks:

```{r}
# Pearson correlation on **ranked** data:
cor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = "pearson")
# Spearman correlation applied to original data (letting R do the ranking)
cor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = "spearman")
```

The $r$ value is now .857, suggesting that the overlap between $gdp$ and $life$ $expectancy$ explains 85.7% of the total variance of the ranks for both of them.

Lets visualise this using similar principles above on the **ranks** of $gdp$ and $life$ $expectancy$:

```{r}
rank_resid <- ggplot(
  data = gapminder_2007, 
  aes(
    x=gdpPercap_rank, 
    y=lifeExp_rank
  )
) + 
  geom_point() +
  geom_vline(
    xintercept = mean(gapminder_2007$gdpPercap_rank), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) +
  #coord_cartesian(ylim = c(30, 85)) +
  xlab("GDP per capita (RANK)") +
  ylab("Life Expectancy (RANK)") +
  geom_segment(
    aes(
      xend = mean(gdpPercap_rank),
      yend = lifeExp_rank,
      color = "GDP residuals"
    )
  ) +
  geom_segment(
    aes(
      xend = gdpPercap_rank,
      yend = mean(lifeExp_rank),
      color = "Life Expectancy Residuals"
    )
  ) +
  geom_hline(
    yintercept = mean(gapminder_2007$lifeExp_rank), 
    linetype   = "dashed",  
    color      = "#006599", 
    size       = 1
  ) + theme(
    legend.position = "none",
    text=element_text(
      family="gochi",
      size = 20
    )
  )

rank_resid
```

You may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):

```{r}
shared_resid
```
