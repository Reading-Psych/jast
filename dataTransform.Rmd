---
title: "Data transformation"
output: html_notebook
---

# Describing data (AKA univariate statistics)

**Univariate statistics** is a term for statistics that describes a numerical and continuous variable. We will be describing the data along the following parameters:

- central tendency (mean and median)
- dispersion (range, variance and standard deviation)
- distribution (skewness and kurtosis)


## Central tendancy
Central tendancy describes typical values of a variable, such as it's mean and median.

### Mean vs. Median
The **mean** is often called the "average" informally, but is actually a specific type of "average". The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we'll re-use the gapminder data referred to above:

```{r}
gapminder_2007                                               # a reminder of the data frame
sum_life_expectancy  = sum(gapminder_2007$lifeExp)           # total of all years
n_life_expectancy    = length(gapminder_2007$lifeExp)        # count the people
mean_life_expectancy = sum_life_expectancy/n_life_expectancy 
mean_life_expectancy
```
For those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:

```{r}
mean(gapminder_2007$lifeExp)
```
*Whew* - it's the same as the manual calculation above. 

Now **median** is less known than **mean**, but median is the value in the middle once you order all your data. It's well explained in the first paragraph on wikipedia: <a href="https://en.wikipedia.org/wiki/Median" target="_blank">https://en.wikipedia.org/wiki/Median</a>, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):

```{r}
mean(gapminder_2007$lifeExp)
median(gapminder_2007$lifeExp)
```

## Dispersion
To understand distribution below, it's helpful to clarify some more basic concepts around how data is **dispersed** around the mean. 

**Range** simply captures the min and the maximum values.


## Distribution
Lets just imagine we want to summarise the life expectancy of people across the world. Using the gapminder [https://www.gapminder.org/] dataset that is freely available about the world, we can look at the distribution of life expectancies across countries in 2007:

```{r}
# install (if required) and load the gapminder data
if(!require(gapminder)){install.packages("gapminder")}
library(gapminder)

# create a new data frame that only focuses on data from 2007
gapminder_2007 <- subset(
  gapminder,   # the data set
  year == 2007     
)

# lets have a look at the data
gapminder_2007

# and now plot a histogram of the life expectancy data to see the distribution
hist(gapminder_2007$lifeExp)
```
So whilst this histogram visualises the distribution of the data, it's helpful to be able to **statistically quantify** (i.e. give numbers that represent) this distribution. Remember that for many statistical tests (parmatric) it's helpful to have data that is **normally distributed**. Normally distributed data (also known as **Gaussian distributions** and associated with **parametric statistics**) generally looks like a bell curve:

```{r}
# generating random data to fit a normal distribution.
norm_data <-rnorm(
  100000,               # the number of samples
  mean=0,               # the mean (which in this case should be almost identical to the median)
  sd=1                  # the standard deviation
)
hist(
  norm_data,
  breaks = 100,           # the number of bins (the more bins there are, the thinner each bin is)
  xlim   = c(-3,3),       # the range of the x-axis
  freq   = FALSE,         # changes the y-axis from reporting the absolute frequency to reporting the relative density
  xlab   = "SDs", 
  main   = "Normal distribution (pseudo-randomly generated)"
)
```
So we can see when comparing the shape of life expectancies and the shape of a normal distribution that the two aren't the same. There are 3 things we'll look for to *quantify* how much of a problem we have with our current distribution:

- skewness
- kurtosis

### Skewness

Data can be negatively skewed, where the mean is less than the median:

```{r}
# Skewed to the right
negative_skew = rbeta(10000,5,2)
hist(negative_skew, main = "Negatively skewed data")
abline(
  v=mean(negative_skew),  # where the line for the mean will be 
  lwd=5
)
abline(
  v=median(negative_skew), 
  lwd=3,
  lty=3
)
```
*The thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see * **central tendancy** *below)*

Or positively skewed, where the median is less than the mean:
```{r}

# Skewed to the left
positive_skew = rbeta(10000,2,5)
hist(positive_skew, main="Positively skewed data")
abline(
  v=mean(positive_skew),  # where the line for the mean will be 
  lwd=5
)
abline(
  v=median(positive_skew), # where the line for the median will be
  lwd=3,
  lty=3
)
```
*The thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see central tendancy below)*

So now that we know what skewed distributions look like, we now need to **quantify** how much of a problem with skewness there is. If we add together the amount of skewness for each data point together and then divide by a general summary of the total standard deviation, then you get an estimate of skewness. The next section is a breakdown of the formula for those interested in it (but this is *not crucial*), but **the key point point is that outliers skew the data, and so outliers that are larger than the mean positively skew the data, and outliers below the mean negatively skew it. If there are an equal number of outliers on either side of the mean then the data will not be skewed.**

<div class="optional_content">
#### Optional
If we want to manually calculate skewness:

$$
\tilde{\mu_{3}} = \sum((\frac{x_i- \bar{x}{}} {\sigma} )^3) * \frac{N}{(N-1) * (N-2)}
$$
To do this in R you *could* calculate it manually
```{r}
# applying this to the positive skew data above
positive_skew_n = length(positive_skew)
positive_skewness = sum(((positive_skew - mean(positive_skew))/sd(positive_skew))^3) * (
  positive_skew_n / ((positive_skew_n-1) * (positive_skew_n - 2))
)
# to show the output:
positive_skewness
```


... or just 
</div>
use code from https://stackoverflow.com/a/54369572 to give you values for skewness (this has been chosen as this gives skewness and its standard error as calculated by major software like SPSS and JASP):
```{r}

# Skewness and kurtosis and their standard errors as implement by SPSS
#
# Reference: pp 451-452 of
# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf
# 
# See also: Suggestion for Using Powerful and Informative Tests of Normality,
# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,
# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321

spssSkewKurtosis=function(x) {
  w=length(x)
  m1=mean(x)
  m2=sum((x-m1)^2)
  m3=sum((x-m1)^3)
  m4=sum((x-m1)^4)
  s1=sd(x)
  skew=w*m3/(w-1)/(w-2)/s1^3
  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )
  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)
  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )
  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis), 2,
        dimnames=list(c("skew","kurtosis"), c("estimate","se")))
  return(mat)
}
spssSkewKurtosis(positive_skew)

```

However, this value is not very meaningful by itself to confirm whether there's a significant problem with skewness. To address this, lets use the skewness standard error (see above), to calculate a **z-score**. **Z-scores** can capture how *significant* a value is, i.e. how unlikely the number is considering what you would normally expect. In this case, when you divide skewness by standard error, you get a z-score, and if the **absolute value** (i.e. ignoring whether it is positive or negative) of the z-score is greater than 1.96 then it is significantly skewed. 

<div class="optional_content">
#### Optional

##### Why is 1.96 (sometimes) the threshold for "significance" with z-values?

**Parametric statistics** often compare values to a **normal** distribution of **expected results**, based on the **estimated mean and SD**. For example, lets imagine that a man called John thinks he's either unusually tall, or unusally short, but he isn't sure which. He could compare his height to a normal distribution of heights, and see whether he's in the top or bottom 2.5%. 

```{r}
# Plot a normal distribution of heights
z_score_x <- seq(
  -3,    # min
  3,    # max
  by = .1  
)
z_score_y <- dnorm(
  z_score_x,
  mean = 0,
  sd   = 1
)
plot(
  z_score_x,
  z_score_y
)
# Add line to show mean
abline(
  v=0,  # where the line for the mean will be 
  lwd=3
)
# Add line to show lower threshold
abline(
  v= -1.96,
  lwd=3,
  lty=3
)
# Add line to show higher threshold
abline(
  v= 1.96,
  lwd=3,
  lty=3
)

```

# draw red blocks to highlight the values less than -1.96 and greater than 1.96


So lets say he's 170cm tall, the average person is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:

```{r}
# Plot a normal distribution of heights
population_heights_x <- seq(
  140,    # min
  200,    # max
  by = 1  
)
population_heights_y <- dnorm(
  population_heights_x,
  mean = 150,
  sd   = 10
)
plot(
  population_heights_x,
  population_heights_y
)
# Add line to show mean
abline(
  v=150,  # where the line for the mean will be 
  lwd=5
)
# Add line to show John's height
abline(
  v=170, 
  lwd=3,
  lty=3
)
```
*The thick line refers to the mean height, the dashed line refers to John's height*

You can see above that John is pretty unusually tall. To work out whether he's in the top 5%, lets work out his z-score:

```{r}
john_height <- 170
mean_height <- 150
height_sd   <- 10
john_z_score <- (john_height - mean_height)/height_sd
john_z_score
```

So based on the above, John's Z-score is greater than 1.96, which I claimed above was meaningful. To understand why this is meaningful, lets start by plotting John's score as a z-score:

```{r}
# Plot a normal distribution of heights
z_score_x <- seq(
  -1,    # min
  5,    # max
  by = .1  
)
z_score_y <- dnorm(
  z_score_x,
  mean = 0,
  sd   = 1
)
plot(
  z_score_x,
  z_score_y
)
# Add line to show mean
abline(
  v=0,  # where the line for the mean will be 
  lwd=3
)
# Add line to show John's height
abline(
  v=john_z_score,
  lwd=3,
  lty=3
)

```

As you may have noticed, these figures are identical in their distributions and the relative locations of the vertical lines. So when you have calculated a **z-score** you have calculated a standardised value of how many standard deviations you are above (or below) the mean. People have previously calculated that if you get a z-value of 1.96 and above then you are in the top 5 percentile. However, lets double check that using the pnorm function:

```{r}
# compare the values below with the plot of the z_score plot
pnorm(
  q = 1.96, # put in the z-value here
  mean = 0,
  sd = 1,
  lower.tail = F
)

```

The above p-value of .024998 shows that any Z-score above  



Note that how you ask a question **greatly** impacts your approach, and so my claim that I'm in the top 5% of the population is very different to a more general claim I'm taller than the average person. So whether I'm in the top 5% of the population is a very 

you can calculate the *standard error of skewness* as below:

```{r}
# 
# sqrt((6*positive_skew_n*(positive_skew_n-1))/((positive_skew_n-2)*(positive_skew_n+1)*(positive_skew_n+3)))
# 
# 
# 
# 
# positive_skew_n = length(positive_skew)
# 
# 
# gini_n = length(gini_clean)
# 
# gini_skewness = sum(((gini_clean - mean(gini_clean))/sd(gini_clean))^3) * (
#   gini_n / ((gini_n-1) * (gini_n - 2))
# )
# 
# pos_skew_ses = sqrt((6*positive_skew_n*(positive_skew_n-1))/((positive_skew_n-2)*(positive_skew_n+1)*(positive_skew_n+3)))
# 
# spssSkewKurtosis=function(x) {
#   w=length(x)
#   m1=mean(x)
#   m2=sum((x-m1)^2)
#   m3=sum((x-m1)^3)
#   m4=sum((x-m1)^4)
#   s1=sd(x)
#   skew=w*m3/(w-1)/(w-2)/s1^3
#   sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )
#   kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)
#   sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )
#   mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis), 2,
#         dimnames=list(c("skew","kurtosis"), c("estimate","se")))
#   return(mat)
# }
# 
# skewness(ineq_data$Gini)
# kurtosis(ineq_data$Gini)
# skewness(gini_clean)
# describe(gini_clean)
# gini_n = length(gini_clean)
# sum((gini_clean - mean(gini_clean))^3)/
#   (gini_n-1) * (gini_n-2) * sd(gini_clean)^3)
# #sum(((gini_clean - mean(gini_clean))/sd(gini_clean))^3) * (1/length(gini_clean)
```

</div>
### kurtosis






# Transforming data