[
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency",
    "section": "",
    "text": "Mean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\nLoading required package: gapminder\n\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n  \n\n\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n# a reminder of the data frame\ngapminder_2007\n\n# total of all years\n\n                 country continent  year  lifeExp       pop     gdpPercap\n11           Afghanistan      Asia  2007   43.828  31889923    974.580338\n23               Albania    Europe  2007   76.423   3600523   5937.029526\n35               Algeria    Africa  2007   72.301  33333216   6223.367465\n47                Angola    Africa  2007   42.731  12420476   4797.231267\n59             Argentina  Americas  2007   75.320  40301927  12779.379640\n...                  ...       ...   ...      ...       ...           ...\n1655             Vietnam      Asia  2007   74.249  85262356   2441.576404\n1667  West Bank and Gaza      Asia  2007   73.422   4018332   3025.349798\n1679         Yemen, Rep.      Asia  2007   62.698  22211743   2280.769906\n1691              Zambia    Africa  2007   42.384  11746035   1271.211593\n1703            Zimbabwe    Africa  2007   43.487  12311143    469.709298\n\n[142 rows x 6 columns]\n\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n67.00742253521126\n\n\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n67.00742253521126\n\n\n\n\n\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n67.00742253521126\n\ngapminder_2007['lifeExp'].median()\n\n71.93549999999999\n\n\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\ngapminder_2007['lifeExp'].mode()\n\n0      39.613\n1      42.082\n2      42.384\n3      42.568\n4      42.592\n        ...  \n137    81.235\n138    81.701\n139    81.757\n140    82.208\n141    82.603\nName: lifeExp, Length: 142, dtype: float64\n\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\nlength(gapminder_2007$lifeExp)\n\n[1] 142\n\nlength(unique(gapminder_2007$lifeExp))\n\n[1] 142\n\n\n\n\n\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\n\n142\n\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n142\n\n\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n[1] 2 4\n\n\n\n\n\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n   0\n0  2\n1  4\n\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most."
  },
  {
    "objectID": "describingData/describingData.html",
    "href": "describingData/describingData.html",
    "title": "Describing Data",
    "section": "",
    "text": "Univariate statistics is a term for statistics that describes a numerical and continuous variable. We will be describing the data along the following parameters:\n\ncentral tendency (mean and median)\ndispersion (range, variance and standard deviation)\ndistribution (skewness and kurtosis)"
  },
  {
    "objectID": "describingData/dispersion.html",
    "href": "describingData/dispersion.html",
    "title": "Dispersion",
    "section": "",
    "text": "To understand distribution (later), it’s helpful to clarify some more basic concepts around how data is dispersed or spread."
  },
  {
    "objectID": "describingData/dispersion.html#range",
    "href": "describingData/dispersion.html#range",
    "title": "Dispersion",
    "section": "Range",
    "text": "Range\nRange simply captures the min and the maximum values. Lets look at the min and max for the life expectancy data from 2007:\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmin(gapminder_2007$lifeExp)\n\n[1] 39.613\n\nmax(gapminder_2007$lifeExp)\n\n[1] 82.603\n\n\nSo the range for life expectancy in 2007 was between 39.613 and 82.603."
  },
  {
    "objectID": "describingData/dispersion.html#variance",
    "href": "describingData/dispersion.html#variance",
    "title": "Dispersion",
    "section": "Variance",
    "text": "Variance\n\nPopulation Variance\nVariance is how much the data varies around a mean. To capture this, we compare each individual’s score with the mean, so lets do this with our gapminder data’s life expectancy:\n\nlife_expectancy_variance_table <- data.frame(\n  life_expectancy = gapminder_2007$lifeExp,\n  diff_from_mean  = gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp)\n)\n\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\nSo we know for each country how different their life expectacy is to the mean life expectancy. But ideally we would like a single value to summarise variance. Lets see what would happen if we tried to summarise these differences from the mean by calculating the mean difference from the mean:\n\nmean(life_expectancy_variance_table$diff_from_mean) \n\n[1] 5.153937e-15\n\n\nWe get a number that is effectively zero (go here for an explanation about e-numbers), because all the values above the mean balance out those below the mean. So to address this, we can square the differences to force all the numbers to be positive:\n\nlife_expectancy_variance_table$diff_squared = life_expectancy_variance_table$diff_from_mean^2\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\nIf we calculate the average of this, then we get a summary of the variance that is more informative:\n\nmean(life_expectancy_variance_table$diff_squared)\n\n[1] 144.7314\n\n\nHowever, as mean is what you get when you add all the items together and then divide it by the number of items, this can also be done in 2 steps in R (this will help us understand the formula later):\n\nsum_of_squares = sum(life_expectancy_variance_table$diff_squared)\nthis_variance  = sum_of_squares/length(life_expectancy_variance_table$diff_squared)\nthis_variance\n\n[1] 144.7314\n\n\nWe can represent the above in the following formula for the population’s (remember, this is when you have everyone from the group you are measuring) variance:\n\\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N}\n\\]\nLet’s break down each of the above symbols: σ^2 is population variance Σ is sum xi refers to the value for each participant x̄ refers to the mean for all participants N refers to the number of participants\n(note that the above is written as if we’re looking at the variance of a group of participants, but the principles still work if looking at non-participant data)\n\n\nSample variance\nTo calculate the variance for a sample of participants, rather than every participant in the group you’re measuring, you need a slightly different formula:\n\\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N - 1}\n\\] Note that Sample variance is represented by S^2 rather than σ^2\nSo why do we divide by N-1 rather than N? This is because the sample variance is an estimate rather than the actual population variance. When estimating the population variance you take into account the actual number of people (N) in the sample, whereas when you are estimating what happens generally in the population based on your sample, you take into account the degrees of freedom (N-1). In broad terms this reduces the risk of you under-estimating the variance of the population. You don’t necessarily need to understand degrees of freedom beyond the idea that you are controlling for the fact that you are analysing a sample rather than the population they represent, so don’t worry if the next section isn’t completely clear."
  },
  {
    "objectID": "describingData/dispersion.html#degrees-of-freedom",
    "href": "describingData/dispersion.html#degrees-of-freedom",
    "title": "Dispersion",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom calculations can be useful to address statistics that are vulnerable to bias within a sample (i.e. the sample being distorted compared to the population). Interestingly, mean is not, but variance is. Lets see this by looking at differences between the population and sample for mean and variance, looking at the height of three people, and combining them into every combination of 2 people possible:\n\nthree_heights = c(150,160,170)\npopulation_height_mean = mean(three_heights)\npopulation_height_variance = sum((three_heights - population_height_mean)^2)/3\n\n#sample participants in pairs\nsample_heights = data.frame(\n  pp1 = c(150,150,NA),\n  pp2 = c(160,NA,160),\n  pp3 = c(NA,170,170),\n  pair = c(\n    \"1 and 2\",\n    \"1 and 3\",\n    \"2 and 3\"\n  )\n)\n\nsample_heights$mean = c(\n  mean(c(three_heights[1], three_heights[2])),\n  mean(c(three_heights[1], three_heights[3])),\n  mean(c(three_heights[2], three_heights[3]))\n)\n\nsample_heights$pop_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/3,\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/3,\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/3\n)\n\nsample_heights$sample_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/(3-1),\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/(3-1),\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/(3-1)\n)\n\n\nrmarkdown::paged_table(sample_heights)\n\n\n\n  \n\n\nmean_sample_mean <- mean(sample_heights$mean)  \nmean_sample_variance <- mean(sample_heights$sample_var)  \nmean_population_variance <- mean(sample_heights$pop_var)  \n\nWhen comparing the population mean to the mean sample mean (i.e., what is the typical mean for any sample), they’re identical (i.e. NOT biased):\n\npopulation_height_mean\n\n[1] 160\n\nmean_sample_mean\n\n[1] 160\n\n\nWhereas when comparing the actual population variance (population_height_variance) to the mean (to identify what is a typical) estimate of variance using the population formula that should not be used for samples (mean_population_variance) finds the estimate of variance is typically smaller than the actual variance in the population:\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_population_variance\n\n[1] 33.33333\n\n\nAs this bias (almost) always underestimates the population variance, degrees of freedom is a useful correction to address this within calculations of sample variance. Lets compare the actual population height variance (population_height_variance) to the mean estimate using degrees of freedom that should be used for samples (mean_sample_variance).\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_sample_variance\n\n[1] 50\n\n\nSo, not perfect, but this is less under-representative of the variance.\nOne thing to bear in mind is that calculation of some statistics does not require use of the degrees of freedom to correct for bias (as seen above, mean was not susceptible to bias).\nIf you would like to understand how degrees of freedom are determined, and what the thinking is behind this term, read on for a brief description of this (otherwise, feel free to skip to the next section).\nDegrees of freedom refers to how many values could change in your variable once you know what the outcome of the relevant statistic is. For example, if you’re interested in the variance of the height of the three people, then you only have 2 degrees of freedom, because once you know the height of 2 of the participants AND the variance of the height, then there the remaining participant only has a 2 possible heights (so their height isn’t free to change)."
  },
  {
    "objectID": "describingData/dispersion.html#standard-deviation-sd",
    "href": "describingData/dispersion.html#standard-deviation-sd",
    "title": "Dispersion",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is the square root of the variance. This takes into account that that the variance includes the square of the difference between the individual values and the mean:\nPopulation Variance \\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{})\\color{Red}{^2})} {N}\n\\]\nPopulation SD \\[\n\\sigma^2 = \\sqrt\\frac{\\sum((x_i- \\bar{x}{})\\color{Red}{^2})} {N}\n\\]\nSample Variance \\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )\\color{red}{^2})} {N - 1}\n\\] Sample SD \\[\nS^2 = \\sqrt\\frac{\\sum((x_i- \\bar{x}{} )\\color{red}{^2})} {N - 1}\n\\]"
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n  \n\n\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n!correct_vector == \"correct\" \n\n[1] FALSE  TRUE FALSE\n\n# which is the same as\n!(correct_vector == \"correct\")\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "rBasics/fundamentals.html",
    "href": "rBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "Often, you can use R as a calculator, for example, if you want to know what two people’s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nIt’s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g. how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. “my object” would be an invalid object name.\nObject names are case-sensitive, so if your object is called “MyObject” you cannot refer to it as “myobject”.\n“.” and “_” are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou can’t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing “t.te” and pressing the tab will give you “t.test”\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you don’t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe “c” above is short for “combine” as you’re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the “typeof” function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (“heights”) is a “double” vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers can’t always be integers. Strings are identified as “character” objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the “mpg” dataframe. To look at it, just type in it’s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below won’t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see what’s in the “manufacturer” column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about “intelligent copy and paste” to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming you’ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code we’ve copied so that you can apply it to your own scripts. There’s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\nggplot(             # R will keep looking at your code until all the open brackets have been closed)\n  mpg,              #\n  aes(              #\n    displ,          #\n    hwy,            #\n    colour = class  #\n  )\n) +                 # R will look to the next line if you end a line with +\ngeom_point()        #\n\n\n\n\nHere’s how I would comment it out:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    displ,          # x-axis\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\n\n\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    cty,            # x-axis - updated\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\n\n\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "rBasics/filetypes.html",
    "href": "rBasics/filetypes.html",
    "title": "Types of Scripts",
    "section": "",
    "text": "type it directly into the console (generally not recommended)\nsave it as a script (better) note that script can refer to any of the below, but in this case is being used to describe a script that doesn’t generate a notebook\nsave it as an R Markdown (better still) - this allows you to make beautiful documents\nsave it as an R Notebook (arguably better than R Markdown) - this allows you to make beautiful documents, and is quicker\n\n\nThe Console\nAt the bottom left of RStudio you should have a console that looks something like what’s highlighted in red below:\n\n\n\nconsole\n\n\nYou can type straight into the console, to get a result. You can scroll through your previous commands by pressing the up arrow in the console. Each time code is run in the console it updates the environment in the top right of R-Studio:\n\n\n\nenvironment\n\n\n\n\nScripts\nThe word “script” can be interpreted specifically, to refer to a type of R file that includes a lot of code, or generally to refer to any file that includes both R code and code that allows you make a nice looking report. In this subsection, we will be focusing on “script” as a particular type of file. To create a script, click on File -> New File -> R Script\n\n\n\nnewScript\n\n\nYou will then be shown a blank script, in which you can write a series of functions, and then run them. To run the lines of code, select a line, and then press CTRL-ENTER, or highlight a chunk of code and then press CTRL-ENTER. In either case, the code will be sent to the console and run there.\nAn advantage of a script over just using the console is that you can analyse your data in both structured and complex ways which is difficult if you are typing code directly into the console.\n\n\nR Markdown\nAs highlighted above, R Markdown is a type of “script” in the general sense of the word, but allows you to create beautiful .html notebooks (.html files are what internet pages are based on). You are in fact reading an example of what can be produced by R Markdown (and R Notebooks). To make an R Markdown file, click on File -> New File -> R Markdown. You will be asked for a title, author and what output you would like. I would suggest “first markdown”, your name and “html” as the respective answers. You should then see something like:\n\n\n\nmarkdown\n\n\nThe following points apply to both R Markdown and R Notebooks\nIf you look above, you may notice that there are 2 types of code: Markdown (to write a nice looking report) and R (in grey chunks). I think these are well explained here: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf so I’ll just explain that R Markdowns run all the code in the chunks each time they generate the output file (e.g. html file). This is important to know, because R Notebooks do not run all the code in all the chunks when you generate them (see below for more on this).\n\n\nR Notebooks\nR Notebooks can be created by clicking on File -> New File -> R Notebook. They look quite similar to R Markdowns, but automatically generate the .html output each time you save the notebook. The output file will be a .nb.html file in the same folder as your notebook.\nVery importantly - the .nb.html file will be built based on what happened the last time you run each R chunk. If you never ran the R Chunk, then the nb.html file will not use the output from that chunk. This makes R Notebooks quicker than R Markdowns, because you don’t have to generate the output from scratch each time, as it will just use whatever was generated the last time the chunk was run. However, this means that there’s a risk your .html output will not be what you expect if you failed to run all of your chunks before saving your file. To address this risk (when you’ve finished editing your file), you can select the “run all” option.\n\n\n\nrunAllChunks"
  },
  {
    "objectID": "installing.html",
    "href": "installing.html",
    "title": "Installing R(Studio)",
    "section": "",
    "text": "Installing RStudio\n\nhttps://www.rstudio.com/products/rstudio/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributors-in-alphabetical-order",
    "href": "index.html#contributors-in-alphabetical-order",
    "title": "About",
    "section": "Contributors (in alphabetical order)",
    "text": "Contributors (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBrady\nDan\nContributor\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nAuthor, Editor\n\n\nMathews\nImogen\nContributor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nAuthors have written (sub)sections\nEditors have managed the formatting of this website/textbook\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics",
    "section": "",
    "text": "In statistics, a variable is any (measurable) attribute that describes any organism or object. It’s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object.\nWhat is a hypothesis? A(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e. randomly). We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\nIt is the p-value threshold that identifies if a result is “significant” or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is “significant” (i.e. so unlikely that this would have happened by chance that we conclude this didn’t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e. we say there is an effect when there isn’t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Note: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n3.1e3 - 3.1 * 10^3\n\n[1] 0\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n2.5e-3 - 2.5 * 10^(-3)\n\n[1] 0"
  },
  {
    "objectID": "templates/ojs.html",
    "href": "templates/ojs.html",
    "title": "ojs",
    "section": "",
    "text": "Note that you won’t be able to preview figures created using this\n\ndata = FileAttachment(\"palmerpenguins.csv\").csv({ typed: true })\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\nathletes = FileAttachment(\"athletes.csv\").csv({typed: true})\n\n\ndotplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndotplot = Plot.dot(athletes, {x: \"weight\", y: \"height\", stroke: \"sex\"}).plot()\n\n\n//dotplot.legend(\"color\")\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"bill_depth_mm\", stroke: \"species\"}).plot()\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"count\", stroke: \"sex\"}).plot()\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  )\n).plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "templates/tabsets.html",
    "href": "templates/tabsets.html",
    "title": "tabsets template",
    "section": "",
    "text": "RPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n\n1 + 2\n\n3"
  },
  {
    "objectID": "distributions/skewness.html",
    "href": "distributions/skewness.html",
    "title": "Skewness (incomplete)",
    "section": "",
    "text": "Parametric analyses are based on the assumption that the data you are analysing is normally distributed (see below):\nNote that “Z-scores” above represent NO LONGER standard deviations from the mean in this leptokurtic distribution\nIf your data fits a normal distribution, then you can draw conclusions based on certain facts about this distribution, e.g. the fact that 97.7% of your population should have a score that is more negative than +2 standard deviations above the mean (because Z-scores represent standard deviations from the mean). As a result, if your data is positively skewed:\nor negatively skewed\nSo whilst this histogram visualises the distribution of the data, it’s helpful to be able to statistically quantify (i.e. give numbers that represent) this distribution. Remember that for many statistical tests (parametric) it’s helpful to have data that is normally distributed. Normally distributed data (also known as Gaussian distributions and associated with parametric statistics) generally looks like a bell curve:\nSo we can see when comparing the shape of life expectancies and the shape of a normal distribution that the two aren’t the same. There are 3 things we’ll look for to quantify how much of a problem we have with our current distribution:"
  },
  {
    "objectID": "distributions/skewness.html#skewness",
    "href": "distributions/skewness.html#skewness",
    "title": "Skewness (incomplete)",
    "section": "Skewness",
    "text": "Skewness\nData can be negatively skewed, where the mean is less than the median:\n\n# Skewed to the right\nnegative_skew = rbeta(10000,5,2)\nhist(negative_skew, main = \"Negatively skewed data\")\nabline(\n  v=mean(negative_skew),  # where the line for the mean will be \n  lwd=5\n)\nabline(\n  v=median(negative_skew), \n  lwd=3,\n  lty=3\n)\n\n\n\n\nThe thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see central tendancy below)\nOr positively skewed, where the median is less than the mean:\n\n# Skewed to the left\npositive_skew = rbeta(10000,2,5)\nhist(positive_skew, main=\"Positively skewed data\")\nabline(\n  v=mean(positive_skew),  # where the line for the mean will be \n  lwd=5\n)\nabline(\n  v=median(positive_skew), # where the line for the median will be\n  lwd=3,\n  lty=3\n)\n\n\n\n\nThe thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see central tendancy below)\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is. If we add together the amount of skewness for each data point together and then divide by a general summary of the total standard deviation, then you get an estimate of skewness. The next section is a breakdown of the formula for those interested in it (but this is not crucial), but the key point point is that outliers skew the data, and so outliers that are larger than the mean positively skew the data, and outliers below the mean negatively skew it. If there are an equal number of outliers on either side of the mean then the data will not be skewed.\n\nOptional\nIf we want to manually calculate skewness:\n\\[\n\\tilde{\\mu_{3}} = \\sum((\\frac{x_i- \\bar{x}{}} {\\sigma} )^3) * \\frac{N}{(N-1) * (N-2)}\n\\] To do this in R you could calculate it manually\n\n# applying this to the positive skew data above\npositive_skew_n = length(positive_skew)\npositive_skewness = sum(((positive_skew - mean(positive_skew))/sd(positive_skew))^3) * (\n  positive_skew_n / ((positive_skew_n-1) * (positive_skew_n - 2))\n)\n# to show the output:\npositive_skewness\n\n[1] 0.5831444\n\n\n… or just\n\nuse code from https://stackoverflow.com/a/54369572 to give you values for skewness (this has been chosen as this gives skewness and its standard error as calculated by major software like SPSS and JASP):\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\")))\n  return(mat)\n}\nspssSkewKurtosis(positive_skew)\n\n           estimate         se\nskew      0.5831444 0.02449122\nkurtosis -0.1509320 0.04897755\n\n\nHowever, this value is not very meaningful by itself to confirm whether there’s a significant problem with skewness. To address this, lets use the skewness standard error (see above), to calculate a z-score. Z-scores can capture how significant a value is, i.e. how unlikely the number is considering what you would normally expect. In this case, when you divide skewness by standard error, you get a z-score, and if the absolute value (i.e. ignoring whether it is positive or negative) of the z-score is greater than 1.96 then it is significantly skewed. If you want to understand why 1.96 is the main number, check out the subsection on normal distributions.\nyou can calculate the standard error of skewness as below:\n\n# \n# sqrt((6*positive_skew_n*(positive_skew_n-1))/((positive_skew_n-2)*(positive_skew_n+1)*(positive_skew_n+3)))\n# \n# \n# \n# \n# positive_skew_n = length(positive_skew)\n# \n# \n# gini_n = length(gini_clean)\n# \n# gini_skewness = sum(((gini_clean - mean(gini_clean))/sd(gini_clean))^3) * (\n#   gini_n / ((gini_n-1) * (gini_n - 2))\n# )\n# \n# pos_skew_ses = sqrt((6*positive_skew_n*(positive_skew_n-1))/((positive_skew_n-2)*(positive_skew_n+1)*(positive_skew_n+3)))\n# \n# spssSkewKurtosis=function(x) {\n#   w=length(x)\n#   m1=mean(x)\n#   m2=sum((x-m1)^2)\n#   m3=sum((x-m1)^3)\n#   m4=sum((x-m1)^4)\n#   s1=sd(x)\n#   skew=w*m3/(w-1)/(w-2)/s1^3\n#   sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n#   kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n#   sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n#   mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis), 2,\n#         dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\")))\n#   return(mat)\n# }\n# \n# skewness(ineq_data$Gini)\n# kurtosis(ineq_data$Gini)\n# skewness(gini_clean)\n# describe(gini_clean)\n# gini_n = length(gini_clean)\n# sum((gini_clean - mean(gini_clean))^3)/\n#   (gini_n-1) * (gini_n-2) * sd(gini_clean)^3)\n# #sum(((gini_clean - mean(gini_clean))/sd(gini_clean))^3) * (1/length(gini_clean)"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "library(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)/2\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n   annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\nor underly clustered around the mean (platykurtik)\nLets just imagine we want to summarise the life expectancy of people across the world. Using the gapminder [https://www.gapminder.org/] dataset that is freely available about the world, we can look at the distribution of life expectancies across countries in 2007:\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# lets have a look at the data\ngapminder_2007\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>     <int>     <dbl>\n 1 Afghanistan Asia       2007    43.8  31889923      975.\n 2 Albania     Europe     2007    76.4   3600523     5937.\n 3 Algeria     Africa     2007    72.3  33333216     6223.\n 4 Angola      Africa     2007    42.7  12420476     4797.\n 5 Argentina   Americas   2007    75.3  40301927    12779.\n 6 Australia   Oceania    2007    81.2  20434176    34435.\n 7 Austria     Europe     2007    79.8   8199783    36126.\n 8 Bahrain     Asia       2007    75.6    708573    29796.\n 9 Bangladesh  Asia       2007    64.1 150448339     1391.\n10 Belgium     Europe     2007    79.4  10392226    33693.\n# … with 132 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n# and now plot a histogram of the life expectancy data to see the distribution\nhist(gapminder_2007$lifeExp)"
  },
  {
    "objectID": "distributions/transforming.html",
    "href": "distributions/transforming.html",
    "title": "Transforming Data",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "distributions/transforming.html#running-code",
    "href": "distributions/transforming.html#running-code",
    "title": "Transforming Data",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "distributions/normal.html",
    "href": "distributions/normal.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "Parametric statistics often compare values to a normal distribution of expected data, based on the estimated mean and SD. Lets start by showing a (made up) normal distribution of heights in centimeters:\nSo lets say the average person’s height is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:\n\n# Plot a normal distribution of heights\npopulation_heights_x <- seq(\n  120,    # min\n  180,    # max\n  by = 1  \n)\npopulation_heights_y <- dnorm(\n  population_heights_x,\n  mean = 150,\n  sd   = 10\n)\nplot(\n  population_heights_x,\n  population_heights_y,\n  xlab = \"height\",\n  ylab = \"frequency\"\n)\n# Add line to show mean and median\nabline(\n  v=150,  # where the line for the mean will be \n  lwd=5\n)\n\n\n\n\nYou can see that the above fits a bell-curve, and the line in the middle represents both the mean and the median as the data is symmetrical. In reality, almost no data is a perfect bell-curve, but there are ways to test if the data isn’t sufficiently normal to use parametric tests with.\nNext, we will look at how normal distributions allow you to transform your data to z-scores to compare to a z-distribution."
  },
  {
    "objectID": "distributions/normal.html#z-scores-and-the-z-distribution",
    "href": "distributions/normal.html#z-scores-and-the-z-distribution",
    "title": "Normal Distribution",
    "section": "Z-scores and the z-distribution",
    "text": "Z-scores and the z-distribution\nA z-score is a standardised value that captures how many standard deviations above or below the mean an individual value is. Thus, to calculate the z-score\n\\[\nZ = \\frac{individualScore-averageScore}{StandardDeviation}\n\\]\nOr in formal terminology:\n\\[\nZ = \\frac{x-\\bar{x}}{\\sigma}\n\\]\nThe calculated score can then be applied to a z-distribution, which is parametric/normally distributed. Lets have a look at a z-distribution:\n\n# vector for the x-axis\nz_score_x <- seq(\n  -3,    # min\n  3,    # max\n  by = .1  \n)\n\n# vector for the y-axis\nz_score_y <- dnorm(\n  z_score_x,\n  mean = 0,\n  sd   = 1\n)\n\nplot(\n  z_score_x,\n  z_score_y,\n  xlab = \"z-score (SDs from the mean)\",\n  ylab = \"frequency\"\n)\n\n\n\n\nIf you compare the height distribution above to the z-score distribution, you should see that they are identically distributed. This is useful, as we know what percentage of a population fits within each standard deviation of a normal distribution:\n\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_13.6 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   \n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6, \n     aes(\n       x, \n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   \n   annotate(\n     \"text\", \n     x=0.5, \n     y=0.01, \n     label= \"34.1%\"\n   ) + \n   annotate(\n     \"text\", \n     x=1.5, \n     y=0.01, \n     label= \"13.6%\"\n   ) + \n   annotate(\n     \"text\", \n     x=2.3, \n     y=0.01, \n     label= \"2.3%\"\n   ) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\nThe above visualises how 34.1% of a population’s scores will be between 0 and 1 standard deviation from the mean, 13.6% of the population’s scores will be between 1-2 standard deviations from the mean, and 2.3% of the population will be more then 2 standard deviations from the mean. Remember that the normal distribution is symmetrical, so we also know that 34.1% of the population’s score will be between -1 to 0 standard deviations from the mean, 13.6% of the population’s score will be between -2 to -1 standard deviations from the mean, and 2.3% of the population’s score will be more negative than -2 standard deviations from the mean. Lets look at this cumulative distribution:\n\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n   annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\nThe above figure visualises how 13.6% of the population have score that is more negative than -2 standard deviations from the mean, 34.1% of the population have a standard deviation that is more negative than -1 standard deviations from the mean (this also include all the people who are more than -2 standard deviations from the mean), etc.\nWe can now use the above information to identify which percentile an individual is within a distribution.\nFor example, let’s imagine that an individual called Jane wants to know what percentile she’s at with her height. Lets imagine she is 170cm tall, the mean height of people 150cm, and the SD 10cm. That would make her z-score:\n\\[\nZ-Score = \\frac{170 - 150}{10} = 2\n\\]\nAs we can see from the figure above, that puts her above 97.7% of the population, putting her in the top 2.3%."
  }
]