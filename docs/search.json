[
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency (R,Python)",
    "section": "",
    "text": "Central tendancy describes typical values of a variable, such as itâ€™s mean and median.\n\nMean vs.Â Median vs.Â Mode\nThe mean is often called the â€œaverageâ€ informally, but is actually a specific type of â€œaverageâ€. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, weâ€™ll use gapminder data\n\nRPython\n\n\n\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\nLoading required package: gapminder\n\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n  \n\n\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# import the tabulate\nfrom tabulate import tabulate\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n#display table\nprint(tabulate(gapminder_2007[:6], headers=gapminder_2007.head() , tablefmt=\"fancy_grid\",showindex=False ))\n\n# total of all years\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n\n\nTable\n\n\n67.00742253521126\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n67.00742253521126\n\n\n\nWhew - itâ€™s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. Itâ€™s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\ngapminder_2007['lifeExp'].median()\n\n67.00742253521126\n71.93549999999999\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\ngapminder_2007['lifeExp'].mode()\n\n\n\n\nMode of â€˜lifeExpâ€™\n\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\nlength(gapminder_2007$lifeExp)\n\n[1] 142\n\nlength(unique(gapminder_2007$lifeExp))\n\n[1] 142\n\n\n\n\n\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n142\n142\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that thereâ€™s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n[1] 2 4\n\n\n\n\n\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n\n\nExample of Mode\n\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most.\n\nQuestion 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/centralTendencyQuestions.html",
    "href": "describingData/centralTendencyQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/dispersion.html",
    "href": "describingData/dispersion.html",
    "title": "Dispersion (R,Python)",
    "section": "",
    "text": "To understand distributions such as the normal distribution, itâ€™s helpful to clarify some more basic concepts around how data is dispersed or spread."
  },
  {
    "objectID": "describingData/dispersion.html#range",
    "href": "describingData/dispersion.html#range",
    "title": "Dispersion (R,Python)",
    "section": "Range",
    "text": "Range\nRange simply captures the min(imum) and the max(imum) values. Lets look at the min and max for the life expectancy data from 2007:\n\nRPython\n\n\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmin(gapminder_2007$lifeExp)\n\n[1] 39.613\n\nmax(gapminder_2007$lifeExp)\n\n[1] 82.603\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].min()\n\n82.603\n\ngapminder_2007['lifeExp'].max()\n\n39.613\n\n\n\nSo the range for life expectancy in 2007 was between 39.613 and 82.603."
  },
  {
    "objectID": "describingData/dispersion.html#variance",
    "href": "describingData/dispersion.html#variance",
    "title": "Dispersion (R,Python)",
    "section": "Variance",
    "text": "Variance\n\nPopulation Variance\nVariance is how much the data varies around a mean. To capture this, we compare each individualâ€™s score with the mean, so lets do this with our gapminder dataâ€™s life expectancy:\n\nRPython\n\n\n\nlife_expectancy_variance_table <- data.frame(\n  life_expectancy = gapminder_2007$lifeExp,\n  diff_from_mean  = gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp)\n)\n\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nimport pandas as pd\nfrom tabulate import tabulate\n\nlife_expectancy_variance_table = {\n  'life_expectancy' : gapminder_2007['lifeExp'],\n  'diff_from_mean': gapminder_2007['lifeExp']- gapminder_2007['lifeExp'].mean(),\n}\n\n# convert it to a data frame\nlife_expectancy_variance_table = pd.DataFrame(life_expectancy_variance_table)\n\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\nTable\n\n\n\n\n\nSo we know for each country how different their life expectacy is to the mean life expectancy. But ideally we would like a single value to summarise variance. Lets see what would happen if we tried to summarise these differences from the mean by calculating the mean difference from the mean:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_from_mean) \n\n[1] 5.153937e-15\n\n\n\n\n\nlife_expectancy_variance_table['diff_from_mean'].mean()\n\n5.254013186958487e-15\n\n\n\nWe get a number that is effectively zero (go here for an explanation about e-numbers), because all the values above the mean balance out those below the mean. So to address this, we can square the differences to force all the numbers to be positive:\n\nRPython\n\n\n\nlife_expectancy_variance_table$diff_squared = life_expectancy_variance_table$diff_from_mean^2\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'] = life_expectancy_variance_table['diff_from_mean'].pow(2)\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\nTable\n\n\n\n\n\nIf we calculate the average of this, then we get a summary of the variance that is more informative:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_squared)\n\n[1] 144.7314\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'].mean()\n\n144.73136049752028\n\n\n\nHowever, as mean is what you get when you add all the items together and then divide it by the number of items, this can also be done in 2 steps in R (this will help us understand the formula later):\n\nRPython\n\n\n\nsum_of_squares = sum(life_expectancy_variance_table$diff_squared)\nthis_variance  = sum_of_squares/length(life_expectancy_variance_table$diff_squared)\nthis_variance\n\n[1] 144.7314\n\n\n\n\n\nsum_of_squares = life_expectancy_variance_table['diff_squared'].sum()\nthis_variance = sum_of_squares/life_expectancy_variance_table['diff_squared'].count()\nthis_variance\n\n144.73136049752028\n\n\n\nWe can represent the above in the following formula for the populationâ€™s (remember, this is when you have everyone from the group you are measuring) variance:\n\\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N}\n\\]\nLetâ€™s break down each of the above symbols: Ïƒ^2 is population variance Î£ is sum xi refers to the value for each participant xÌ„ refers to the mean for all participants N refers to the number of participants\n(note that the above is written as if weâ€™re looking at the variance of a group of participants, but the principles still work if looking at non-participant data)\n\n\nSample variance\nTo calculate the variance for a sample of participants, rather than every participant in the group youâ€™re measuring, you need a slightly different formula:\n\\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N - 1}\n\\] Note that Sample variance is represented by S^2 rather than Ïƒ^2\nSo why do we divide by N-1 rather than N? This is because the sample variance is an estimate rather than the actual population variance. When estimating the population variance you take into account the actual number of people (N) in the sample, whereas when you are estimating what happens generally in the population based on your sample, you take into account the degrees of freedom (N-1). In broad terms this reduces the risk of you under-estimating the variance of the population. You donâ€™t necessarily need to understand degrees of freedom beyond the idea that you are controlling for the fact that you are analysing a sample rather than the population they represent, so donâ€™t worry if the next section isnâ€™t completely clear."
  },
  {
    "objectID": "describingData/dispersion.html#degrees-of-freedom",
    "href": "describingData/dispersion.html#degrees-of-freedom",
    "title": "Dispersion (R,Python)",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom calculations can be useful to address statistics that are vulnerable to bias within a sample (i.e.Â the sample being distorted compared to the population). Interestingly, mean is not, but variance is. Lets see this by looking at differences between the population and sample for mean and variance, looking at the height of three people, and combining them into every combination of 2 people possible:\n\nRPython\n\n\n\nthree_heights = c(150,160,170)\npopulation_height_mean = mean(three_heights)\npopulation_height_variance = sum((three_heights - population_height_mean)^2)/3\n#sample participants in pairs\nsample_heights = data.frame(\n  pp1 = c(150,150,NA),\n  pp2 = c(160,NA,160),\n  pp3 = c(NA,170,170),\n  pair = c(\n    \"1 and 2\",\n    \"1 and 3\",\n    \"2 and 3\"\n  )\n)\n\nsample_heights$mean = c(\n  mean(c(three_heights[1], three_heights[2])),\n  mean(c(three_heights[1], three_heights[3])),\n  mean(c(three_heights[2], three_heights[3]))\n)\n\nsample_heights$pop_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/3,\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/3,\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/3\n)\n\nsample_heights$sample_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/(3-1),\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/(3-1),\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/(3-1)\n)\n\n\nrmarkdown::paged_table(sample_heights)\n\n\n\n  \n\n\nmean_sample_mean <- mean(sample_heights$mean)\nmean_sample_variance <- mean(sample_heights$sample_var)\nmean_population_variance <- mean(sample_heights$pop_var)\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nthree_heights = [150,160,170]\nthree_heights = pd.DataFrame(three_heights)\npopulation_height_mean = three_heights.mean()\npopulation_height_variance = (three_heights - population_height_mean).pow(2).sum()/3\n\n#sample participants in pairs\nsample_heights = {\n  'pp1': [150,150, np.nan],\n  'pp2': [160, np.nan, 160],\n  'pp3': [np.nan, 170, 170],\n  'pair': [\"1 and 2\", \"1 and 3\", \"2 and 3\"]\n}\n\nsample_heights = pd.DataFrame(sample_heights)\nmean = [three_heights.iloc[0:2].mean(),three_heights.iloc[[0,2],].mean(),three_heights.iloc[1:3].mean()]\nmean = pd.DataFrame(mean)\nsample_heights['mean']=mean\n\npop_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/3,((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/3,((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/3]\npop_var = pd.DataFrame(pop_var)\nsample_heights['pop_var']= pop_var\n\nsample_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/(3-1),((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/(3-1),((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/(3-1)]\nsample_var=pd.DataFrame(sample_var)\nsample_heights['sample_var']=sample_var\n\n\nsample_heights = pd.DataFrame(sample_heights)\n#print(markdownTable(sample_heights.to_dict(orient='records')).getMarkdown())\nprint(tabulate(sample_heights, headers=sample_heights.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nmean_sample_mean = sample_heights['mean'].mean()\nmean_sample_variance = sample_heights['sample_var'].mean()\nmean_population_variance = sample_heights['pop_var'].mean()\n\n\n\n\nTable\n\n\n\n\n\nWhen comparing the population mean to the mean sample mean (i.e., what is the typical mean for any sample), theyâ€™re identical (i.e.Â NOT biased):\n\nRPython\n\n\n\npopulation_height_mean\n\n[1] 160\n\nmean_sample_mean\n\n[1] 160\n\n\n\n\n\npopulation_height_mean\nmean_sample_mean\n\n0    160.0\ndtype: float64\n\n160.0\n\n\n\nWhereas when comparing the actual population variance (population_height_variance) to the mean (to identify what is a typical) estimate of variance using the population formula that should not be used for samples (mean_population_variance) finds the estimate of variance is typically smaller than the actual variance in the population:\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_population_variance\n\n[1] 33.33333\n\n\n\n\n\npopulation_height_variance\nmean_population_variance\n\n0    66.666667\ndtype: float64\n\n33.333333333333336\n\n\n\nAs this bias (almost) always underestimates the population variance, degrees of freedom is a useful correction to address this within calculations of sample variance. Lets compare the actual population height variance (population_height_variance) to the mean estimate using degrees of freedom that should be used for samples (mean_sample_variance).\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_sample_variance\n\n[1] 50\n\n\n\n\n\npopulation_height_variance\nmean_sample_variance\n\n0    66.666667\ndtype: float64\n\n50.0\n\n\n\nSo, not perfect, but this is less under-representative of the variance.\nOne thing to bear in mind is that calculation of some statistics does not require use of the degrees of freedom to correct for bias (as seen above, mean was not susceptible to bias).\nIf you would like to understand how degrees of freedom are determined, and what the thinking is behind this term, read on for a brief description of this (otherwise, feel free to skip to the next section).\nDegrees of freedom refers to how many values could change in your variable once you know what the outcome of the relevant statistic is. For example, if youâ€™re interested in the variance of the height of the three people, then you only have 2 degrees of freedom, because once you know the height of 2 of the participants AND the variance of the height, then there the remaining participant only has a 2 possible heights (so their height isnâ€™t free to change)."
  },
  {
    "objectID": "describingData/dispersion.html#standard-deviation-sd",
    "href": "describingData/dispersion.html#standard-deviation-sd",
    "title": "Dispersion (R,Python)",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is the square root of the variance. This takes into account that that the variance includes the square of the difference between the individual values and the mean:\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nVariance\n\\[                                                                  \n                                   \\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}    \n                                                                                          \\]\n\\[                                                                                                          \n                                                                                                                                                                                                                     S^2 = \\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}    \n                                                                                                                                                                                                                                                                            \\]\n\n\nSD\n\\[                                                                  \n                                  \\sigma = \\sqrt\\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}  \n                                                                                          \\]\n\\[                                                                                                          \n                                                                                                                                                                                                                    S = \\sqrt\\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}  \n                                                                                                                                                                                                                                                                            \\]\n\n\n\n\nQuestion 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/dispersionQuestions.html",
    "href": "describingData/dispersionQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "validityChecks/multipleTestingQuestions.html",
    "href": "validityChecks/multipleTestingQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof multiple_testing_1_response = Inputs.radio(['True','False']);\ncorrect_multiple_testing_1 = 'False';\nmultiple_testing_1_result = {\n  if(multiple_testing_1_response == correct_multiple_testing_1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof multiple_testing_2_response = Inputs.radio(['Bonferroni','Å idÃ¡k']);\ncorrect_multiple_testing_2 = 'Å idÃ¡k';\nmultiple_testing_2_result = {\n  if(multiple_testing_2_response == correct_multiple_testing_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "validityChecks/fdr.html",
    "href": "validityChecks/fdr.html",
    "title": "False Discovery Rate(incomplete)",
    "section": "",
    "text": "The false discovery rate (FDR) is the expected proportion of false rejections out of all rejections."
  },
  {
    "objectID": "validityChecks/multipleTesting.html",
    "href": "validityChecks/multipleTesting.html",
    "title": "Multiple Testing Corrections (R)",
    "section": "",
    "text": "Alpha threshold of .05 as a default\n\n\n\nThe page below assumes that the alpha threshold for a false positive is .05, and so some formulas below have been simplified to reflect this convention. However, this \\(\\alpha\\) threshold of .05 is not an absolute rule, and so you would need to adapt the relevant formulas below if you decided you wanted to accept a different likelihood of false positives.\nAs mentioned in statistics basics, researchers generally accept a 5% risk of a study reporting a significant finding when the effect found in their sample is not representative of the population (i.e.Â an \\(\\alpha\\) value of .05). However, what happens if you conduct multiple tests within a study? Applying the same \\(\\alpha\\) value of .05 to each test within your study becomes a problem of multiple testing increasing the risk of a false positive (unless you happen to only be investigating real effects, which you cannot be sure you are doing). To illustrate the issue, letâ€™s create some random data in which you want to test if there are optimal combinations of height and foot size that predict maths ability. One thing you might be thinking is that this is a meaningless result, that there is no optimal height and foot-size in relation to maths ability - and that seems like a sensible opinion. However, without correcting for multiple testing, itâ€™s almost guaranteed that the following analysis on randomised data will produce false-positives, i.e.Â identify certain combinations of height and foot-size that are associated with significantly higher or lower mathematics scores.\nThe analysis below will create 12 heights and 12 shoe sizes, generate 20 participants in each combination with a random score out of 100 in a maths test. Then there will be comparisons between each group and all other participants to identify if their especially good (or bad) at maths (which, again, is a ridiculous thing to believe).\nThis figure is based on data that is randomly generated, and participants in conditions represented by the blue dots performed significantly worse or better when compared to all other participants. Participants in green dots did not perform significantly better or worse compared to all other participants. Note that some participants in higher groups did not perform significantly better than all others because they had a wider range of scores within their group.\nThe above figure shows that with an analysis on randomised data there were multiple combinations of foot-size and height that were associated with significantly better or worse performance in the maths task. As this is randomised data, it should illustrate the risk of multiple testing without correction. You may be wondering how likely was it that we would get a false-positive. Assuming that the comparisons you are making are meaningless, the risk of a false positive can be summarised as:\n\\[\nfalsePositive_{risk} = (1-\\alpha)^{comparisons}\n\\]\nNow letâ€™s discuss some corrections for multiple testing and see whether they successfully reduce the number of false positives in our hypothetical scenario."
  },
  {
    "objectID": "validityChecks/multipleTesting.html#bonferroni-correction",
    "href": "validityChecks/multipleTesting.html#bonferroni-correction",
    "title": "Multiple Testing Corrections (R)",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nA simple correction to avoid the risk of false positives is to divide the \\(a\\) threshold by the number of tests:\n\\[\n\\alpha = \\frac{.5}{m}\n\\]\n\n\\(m\\) is the number of comparisons\n\nIn some ways, this does the job of correcting for false-positives well, as your p-value threshold is directly proportionate to the number of tests you complete. Letâ€™s see if a Bonferroni correction removes all the false-positives:\n\n# reset the \"sig\" column\nmaths_summarised$sig = \"no\"\nmaths_summarised$color = \"green\"\n\ncomparisons = length(heights) * length(shoe_size)\nfor(height in heights){\n  for(shoe_size in shoe_sizes){\n    exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]\n    con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]\n    this_t.test <- t.test(exp_group_scores, con_group_scores)\n    \n    # here is where the new alpha threshold is applied\n    if(this_t.test$p.value < .05/comparisons){\n      maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"yes\"\n      maths_summarised$color[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"blue\"\n    }\n  }\n}\n\nplot_ly(\n  data = maths_summarised, \n  x=~height, \n  y=~shoe,\n  z=~score, \n  color=~sig, \n  error_z = list(array=~se), \n  type = \"scatter3d\",\n  mode = \"markers\",\n  size = 1,\n  colors = yes_no_colors\n) \n\n\n\n\n\nAll the points are red above, confirming that there were no false-positives. However, this was the new alpha value:\n\\[\n\\alpha = \\frac{.05}{N_{tests}} = \\frac{.05}{144} = 0.0003472222\n\\]\nWhich means that you would need either to be investigating a very large effect or testing very many participants. Bonferroni correction is thus very conservative, and increases the risk of a false-negative (incorrectly concluding that there is no effect). There is also an impact on family-wise error rate which needs to be explained before you consider other corrections."
  },
  {
    "objectID": "validityChecks/multipleTesting.html#family-wise-error-rate-fwer",
    "href": "validityChecks/multipleTesting.html#family-wise-error-rate-fwer",
    "title": "Multiple Testing Corrections (R)",
    "section": "Family-wise error rate (FWER)",
    "text": "Family-wise error rate (FWER)\nIf our aim is to reduce the risk of a study reporting at least one false-positive to 5%, this is the family-wise error rate (FWER) and is distinct to the risk of a single statistical test giving you a false positive which is the \\(\\alpha\\) threshold. Without correcting the for multiple testing the family-wise error rate would increase with more tests (as shown above). However, some corrections for multiple testing are so conservative that the family-wise error rate will drop below 5%.\nA problem with Bonferroni corrections is that they do not maintain a family-wise error rate of .05. The more tests you conduct, the lower the family-wise error rate is. To illustrate this, we first need to calculate the likelihood that there are not any false-positives across all tests. For each individual test the likelihood is:\n\\[\n1-\\alpha_{bonferroni}\n\\]\nIf you ran 2 tests, a Bonferroni correction means that for each tests there is only a .025 (.05 divided by 2) chance that the test would give a false-positive. So for this example of 2 tests you would get:\n\\[\n1 - .025 = .975\n\\]\nA 97.5% chance of avoiding a false positive for one test alone. For each test you run, you multiply this likelihood by itself to calculate the likelihood that all your tests are not false-positives. So if there are 2 tests:\n\\[\n(1 - .025) * (1 - .025) = .975 * .975 = .950625\n\\]\nOr, you could just take the likelihood for any individual testâ€™s false-positive and apply the power of the number of tests, e.g.:\n\\[\n(1-.025)^2 = .975^2 = .950625\n\\]\nOr more generally\n\\[\n(1 - \\alpha_{bonferroni})^m\n\\]\n\n\\(\\alpha_{bonferroni}\\) is the Bonferroni corrected alpha value\n\\(m\\) is still the number of tests\n\nNow that we have the likelihood of getting no false-positives, we can calculate the family-wise error rate (FWER) by subtracting this likelihood from 1:\n\\[\nFWER = 1 - (1 - \\alpha_{bonferroni})^m\n\\]\nIf there are 2 tests, the FWER is:\n\\[\nFWER = 1 - (1 - .05)^2 = 1 - .975^2 = 1 - .950625 = .049375\n\\]\nWhilst this is very similar to our original \\(\\alpha\\) threshold of .05, it is more conservative because the threshold for a false-positive is stricter (i.e.Â lower) than .05. Letâ€™s check these out for Å idÃ¡k and Holm-Bonferroni corrections."
  },
  {
    "objectID": "validityChecks/multipleTesting.html#Å¡idÃ¡k-correction",
    "href": "validityChecks/multipleTesting.html#Å¡idÃ¡k-correction",
    "title": "Multiple Testing Corrections (R)",
    "section": "Å idÃ¡k correction",
    "text": "Å idÃ¡k correction\nA less conservative correction is Å idÃ¡k which aims to maintain a family-wise error rate across all your tests. If your philosophy is that you only want a 5% chance of a false positive across your whole study, then Å idÃ¡kâ€™s formula successfully calculates the required \\(\\alpha\\) value:\n\\[\n\\alpha_{Å idÃ¡k} = 1 - (1-.05)^{(1/m)}\n\\]\n\n\\(\\alpha_{Å idÃ¡k}\\) is the Å idÃ¡k corrected\n\\(.05\\) is included as the FWER that this formula aims to achieve. If you wanted a higher or lower FWER you would need to adjust this number in your formula\n\\(m\\) is (still) the number of tests you conduct in your study\n\n\n\n\n\n\n\nOptional - how do we get Å idÃ¡kâ€™s formula?\n\n\n\nFor those of you interested in how we get to Å idÃ¡kâ€™s formula, itâ€™s a re-organisation of the FWER formula above to focus on calculating Alpha rather than FWER:\n\\[\nFWER = 1 - (1 - \\alpha)^m\n\\]\nLetâ€™s minus 1 from both sides:\n\\[\nFWER - 1 = -(1 - \\alpha)^m\n\\]\nThen multiply both sides by -1:\n\\[\n1 - FWER = (1 - \\alpha)^m\n\\]\nThen apply the power of (1/m) to both sides:\n\\[\n(1 - FWER)^{1/m} = 1 - \\alpha\n\\]\nFollowed by adding alpha to both sides:\n\\[\n\\alpha + (1 - FWER)^{1/m} = 1\n\\]\nThen subtracting \\((1 - FWER)^{1/m}\\) from both sides:\n\\[\n\\alpha = 1 - (1 - FWER)^{1/m}\n\\]\nBoom, youâ€™re done.\n\n\nNow letâ€™s apply Å idÃ¡kâ€™s corrected \\(\\alpha\\) to the above maths test data.\n\n# reset the \"sig\" column\nmaths_summarised$sig = \"no\"\nmaths_summarised$color = \"green\"\nsidak_alpha =  1 - (1 - .05)^(1/144)\n\ncomparisons = length(heights) * length(shoe_size)\nfor(height in heights){\n  for(shoe_size in shoe_sizes){\n    exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]\n    con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]\n    this_t.test <- t.test(exp_group_scores, con_group_scores)\n    \n    # here is where the new alpha threshold is applied\n    if(this_t.test$p.value < sidak_alpha){\n      maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"yes\"\n      maths_summarised$color[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"blue\"\n    }\n  }\n}\n\nplot_ly(\n  data = maths_summarised, \n  x=~height, \n  y=~shoe,\n  z=~score, \n  color=~sig, \n  error_z = list(array=~se), \n  type = \"scatter3d\",\n  mode = \"markers\",\n  size = 1,\n  colors = yes_no_colors\n) \n\n\n\n\n\nGood news - no false positives ğŸ˜º. However, the \\(\\alpha\\) threshold is similarly sharp as a Bonferroni correction:\n\nBonferroni: 0.0003472\nÅ idÃ¡k: 0.0003561\n\nAnd so both would require unusually strong effects or extremely large sample sizes to detect true-positive results."
  },
  {
    "objectID": "validityChecks/multipleTesting.html#holm-bonferroni-method",
    "href": "validityChecks/multipleTesting.html#holm-bonferroni-method",
    "title": "Multiple Testing Corrections (R)",
    "section": "Holm-Bonferroni method",
    "text": "Holm-Bonferroni method\nThe Holm-Bonferroni method orders the p-values from the relevant tests in order of significance, and applies a Bonferroni correction threshold to the most significant results, and gradually reduces the threshold as you proceed through the ranks. To illustrate this, letâ€™s imagine a researcher ran 4 tests, and got the following 4 p-values (already ranked)\n\n\\(p_1\\) = .005\n\\(p_2\\) = .0125\n\\(p_3\\) = .03\n\\(p_4\\) = .049\n\nIf we were to apply a Bonferroni correction, all p-values would need to be less than .05/4 = .0125 to be significant. Considering the fact that all results would be significant with an uncorrected \\(\\alpha\\) threshold, it seems that rejecting 3 out of 4 p-values could reflect an overly conservative \\(\\alpha\\). Holm-Bonferroni only compares the most significant result against a Bonferroni corrected threshold, and then reduces the denominator by 1 for each comparison made (\\(k\\) below):\n\\[\n\\frac{.05}{m + 1 - k}\n\\]\n\n\\(m\\) is the number of tests/comparisons\n\\(k\\) is the rank of the current comparisonâ€™s p-value (1 for the smallest p-value)\n\nLetâ€™s apply this formula to each of our 4 p-values:\n\n\\(p_1\\) = .005 compared to the threshold of \\(.05/(4 + 1 - 1) = .0125\\) significant\n\\(p_2\\) = .0125 compared to the threshold of \\(.05/(4 + 1 - 2) = .0167\\) significant\n\\(p_3\\) = .03 compared to the threshold of \\(.05/(4 + 1 - 3) = .025\\) not significant\n\nOnce you reach a non-significant result, you stop your comparisons as youâ€™ve identified the p-values that are at too high a risk of a false-positive. To highlight the weirdness of what would happen if you continued:\n\n\\(p_4\\) = .049 compared to the threshold of \\(.05/(4 + 1 - 4) = .05\\) significant!?!?!?!?\n\nAs \\(p_3\\) is more significant than \\(p_4\\) it would be strange to conclude that the finding associated with \\(p_4\\) is significant but \\(p_3\\) isnâ€™t.\nAn advantage of this approach is that you wonâ€™t only capture the most significant result(s) in the way that you would with a Bonferroni correction, but the scaling adjustment is (arguably) an acceptable risk to false-positives compared to the risk of false-negatives for all p-values that a Bonferroni correction involves.\nBut what about the family-wise error rate (FWER)? Well, remember that FWER refers to the risk of at least one false positive, so the FWER formula is identical to a Bonferroni correction as the likelihood of the first false positive across all tests is the Bonferroni corrected alpha value:\n\\[\nFWER = 1 - (1 - \\alpha_{bonferroni})^m\n\\]\n\n\n\n\n\n\nOptional - likelihood of a second false-positive?\n\n\n\nWhilst FWER is identical for Bonferroni and Holm-Bonferroni corrections, Holm-Bonferroni has a higher risk of getting multiple false-positives. Letâ€™s illustrate this with binomial mathematics, starting with the likelihood of a Bonferroni testâ€™s likelihood of 2 false positives if there are 2 tests:\n\nbonferroni_comb <- data.frame(\n  description = c(\n    \"Both tests are false-positives\",\n    \"Only the first test is a false-positive\",\n    \"Only the second test is a false-positive\",\n    \"Neither test is a false-positive\"\n  ),\n  first.test = c(.025,.025,.975,.975),\n  second.test = c(.025,.975,.025,.975)\n)\nbonferroni_comb$likelihood = bonferroni_comb$first.test * bonferroni_comb$second.test\n\nknitr::kable(bonferroni_comb)\n\n\n\n\n\n\n\n\n\n\ndescription\nfirst.test\nsecond.test\nlikelihood\n\n\n\n\nBoth tests are false-positives\n0.025\n0.025\n0.000625\n\n\nOnly the first test is a false-positive\n0.025\n0.975\n0.024375\n\n\nOnly the second test is a false-positive\n0.975\n0.025\n0.024375\n\n\nNeither test is a false-positive\n0.975\n0.975\n0.950625\n\n\n\n\nsum(bonferroni_comb$likelihood)\n\n[1] 1\n\n\nIf you add all the likelihoods together you get 1, confirming that we have addressed all outcomesâ€™ likelihoods. The likelihood of 2 false-positives is .0025. We can visualise the above as follows:\n\nggplot() + \n  geom_vline(xintercept = 1) + \n  geom_hline(yintercept = 1) +\n  xlim(0,1) + \n  ylim(0,1) +\n  xlab(\"first result p-value\") +\n  ylab(\"second result p-value\") +\n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .05,\n      ymin = 0,\n      ymax = .05\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .05,\n      ymin = .05,\n      ymax = 1\n    ),\n    fill = \"green\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.05,\n      xmax = 1,\n      ymin = 0,\n      ymax = .05\n    ),\n    fill = \"blue\",\n    alpha = .5\n  )   + \n  geom_rect(\n    mapping = aes(\n      xmin = .05,\n      xmax = 1,\n      ymin = 0.05,\n      ymax = 1\n    ),\n    fill = \"orange\",\n    alpha = .5\n  ) \n\n\n\n\nThe area for each color on the above figure represents the likelihood of its respective outcome:\n\nRed: â€œBoth tests are false-positivesâ€, p = 0.000625, i.e.Â takes 0.000625 of the total area of the above figure\nGreen: â€œOnly the first test is a false-positiveâ€, p = 0.024375, i.e.Â takes 0.024375 of the total area of the above figure\nBlue: â€œOnly the second test is a false-positiveâ€, p = 0.024375, i.e.Â takes 0.024375 of the total area of the above figure\nOrange: â€œNeither test is a false-positiveâ€, p = 0.950625, i.e.Â takes 0.950625 of the total area of the above figure\n\nLetâ€™s now compare the above table to a Bonferroni-Holm methodology. Itâ€™s a little more complicated by the fact that there are 2 different p-value thresholds, which makes it more difficult to calculate outcomes that are mutually exclusive. We will try to be precise to avoid making references to overlapping outcomes:\n\nbonferroni_holm_comb <- data.frame(\n  description = c(\n    \"Both are false positives, but only the first test is <.025\",\n    \"Both are false positives, but only the second test is <.025\",\n    \"Both are false positives and both < .025\",\n    \"Only the first test is a false-positive\",\n    \"Only the second test is a false-positive\",\n    \"Neither test is a false-positive\"\n  ),\n  first.test = c(\n    .025, # likelihood of 0 to .025 as it is the ONLY test that it < .025\n    .025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025\n    .025, # likelihood of 0 to .025 as both tests are < .025\n    .025, # likelihood of 0 to .025 as it is the ONLY false-positive\n    .95,  # likelihood of 0 to .95 as the other test is significant\n    .975  # likelihood of 0 to .975 as the other test is not significant\n  ),\n  second.test = c(\n    .025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025\n    .025, # likelihood of 0 to .025 as it is the ONLY test that it < .025\n    .025, # likelihood of 0 to .025 as both tests are <.025\n    .95,  # likelihood of 0 to .025 as it is the ONLY false-positive\n    .025, # likelihood of 0 to .95 as the other test is significant\n    .975  # likelihood of 0 to .975 as the other test is not significant\n  )\n)\nbonferroni_holm_comb$likelihood = bonferroni_holm_comb$first.test * bonferroni_holm_comb$second.test\n\n# knitr::kable(bonferroni_holm_comb)\nsum(bonferroni_holm_comb$likelihood)\n\n[1] 1\n\n\nLooks like we succeeded ğŸ˜¸. Now letâ€™s visualise the above:\n\nggplot() + \n  geom_vline(xintercept = 1) + \n  geom_hline(yintercept = 1) +\n  xlim(0,1) + \n  ylim(0,1) +\n  xlab(\"First result p-value\") +\n  ylab(\"Second result p-value\") +\n  geom_rect(\n    mapping = aes(\n      xmin = .0,\n      xmax = .025,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"blue\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .025,\n      ymin = .025,\n      ymax = .05\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = .025,\n      xmax = .05,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"green\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.05,\n      xmax = 1,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"purple\",\n    alpha = .5\n  )   + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.0,\n      xmax = .025,\n      ymin = 0.05,\n      ymax = 1\n    ),\n    fill = \"brown\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.025,\n      xmax = 1,\n      ymin = 0.025,\n      ymax = 1\n    ),\n    fill = \"black\",\n    alpha = .5\n  ) \n\n\n\n\n\nRed: â€œBoth are false positives, but only the first test is <.025â€, p = 0.000625, i.e.Â takes 0.000625 of the total area of the above figure\nGreen: â€œBoth are false positives, but only the second test is <.025â€, p = 0.000625, i.e.Â takes 0.024375 of the total area of the above figure\nBlue: â€œBoth are false positives and both < .025â€, p = 0.024375, i.e.Â takes 0.000625 of the total area of the above figure\nBrown: â€œOnly the first test is a false-positiveâ€, p = 0.02375, i.e.Â takes 0.950625 of the total area of the above figure\nPurple: â€œOnly the second test is a false-positiveâ€, p = 0.02375, i.e.Â takes 0.950625 of the total area of the above figure\nGrey: â€œNeither test is a false-positiveâ€, p = 0.02375, i.e.Â takes 0.950625 of the total area of the above figure"
  },
  {
    "objectID": "validityChecks/multipleTesting.html#holm-Å¡idÃ¡k-method",
    "href": "validityChecks/multipleTesting.html#holm-Å¡idÃ¡k-method",
    "title": "Multiple Testing Corrections (R)",
    "section": "Holm-Å idÃ¡k method",
    "text": "Holm-Å idÃ¡k method\nCombining the principles above, you can apply Holmâ€™s philosophy\n\\[\n\\frac{.05}{m + 1 - k}\n\\]\nto the Å idÃ¡k correction:\n\\[\n\\alpha_{Å idÃ¡k} = 1 - (1-.05)^{(1/m)}\n\\]\nto get\n\\[\n\\alpha_{h-s} = 1 - ( 1 - .05)^{1/(m + 1-k)}\n\\]\n\n\\(\\alpha_{h-s}\\) refers to the Holm-Å idÃ¡k alpha value\n\\(m\\) refers to the number of tests\n\\(k\\) refers to the rank of the current test you are calculating the threshold for\n\nAn advantage of using this Holm-Å idÃ¡k method is that it will maintain a FWER of .05 in the same way that the Å idÃ¡k method does. This is because the FWER reflects the likelihood of at least one false-positive (as described above), and the Å idÃ¡k method maintains a FWER of .05.\n\nQuestion 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof multiple_testing_1_response = Inputs.radio(['True','False']);\ncorrect_multiple_testing_1 = 'False';\nmultiple_testing_1_result = {\n  if(multiple_testing_1_response == correct_multiple_testing_1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof multiple_testing_2_response = Inputs.radio(['Bonferroni','Å idÃ¡k']);\ncorrect_multiple_testing_2 = 'Å idÃ¡k';\nmultiple_testing_2_result = {\n  if(multiple_testing_2_response == correct_multiple_testing_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html",
    "href": "itemAnalyses/cronbachsAlpha.html",
    "title": "Cronbachâ€™s Alpha (R)",
    "section": "",
    "text": "If you want to address whether a questionnaire or scaleâ€™s items are measuring the same underlying variable, it can be helpful to assess how much overlap there is between the items.\nLetâ€™s use some publicly available data to investigate this issue. The Empathy Quotient (Lawrence et al. 2004) is a self-report measure of how empathic someone is. There are a variety of data sets from people who have used this measure, so weâ€™ll use some data from the following repository:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\nWhilst this is a general measure of empathy, empathy is a multifacted concept, so letâ€™s focus on one of the types of empathy, cognitive empathy. This will involve focusing on just items associated with this facet. Note that the items for this are 14, 15, 29, 34 and 35. Letâ€™s see how much covariance (see here for a reminder on covariance or shared variance) there is between the items:\n\neq <- read.csv(\"EQ_Data.csv\")\ncog_eq <- eq[,c(\"Q14.processed\", \"Q15.processed\", \"Q29.processed\", \"Q34.processed\", \"Q35.processed\")]\n\n# to improve readability, relabel the variables\ncolnames(cog_eq) <-c(\"cog_1\",\"cog_2\",\"cog_3\",\"cog_4\",\"cog_5\")\nknitr::kable(data.frame(cov(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.3687127\n0.1106138\n0.0976130\n0.1628303\n0.1012361\n\n\ncog_2\n0.1106138\n0.2787724\n0.1086957\n0.0812020\n0.0230179\n\n\ncog_3\n0.0976130\n0.1086957\n0.3060529\n0.0829071\n0.0967604\n\n\ncog_4\n0.1628303\n0.0812020\n0.0829071\n0.3687127\n0.0865303\n\n\ncog_5\n0.1012361\n0.0230179\n0.0967604\n0.0865303\n0.3674339\n\n\n\n\n\nNote that covariance of an item with itself is just variance, e.g.Â cog_1 with itself:\n\nvar(cog_eq$cog_1)\n\n[1] 0.3687127\n\n\nThe above covariance matrix suggests all the items have some overlap - all associations are positive. To get a sense of how strongly they overlap a correlation matrix could be more useful, so letâ€™s quickly look at that:\n\nknitr::kable(data.frame(cor(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n1.0000000\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\n1.0000000\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\n1.0000000\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\n1.0000000\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\n1.0000000\n\n\n\n\n\nTo create an aggregate of whether these all seem to reliably be measuring the same construct, we can calculate the Cronbachâ€™s Alpha, so letâ€™s do that next. Cronbachâ€™s Alpha can be calculated in a few ways. One way to conceptualise the cronbachâ€™s alpha is:\n\\[\n\\frac{average Covariance Between Variables}{average(co)Variance}\n\\]\nUsing the table above: this is the average of all the cells that capture covariance between items:\n\n(which captures how much the variables overlap) divided by the average of all variance of items with both with other items and with themselves\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.3687127\n0.1106138\n0.0976130\n0.1628303\n0.1012361\n\n\ncog_2\n0.1106138\n0.2787724\n0.1086957\n0.0812020\n0.0230179\n\n\ncog_3\n0.0976130\n0.1086957\n0.3060529\n0.0829071\n0.0967604\n\n\ncog_4\n0.1628303\n0.0812020\n0.0829071\n0.3687127\n0.0865303\n\n\ncog_5\n0.1012361\n0.0230179\n0.0967604\n0.0865303\n0.3674339\n\n\n\n\n\nWhich can be summarised as follows:\n\\[\n\\alpha = \\frac{\\bar{COV}}{(\\sum{s_i^2} + \\sum{COV_i})/N^2}\n\\]\nBased on a formula from (Field 2013) (Fourth edition, page 708)\n\n\\(N\\) is the number of items\n\\(\\bar{COV}\\) is the average covariance between items (note that this does not include an items covariance with itself)\n\\(s_i\\) is the standard deviation for a single item (remember that squaring it give\n\\(COV_i\\) is the covariance for an item with another item\nThe bottom half together is the sum of a complete covariance matrix like the one directly above.\n\nWhatâ€™s nice about this formula, is that itâ€™s quite easy to implement:\n\n\\(N^2\\) is \\(5^2\\) which makes 25\n\\(\\bar{COV}\\) is the mean for the above covariance matrix after removing comparisons of items with themselves, so the mean of:\n\n\n# we know that a correlation matrix will have 1 where an item is correlating with itself, so we'll create an index to skip those items:\ncov_itself = cor(cog_eq) == 1\n\n# now let's calculate the mean of the valid parts of the covariance matrix\ncov_df = data.frame(cov(cog_eq))\ncov_df[cov_itself] = \"\"\nknitr::kable(cov_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n\n0.110613810741688\n0.097612958226769\n0.162830349531117\n0.101236146632566\n\n\ncog_2\n0.110613810741688\n\n0.108695652173913\n0.0812020460358057\n0.0230179028132992\n\n\ncog_3\n0.097612958226769\n0.108695652173913\n\n0.0829070758738278\n0.096760443307758\n\n\ncog_4\n0.162830349531117\n0.0812020460358057\n0.0829070758738278\n\n0.0865302642796247\n\n\ncog_5\n0.101236146632566\n0.0230179028132992\n0.096760443307758\n0.0865302642796247\n\n\n\n\n\nmean_cov_df = mean(cov(cog_eq)[!cov_itself])\n\nwhich would make 0.0951407\n\n\\(\\sum{s_i^2}\\) is the sum of the variance of each item. In our case, we already have that information when we calculated the â€œcovarianceâ€ of each item with itself:\n\n\nmean_var_df <- data.frame(cov(cog_eq))\nmean_var_df[!cov_itself] = \"\"\nknitr::kable(mean_var_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.368712702472293\n\n\n\n\n\n\ncog_2\n\n0.278772378516624\n\n\n\n\n\ncog_3\n\n\n0.306052855924979\n\n\n\n\ncog_4\n\n\n\n0.368712702472293\n\n\n\ncog_5\n\n\n\n\n0.367433930093777\n\n\n\n\nsum_var_df = mean(cov(cog_eq)[cov_itself])\n\n\n\\(\\sum{COV_i}\\) is just the sum of the all the covariances, which actually are the same values we focus on for the mean:\n\n\nknitr::kable(cov_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n\n0.110613810741688\n0.097612958226769\n0.162830349531117\n0.101236146632566\n\n\ncog_2\n0.110613810741688\n\n0.108695652173913\n0.0812020460358057\n0.0230179028132992\n\n\ncog_3\n0.097612958226769\n0.108695652173913\n\n0.0829070758738278\n0.096760443307758\n\n\ncog_4\n0.162830349531117\n0.0812020460358057\n0.0829070758738278\n\n0.0865302642796247\n\n\ncog_5\n0.101236146632566\n0.0230179028132992\n0.096760443307758\n0.0865302642796247\n\n\n\n\n\n\nThis makes it quite simple to calculate the whole of the bottom row, as we can sum the entire covariance matrix (that includes the variances of each item with itself):\n\nsum(cov(cog_eq))\n\n[1] 3.592498\n\n\nPut together, we can calculate \\(alpha\\):\n\nmean(cov(cog_eq)[cor(cog_eq) != 1])/ # mean covariance\nmean(cov(cog_eq))                    # mean of covariance and variance combined\n\n[1] 0.6620788\n\n\nLetâ€™s compare this to the psych packages alpha function (note that it is likely that RStudio will have another alpha function active that is not cronbachâ€™s alpha if you havenâ€™t loaded the psych package!):\n\neq_alpha_output <- psych::alpha(cog_eq)\nknitr::kable(eq_alpha_output$total) # note that there other outputs available\n\n\n\nTableÂ 1: Psych Packageâ€™s Cronbach Alpha calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_alpha\nstd.alpha\nG6(smc)\naverage_r\nS/N\nase\nmean\nsd\nmedian_r\n\n\n\n\n\n0.6620788\n0.662596\n0.6330763\n0.2820018\n1.963806\n0.0642007\n1.22029\n0.3790777\n0.2817932"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#standardised-alpha",
    "href": "itemAnalyses/cronbachsAlpha.html#standardised-alpha",
    "title": "Cronbachâ€™s Alpha (R)",
    "section": "Standardised Alpha",
    "text": "Standardised Alpha\nVery similar logic to above, but instead of dividing the average covariate between items by the average covariate between and within items, we divide the average r value between items by the average r-value for both between and within items:\n\\[\n\\alpha = \\frac{\\bar{r_{b}}}{\\bar{r}}\n\\]\n\n\\(\\bar{r_b}\\) being the average r-value between items\n\\(\\bar{r}\\) being the average r-value both between and within items\n\n\n\n\n\n\n\nCronbachâ€™s Alpha vs.Â Standardised Cronbachâ€™s Alpha\n\n\n\nRemember that r-values are comparisons of covariates to the total variance (see here), and are thus standardised compared to covariates that are not divided by the total variance. One way to think about this is that correlations are standardised between -1 to 1, whereas variance has no lower or upper limit.\n\n\nThis is dividing the mean of:\n\ncor_df = data.frame(cor(cog_eq))\ncor_df[cov_itself] = NA\nknitr::kable(cor_df)\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\nNA\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\nNA\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\nNA\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\nNA\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\nNA\n\n\n\n\n\nby the mean of:\n\nknitr::kable(data.frame(cor(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n1.0000000\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\n1.0000000\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\n1.0000000\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\n1.0000000\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\n1.0000000\n\n\n\n\n\nThis can be calculated quite elegantly:\n\nmean(cor(cog_eq)[cor(cog_eq) != 1])/ # mean correlation between items\nmean(cor(cog_eq))                    # mean of all correlations\n\n[1] 0.662596\n\n\nWhich matches the value of std.alpha in the output in TableÂ 1."
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#general-benchmarks-for-cronbachs-alpha",
    "href": "itemAnalyses/cronbachsAlpha.html#general-benchmarks-for-cronbachs-alpha",
    "title": "Cronbachâ€™s Alpha (R)",
    "section": "General benchmarks for Cronbachâ€™s alpha",
    "text": "General benchmarks for Cronbachâ€™s alpha\nThere are caveats to these benchmarks, and weird things that can happen with Cronbachâ€™s alpha, but here are some broad benchmarks about how reliable your items are based on their \\(\\alpha\\) value:\n\nLess than .6 is unacceptable\n.6 to .7 is mediocre\nGreater than .7 is acceptable reliability\nGreater than .9 suggests that your measure includes redundant items"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#some-warnings",
    "href": "itemAnalyses/cronbachsAlpha.html#some-warnings",
    "title": "Cronbachâ€™s Alpha (R)",
    "section": "Some warnings",
    "text": "Some warnings\nMore items in a measure generally means a higher CA, even if the associations between the items is consistent. For example, imagine we had the following correlation matrix:\n\nexample_1_small <- data.frame(\n  item_1 = c(1,.1,.1,.1),\n  item_2 = c(.1,1,.1,.1),\n  item_3 = c(.1,.1,1,.1),\n  item_4 = c(.1,.1,.1,1)\n)\nknitr::kable(example_1_small)\n\n\n\n\nitem_1\nitem_2\nitem_3\nitem_4\n\n\n\n\n1.0\n0.1\n0.1\n0.1\n\n\n0.1\n1.0\n0.1\n0.1\n\n\n0.1\n0.1\n1.0\n0.1\n\n\n0.1\n0.1\n0.1\n1.0\n\n\n\n\n\nWe would get the following standardised \\(\\alpha\\):\n\nmean(example_1_small[example_1_small != 1])/ # mean correlation between items\nmean(as.matrix(example_1_small))             # mean of all correlations\n\n[1] 0.3076923\n\n\nLetâ€™s throw in a few more items with exactly the same strength of association to everything else:\n\nexample_1_large <- data.frame(\n  item_1 = c(1,.1,.1,.1,.1,.1,.1,.1),\n  item_2 = c(.1,1,.1,.1,.1,.1,.1,.1),\n  item_3 = c(.1,.1,1,.1,.1,.1,.1,.1),\n  item_4 = c(.1,.1,.1,1,.1,.1,.1,.1),\n  item_5 = c(.1,.1,.1,.1,1,.1,.1,.1),\n  item_6 = c(.1,.1,.1,.1,.1,1,.1,.1),\n  item_7 = c(.1,.1,.1,.1,.1,.1,1,.1),\n  item_8 = c(.1,.1,.1,.1,.1,.1,.1,1)\n)\nexample_1_large\n\n  item_1 item_2 item_3 item_4 item_5 item_6 item_7 item_8\n1    1.0    0.1    0.1    0.1    0.1    0.1    0.1    0.1\n2    0.1    1.0    0.1    0.1    0.1    0.1    0.1    0.1\n3    0.1    0.1    1.0    0.1    0.1    0.1    0.1    0.1\n4    0.1    0.1    0.1    1.0    0.1    0.1    0.1    0.1\n5    0.1    0.1    0.1    0.1    1.0    0.1    0.1    0.1\n6    0.1    0.1    0.1    0.1    0.1    1.0    0.1    0.1\n7    0.1    0.1    0.1    0.1    0.1    0.1    1.0    0.1\n8    0.1    0.1    0.1    0.1    0.1    0.1    0.1    1.0\n\n\n\nmean(example_1_large[example_1_large != 1])/ # mean correlation between items\nmean(as.matrix(example_1_large))             # mean of all correlations\n\n[1] 0.4705882\n\n\n\nDoes this inflation of alpha get worse if the average associations between items are stronger or weaker?\nTo visualise how rapidly the alpha values increase with the number of items, and if this is a bigger or smaller problems if the average association is stronger or weaker, letâ€™s calculate the alpha values you would get for a range of r-values (.1 to .9) and number of items (5 to 100):\n\nlibrary(ggplot2)\nalpha_inflation <- data.frame(\n  item_n  = rep(5:100,9),\n  r_value = (rep(c(.025, .05, .1, .2, .3, .4, .5, .6, .7), each = 96)),\n  alpha   = NA\n)\nfor(i in 1:length(alpha_inflation$item_n)){\n  this_r_value = alpha_inflation$r_value[i]\n  this_r_value = alpha_inflation$item_n[i]\n  this_example_cor_matrix = matrix(\n    alpha_inflation$r_value[i],\n    alpha_inflation$item_n[i],\n    alpha_inflation$item_n[i]\n  )\n  for(j in 1:alpha_inflation$item_n[i]){\n    this_example_cor_matrix[j,j] = 1\n  }\n  alpha_inflation$alpha[i] = mean(this_example_cor_matrix[this_example_cor_matrix != 1])/ # mean correlation between items\nmean(as.matrix(this_example_cor_matrix))             # mean of all correlations\n}\n\nggplot(data=alpha_inflation, aes(x=item_n, y=alpha, group=r_value, color=factor(r_value))) +\n  geom_line()+\n  geom_point() +\n  xlab(\"Number of Items\")  +\n  ylab(\"Standardised Cronbach's Alpha\") +\n  scale_color_discrete(name=\"Average \\nr-value\") + \n  geom_hline(yintercept = .7, color = \"black\") \n\n\n\n\nFigureÂ 1: Visualisation of how an increased number of items inflates Cronbachâ€™s Alpha even if the average association between items is kept consistent. Note that whilst this has been done on standardised Cronbachâ€™s Alpha, the principle applies even for calculation of non-standardised cronbachâ€™s Alpha. The black line is at a value of .7.\n\n\n\n\nWe can see above that the alpha value isnâ€™t inflated hugely when the average correlation is already high, which makes sense as there isnâ€™t much scope for it to be inflated. Measures with items with only weak associations are heavily influenced though. If you have a measure with 100 items, a Standardised Cronbachâ€™s Alpha of .7 will be achieved even if the average \\(r\\) value is as low as .025. In this sort of situation, it might be more helpful to identify sub-factors within your measure using analysis like Principle Components Analysis or Confirmatory Factor Analysis."
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#consolidation-questions",
    "href": "itemAnalyses/cronbachsAlpha.html#consolidation-questions",
    "title": "Cronbachâ€™s Alpha (R)",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\nCronbachâ€™s Alpha is useful to check whether items in a measure areâ€¦\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbachâ€™s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "itemAnalyses/cfa.html",
    "href": "itemAnalyses/cfa.html",
    "title": "Confirmatory Factor Analysis",
    "section": "",
    "text": "Questionnaires and surveys often have items that reflect multiple sub-scales or multiple (possibly related) factors (AKA components or sub-scales), in a single measure. In Confirmatory Factor Analysis you already have allocated items to specific factors (e.g.Â based on literature), but need to confirm whether these are valid allocations.\nFor example, the empathy quotient [@Lawrence2004] has multiple types of empathy within it:\n\ncognitive empathy\nemotional reactivity\nsocial skills\n\nLetâ€™s use one publicly available data set to check if we can confirm that the allocations of items to each of these subscales is consistent with the current data:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\n\neq <- read.csv(\"EQ_Data.csv\")\n# focusing on q1.raw to q40.raw\neq_processed <- eq[,c(paste(\"Q\",1:40,\".processed\",sep=\"\"))]\nrmarkdown::paged_table(eq_processed)\n\n\n\n  \n\n\ncolnames(eq_processed) <- paste(\"eq\",1:40, sep=\"\")\n\n\nrmarkdown::paged_table(data.frame(cor(eq_processed)))"
  },
  {
    "objectID": "itemAnalyses/cfa.html#confirmatory-factor-analysis",
    "href": "itemAnalyses/cfa.html#confirmatory-factor-analysis",
    "title": "Confirmatory Factor Analysis",
    "section": "Confirmatory factor analysis",
    "text": "Confirmatory factor analysis\nWe already have three subscales for the empathy quotient, so letâ€™s confirm whether our current data is consistent with them. The subscales are:\n\n\n\n\n\n\nSome of the items below are reverse scored\n\n\n\nThe items that would go against the component, e.g.Â â€œI find it hard to know what to do in a social situationâ€ if the participant agreed are reverse scored, meaning that strongly disagreeing increases their score for the relevant subscale (in this case â€œsocial skillsâ€).\n\n\nCognitive:\n\n14: I am good at predicting how someone will feel.\n15: I am quick to spot when someone in a group is feeling awkward or uncomfortable.\n29: I can sense if I am intruding, even if the other person doesnâ€™t tell me.\n34: I can tune into how someone else feels rapidly and intuitively.\n35: I can easily work out what another person might want to talk about.\n\nSocial Skills:\n\n2: I find it difficult to explain to others things that I understand easily, when they donâ€™t understand it first time.\n4: I find it hard to know what to do in a social situation.\n7: Friendships and relationships are just too difficult, so I tend not to bother with them.\n8: I often find it difficult to judge if something is rude or polite.\n21: I donâ€™t tend to find social situations confusing.\n\nEmotion:\n\n3: I really enjoy caring for other people.\n16: If I say something that someone else is offended by, I think that thatâ€™s their problem, not mine.\n19: Seeing people cry doesnâ€™t really upset me.\n33: I usually stay emotionally detached when watching a film.\n39: I tend to get emotionally involved with a friendâ€™s problems.\n\nLetâ€™s start by looking at correlation matrices to see if the items tend to correlate with each other in the current data set:\n\nCognitive empathy\n\ncor(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")])\n\n          eq14       eq15      eq29      eq34       eq35\neq14 1.0000000 0.34501695 0.2905797 0.4416185 0.27504384\neq15 0.3450170 1.00000000 0.3721252 0.2532783 0.07192026\neq29 0.2905797 0.37212520 1.0000000 0.2468024 0.28854264\neq34 0.4416185 0.25327834 0.2468024 1.0000000 0.23509011\neq35 0.2750438 0.07192026 0.2885426 0.2350901 1.00000000\n\n\nSo far looking good, as everything positively correlates - mostly with r-values greater than .25.\n\n\nSocial Skills\n\ncor(eq_processed[,c(\"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\")])\n\n             eq2       eq4         eq7         eq8        eq21\neq2   1.00000000 0.2067016 -0.06714029  0.06086131  0.30817434\neq4   0.20670163 1.0000000  0.16596510  0.16674209  0.25085525\neq7  -0.06714029 0.1659651  1.00000000 -0.02137924  0.21971081\neq8   0.06086131 0.1667421 -0.02137924  1.00000000 -0.04748266\neq21  0.30817434 0.2508553  0.21971081 -0.04748266  1.00000000\n\n\nThis is a bit less convincing, as the 8th item doesnâ€™t consistently correlate with other items in this subscale.\n\n\nEmotional empathy\n\ncor(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n\n           eq3       eq16       eq19      eq33      eq39\neq3  1.0000000 0.19886409 0.23743638 0.2444680 0.3494996\neq16 0.1988641 1.00000000 0.09384386 0.2801024 0.2080836\neq19 0.2374364 0.09384386 1.00000000 0.1884394 0.3160680\neq33 0.2444680 0.28010241 0.18843936 1.0000000 0.2923009\neq39 0.3494996 0.20808361 0.31606800 0.2923009 1.0000000\n\n\nEmotional empathy seems similarly consistent to cognitive empathy.\nLetâ€™s now see how well each item loads onto the total of these scores:\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.4.0      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\neq_subscales_r <- data.frame(\n  cor(\n    eq_processed[,c(\n      # Cog\n      \"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\",\n      \n      # Social Skills\n      \"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\",\n      \n      # Emotion\n      \"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\"\n      )],\n    matrix(data = c(\n      rowSums(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")]),\n      rowSums(eq_processed[,c(\"eq2\",\"eq4\", \"eq7\", \"eq8\", \"eq21\")]),\n      rowSums(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n      ), ncol = 3\n    )\n  )\n) \n\ntrying to create a proper tableâ€¦\n\npsych::fa(eq_processed, nfactors = 3, rotate=\"oblimin\")\n\nLoading required namespace: GPArotation\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : I\nam sorry, to do these rotations requires the GPArotation package to be installed\n\n\nFactor Analysis using method =  minres\nCall: psych::fa(r = eq_processed, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n       MR1   MR2   MR3    h2   u2 com\neq1   0.53 -0.11  0.03 0.297 0.70 1.1\neq2   0.13  0.24  0.24 0.130 0.87 2.5\neq3   0.49  0.11 -0.20 0.287 0.71 1.4\neq4   0.08 -0.03  0.56 0.325 0.68 1.0\neq5   0.22  0.59  0.01 0.397 0.60 1.3\neq6   0.25  0.05 -0.13 0.085 0.92 1.6\neq7   0.15  0.26  0.24 0.145 0.85 2.6\neq8   0.31  0.13  0.16 0.141 0.86 1.9\neq9   0.28  0.38  0.24 0.282 0.72 2.6\neq10  0.26  0.05 -0.27 0.142 0.86 2.1\neq11  0.21 -0.42  0.11 0.237 0.76 1.6\neq12  0.38  0.23  0.45 0.398 0.60 2.5\neq13  0.57 -0.05 -0.06 0.330 0.67 1.0\neq14  0.61 -0.23 -0.05 0.430 0.57 1.3\neq15  0.43 -0.22  0.13 0.246 0.75 1.7\neq16  0.42  0.42 -0.03 0.347 0.65 2.0\neq17  0.20  0.29 -0.22 0.170 0.83 2.7\neq18  0.46  0.22  0.36 0.394 0.61 2.4\neq19  0.36 -0.12  0.04 0.150 0.85 1.3\neq20  0.26  0.55  0.15 0.390 0.61 1.6\neq21  0.12 -0.08  0.48 0.249 0.75 1.2\neq22  0.62  0.03 -0.27 0.454 0.55 1.4\neq23  0.10  0.11 -0.18 0.055 0.95 2.3\neq24  0.20 -0.18 -0.27 0.146 0.85 2.6\neq25  0.19  0.28 -0.42 0.286 0.71 2.2\neq26  0.42 -0.42  0.12 0.370 0.63 2.2\neq27  0.24 -0.06 -0.01 0.060 0.94 1.1\neq28  0.53  0.18 -0.31 0.413 0.59 1.9\neq29  0.54 -0.19 -0.08 0.337 0.66 1.3\neq30  0.16  0.40 -0.04 0.184 0.82 1.3\neq31  0.35  0.60  0.01 0.483 0.52 1.6\neq32  0.29  0.19  0.20 0.162 0.84 2.6\neq33  0.33  0.30  0.08 0.209 0.79 2.1\neq34  0.59 -0.22 -0.12 0.414 0.59 1.4\neq35  0.41 -0.25  0.23 0.283 0.72 2.3\neq36  0.52 -0.49  0.09 0.516 0.48 2.1\neq37 -0.06 -0.04  0.08 0.013 0.99 2.3\neq38  0.30 -0.54  0.08 0.386 0.61 1.6\neq39  0.47  0.00 -0.31 0.319 0.68 1.7\neq40  0.58 -0.28  0.00 0.419 0.58 1.4\n\n                       MR1  MR2  MR3\nSS loadings           5.70 3.36 2.02\nProportion Var        0.14 0.08 0.05\nCumulative Var        0.14 0.23 0.28\nProportion Explained  0.51 0.30 0.18\nCumulative Proportion 0.51 0.82 1.00\n\nMean item complexity =  1.8\nTest of the hypothesis that 3 factors are sufficient.\n\nThe degrees of freedom for the null model are  780  and the objective function was  22.46 with Chi Square of  1209.17\nThe degrees of freedom for the model are 663  and the objective function was  14.34 \n\nThe root mean square of the residuals (RMSR) is  0.09 \nThe df corrected root mean square of the residuals is  0.1 \n\nThe harmonic number of observations is  69 with the empirical chi square  833.94  with prob <  6.5e-06 \nThe total number of observations was  69  with Likelihood Chi Square =  743.36  with prob <  0.016 \n\nTucker Lewis Index of factoring reliability =  0.754\nRMSEA index =  0.039  and the 90 % confidence intervals are  0.02 0.058\nBIC =  -2063.86\nFit based upon off diagonal values = 0.79\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3\nCorrelation of (regression) scores with factors   0.95 0.92 0.87\nMultiple R square of scores with factors          0.90 0.85 0.75\nMinimum correlation of possible factor scores     0.80 0.70 0.50\n\n# from https://www.anthonyschmidt.co/post/2020-09-27-efa-tables-in-r/\nflex <- function(data, title=NULL) {\n  # this grabs the data and converts it to a flextbale\n  flextable(data) %>%\n  # this makes the table fill the page width\n  set_table_properties(layout = \"autofit\", width = 1) %>%\n  # font size\n  fontsize(size=10, part=\"all\") %>%\n    #this adds a ttitlecreates an automatic table number\n      set_caption(title, \n                  autonum = officer::run_autonum(seq_id = \"tab\", \n                                                 pre_label = \"Table \", \n                                                 post_label = \"\\n\", \n                                                 bkm = \"anytable\")) %>%\n  # font type\n  font(fontname=\"Times New Roman\", part=\"all\")\n}\n\nfa_table <- function(x, cut) {\n  #get sorted loadings\n  loadings <- psych::fa.sort(x)$loadings %>% round(3)\n  #supress loadings\n  loadings[loadings < cut] <- \"\"\n  #get additional info\n  add_info <- cbind(x$communalities, \n                    x$uniquenesses,\n                    x$complexity) %>%\n    # make it a data frame\n    as.data.frame() %>%\n    # column names\n    rename(\"Communality\" = V1,\n           \"Uniqueness\" = V2,\n           \"Complexity\" = V3) %>%\n    #get the item names from the vector\n    rownames_to_column(\"item\")\n  #build table\n  loadings %>%\n    unclass() %>%\n    as.data.frame() %>%\n    rownames_to_column(\"item\") %>%\n    left_join(add_info) %>%\n    mutate(across(where(is.numeric), round, 3))\n}\n\nfa_table(\n  psych::fa(eq_processed, nfactors = 3, rotate=\"oblimin\"), \n  .5\n)\n\nLoading required namespace: GPArotation\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : I\nam sorry, to do these rotations requires the GPArotation package to be installed\n\n\nJoining, by = \"item\"\n\n\n   item   MR1   MR2   MR3 Communality Uniqueness Complexity\n1  eq22 0.617                   0.454      0.546      1.373\n2  eq14 0.613                   0.430      0.570      1.287\n3  eq34 0.591                   0.414      0.586      1.371\n4  eq40 0.584                   0.419      0.581      1.434\n5  eq13 0.569                   0.330      0.670      1.036\n6  eq29 0.544                   0.337      0.663      1.280\n7  eq28 0.535                   0.413      0.587      1.857\n8   eq1 0.532                   0.297      0.703      1.099\n9  eq36 0.515                   0.516      0.484      2.063\n10  eq3                         0.287      0.713      1.437\n11 eq39                         0.319      0.681      1.748\n12 eq18                         0.394      0.606      2.360\n13 eq15                         0.246      0.754      1.679\n14 eq26                         0.370      0.630      2.167\n15 eq16                         0.347      0.653      2.009\n16 eq35                         0.283      0.717      2.294\n17 eq19                         0.150      0.850      1.261\n18 eq33                         0.209      0.791      2.105\n19  eq8                         0.141      0.859      1.885\n20 eq32                         0.162      0.838      2.589\n21  eq6                         0.085      0.915      1.623\n22 eq27                         0.060      0.940      1.117\n23 eq31       0.602             0.483      0.517      1.598\n24  eq5       0.589             0.397      0.603      1.285\n25 eq20       0.548             0.390      0.610      1.602\n26 eq38                         0.386      0.614      1.612\n27 eq11                         0.237      0.763      1.637\n28 eq30                         0.184      0.816      1.338\n29  eq9                         0.282      0.718      2.558\n30 eq17                         0.170      0.830      2.708\n31  eq7                         0.145      0.855      2.578\n32  eq4             0.564       0.325      0.675      1.041\n33 eq21                         0.249      0.751      1.176\n34 eq12                         0.398      0.602      2.493\n35 eq25                         0.286      0.714      2.195\n36 eq24                         0.146      0.854      2.639\n37 eq10                         0.142      0.858      2.085\n38  eq2                         0.130      0.870      2.510\n39 eq23                         0.055      0.945      2.305\n40 eq37                         0.013      0.987      2.339\n\n\n\ncog_eq_sum = (\n  eq_processed$eq14 +\n  eq_processed$eq15 +\n  eq_processed$eq29 +\n  eq_processed$eq34 +\n  eq_processed$eq35\n)\n\ncog_eq_mean = (\n  eq_processed$eq14 +\n  eq_processed$eq15 +\n  eq_processed$eq29 +\n  eq_processed$eq34 +\n  eq_processed$eq35\n)/5\n\n\nlm(eq14 ~ cog_eq_sum, eq_processed)\n\n\nCall:\nlm(formula = eq14 ~ cog_eq_sum, data = eq_processed)\n\nCoefficients:\n(Intercept)   cog_eq_sum  \n    -0.3124       0.2341  \n\nlm(eq14 ~ cog_eq_mean, eq_processed)\n\n\nCall:\nlm(formula = eq14 ~ cog_eq_mean, data = eq_processed)\n\nCoefficients:\n(Intercept)  cog_eq_mean  \n    -0.3124       1.1705  \n\nbop<-lm(cog_eq_mean ~ eq14 + eq15 + eq29 + eq34 + eq35, eq_processed)\n\nbop$coefficients \n\n  (Intercept)          eq14          eq15          eq29          eq34 \n-6.163388e-16  2.000000e-01  2.000000e-01  2.000000e-01  2.000000e-01 \n         eq35 \n 2.000000e-01 \n\ncolnames(eq_subscales_r) <- c( \"Cognitive\", \"Social\", \"Emotion\")\n\neq_subscales_r %>% mutate_all(~cell_spec(\n        .x, \n        color = ifelse(.x < .4, \"black\", \"white\"),\n        background = ifelse(.x > .4, \"red\",\" white\"))) %>%\n    kable(escape = F) %>%\n    kable_styling()\n\n\n\n \n  \n      \n    Cognitive \n    Social \n    Emotion \n  \n \n\n  \n    eq14 \n    0.730729714723979 \n    0.110723735489975 \n    0.31320589104921 \n  \n  \n    eq15 \n    0.60185382395657 \n    0.198874708000659 \n    0.137720182774352 \n  \n  \n    eq29 \n    0.659975490377257 \n    0.0278299656284012 \n    0.364451955577018 \n  \n  \n    eq34 \n    0.679619374819311 \n    0.0226190168052008 \n    0.395730784538071 \n  \n  \n    eq35 \n    0.587492289787915 \n    0.122610871425514 \n    0.242014658835711 \n  \n  \n    eq2 \n    -0.00813200484463844 \n    0.576298173261772 \n    0.133855862092061 \n  \n  \n    eq4 \n    0.0479274896411267 \n    0.662778025977272 \n    0.123591773099612 \n  \n  \n    eq7 \n    -0.0312109379222552 \n    0.438752142983534 \n    0.234889555117118 \n  \n  \n    eq8 \n    0.226816177181639 \n    0.427950397740909 \n    0.134334810841783 \n  \n  \n    eq21 \n    0.161873895295539 \n    0.627680352407105 \n    -0.0206828796782751 \n  \n  \n    eq3 \n    0.29722368237889 \n    -0.00177049925158063 \n    0.62739169422544 \n  \n  \n    eq16 \n    0.268239998089082 \n    0.168294312535106 \n    0.592732439474545 \n  \n  \n    eq19 \n    0.35758009314024 \n    0.183400506448371 \n    0.552243103025108 \n  \n  \n    eq33 \n    0.159850007854409 \n    0.302368129555882 \n    0.665487951247779 \n  \n  \n    eq39 \n    0.349069094803534 \n    0.0110077011019322 \n    0.690638221783324 \n  \n\n\n\n\n\nThis is quite dense, but in broad terms, we want to cluster items that correlate with each other into one component (AKA factor AKA subscale).\nIf we use a package in R we can start identifying the top 3 components and check if the questions map on to what we would expect for each of the three subscales:\n\neq_pca <- prcomp(eq_processed)\nsort(abs(eq_pca$rotation[,1]),decreasing = T)[1:5]\n\n     eq16      eq20      eq22      eq18      eq13 \n0.2752500 0.2436235 0.2428536 0.2329153 0.2315572 \n\nsort(eq_pca$rotation[,1],decreasing = T)[1:5]\n\n       eq37        eq11        eq21         eq4        eq38 \n 0.03183132 -0.02479062 -0.03616440 -0.04118648 -0.04219994 \n\n\nComponent 1 involves questions 20, 33, 31, 16 and 5. These items are: - 20: I am very blunt, which some people take to be rudeness, even though this is unintentional. - 33: I usually stay emotionally detached when watching a film. - 31: Other people often say that I am insensitive, though I donâ€™t always see why. - 16: If I say something that someone else is offended by, I think that thatâ€™s their problem, not mine. - 5: People often tell me that I went too far in driving my point home in a discussion."
  },
  {
    "objectID": "itemAnalyses/cfa.html#learning-from-httpsuc-r.github.iopca",
    "href": "itemAnalyses/cfa.html#learning-from-httpsuc-r.github.iopca",
    "title": "Confirmatory Factor Analysis",
    "section": "learning from https://uc-r.github.io/pca",
    "text": "learning from https://uc-r.github.io/pca\n\nlibrary(tidyverse)  # data manipulation and visualization\nlibrary(gridExtra)  # plot arrangement\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndata(\"USArrests\")\nhead(USArrests, 10)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58 21.2\nAlaska        10.0     263       48 44.5\nArizona        8.1     294       80 31.0\nArkansas       8.8     190       50 19.5\nCalifornia     9.0     276       91 40.6\nColorado       7.9     204       78 38.7\nConnecticut    3.3     110       77 11.1\nDelaware       5.9     238       72 15.8\nFlorida       15.4     335       80 31.9\nGeorgia       17.4     211       60 25.8\n\nscaled_df <- apply(USArrests, 2, scale)\n\narrests.cov <- cov(scaled_df)\narrests.eigen <- eigen(arrests.cov)\n\narrests.cov\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\ntemp_1 <- lm(scaled_df[,1] ~ scaled_df[,2])\ncov(temp_1$residuals, scaled_df[,1])\n\n[1] 0.3569992\n\ncov(temp_1$residuals, scaled_df[,2])\n\n[1] -1.663918e-18\n\nstr(arrests.eigen)\n\nList of 2\n $ values : num [1:4] 2.48 0.99 0.357 0.173\n $ vectors: num [1:4, 1:4] 0.536 0.583 0.278 0.543 0.418 ...\n - attr(*, \"class\")= chr \"eigen\"\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\narrests.eigen$values\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\narrests.eigen$vectors\n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\n(beep <- arrests.eigen$vectors[,1:2])\n\n          [,1]       [,2]\n[1,] 0.5358995  0.4181809\n[2,] 0.5831836  0.1879856\n[3,] 0.2781909 -0.8728062\n[4,] 0.5434321 -0.1673186\n\nbeep <- prcomp(scaled_df)\nbeep$rotation <- beep$rotation * -1\nbeep$rotation <- beep$x * -1\n\nbiplot(beep, scale = 0)\n\n\n\n\nTo start"
  },
  {
    "objectID": "itemAnalyses/pca.html",
    "href": "itemAnalyses/pca.html",
    "title": "Principle Component Analysis (R)",
    "section": "",
    "text": "Questionnaires and surveys often have items that reflect multiple sub-scales or multiple related constructs, in a single measure. For example, the empathy quotient (Lawrence et al. 2004) has multiple types of empathy within it:\n\ncognitive empathy\nemotional reactivity\nsocial skills\n\nLetâ€™s use one publicly available data set to check if we can replicate these subscales:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\n\neq <- read.csv(\"EQ_Data.csv\")\n# focusing on q1.raw to q40.raw\neq_processed <- eq[,c(paste(\"Q\",1:40,\".processed\",sep=\"\"))]\nrmarkdown::paged_table(eq_processed)\n\n\n\n  \n\n\ncolnames(eq_processed) <- paste(\"eq\",1:40, sep=\"\")\n\n\nrmarkdown::paged_table(data.frame(cor(eq_processed)))"
  },
  {
    "objectID": "itemAnalyses/pca.html#confirmatory-factor-analysis",
    "href": "itemAnalyses/pca.html#confirmatory-factor-analysis",
    "title": "Principle Component Analysis (R)",
    "section": "Confirmatory factor analysis",
    "text": "Confirmatory factor analysis\nWe already have three subscales for the empathy quotient, so letâ€™s confirm whether our current data is consistent with them. The subscales are:\n\n\n\n\n\n\nSome of the items below are reverse scored\n\n\n\nThe items that would go against the component, e.g.Â â€œI find it hard to know what to do in a social situationâ€ if the participant agreed are reverse scored, meaning that strongly disagreeing increases their score for the relevant subscale (in this case â€œsocial skillsâ€).\n\n\nCognitive:\n\n14: I am good at predicting how someone will feel.\n15: I am quick to spot when someone in a group is feeling awkward or uncomfortable.\n29: I can sense if I am intruding, even if the other person doesnâ€™t tell me.\n34: I can tune into how someone else feels rapidly and intuitively.\n35: I can easily work out what another person might want to talk about.\n\nSocial Skills:\n\n2: I find it difficult to explain to others things that I understand easily, when they donâ€™t understand it first time.\n4: I find it hard to know what to do in a social situation.\n7: Friendships and relationships are just too difficult, so I tend not to bother with them.\n8: I often find it difficult to judge if something is rude or polite.\n21: I donâ€™t tend to find social situations confusing.\n\nEmotion:\n\n3: I really enjoy caring for other people.\n16: If I say something that someone else is offended by, I think that thatâ€™s their problem, not mine.\n19: Seeing people cry doesnâ€™t really upset me.\n33: I usually stay emotionally detached when watching a film.\n39: I tend to get emotionally involved with a friendâ€™s problems.\n\nLetâ€™s start by looking at correlation matrices to see if the items tend to correlate with each other in the current data set:\n\nCognitive empathy\n\ncor(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")])\n\n          eq14       eq15      eq29      eq34       eq35\neq14 1.0000000 0.34501695 0.2905797 0.4416185 0.27504384\neq15 0.3450170 1.00000000 0.3721252 0.2532783 0.07192026\neq29 0.2905797 0.37212520 1.0000000 0.2468024 0.28854264\neq34 0.4416185 0.25327834 0.2468024 1.0000000 0.23509011\neq35 0.2750438 0.07192026 0.2885426 0.2350901 1.00000000\n\n\nSo far looking good, as everything positively correlates - mostly with r-values greater than .25. Letâ€™s now see how well each item loads onto the total of these scores:\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.4.0      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\neq_subscales_r <- data.frame(\n  cor(\n    eq_processed[,c(\n      # Cog\n      \"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\",\n      \n      # Social Skills\n      \"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\",\n      \n      # Emotion\n      \"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\"\n      )],\n    matrix(data = c(\n      rowSums(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")]),\n      rowSums(eq_processed[,c(\"eq2\",\"eq4\", \"eq7\", \"eq8\", \"eq21\")]),\n      rowSums(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n      ), ncol = 3\n    )\n  )\n) \n\ncolnames(eq_subscales_r) <- c( \"Cognitive\", \"Social\", \"Emotion\")\n\neq_subscales_r %>% mutate_all(~cell_spec(\n        .x, \n        color = ifelse(.x < .4, \"black\", \"white\"),\n        background = ifelse(.x > .4, \"red\",\" white\"))) %>%\n    kable(escape = F) %>%\n    kable_styling()\n\n\n\n \n  \n      \n    Cognitive \n    Social \n    Emotion \n  \n \n\n  \n    eq14 \n    0.730729714723979 \n    0.110723735489975 \n    0.31320589104921 \n  \n  \n    eq15 \n    0.60185382395657 \n    0.198874708000659 \n    0.137720182774352 \n  \n  \n    eq29 \n    0.659975490377257 \n    0.0278299656284012 \n    0.364451955577018 \n  \n  \n    eq34 \n    0.679619374819311 \n    0.0226190168052008 \n    0.395730784538071 \n  \n  \n    eq35 \n    0.587492289787915 \n    0.122610871425514 \n    0.242014658835711 \n  \n  \n    eq2 \n    -0.00813200484463844 \n    0.576298173261772 \n    0.133855862092061 \n  \n  \n    eq4 \n    0.0479274896411267 \n    0.662778025977272 \n    0.123591773099612 \n  \n  \n    eq7 \n    -0.0312109379222552 \n    0.438752142983534 \n    0.234889555117118 \n  \n  \n    eq8 \n    0.226816177181639 \n    0.427950397740909 \n    0.134334810841783 \n  \n  \n    eq21 \n    0.161873895295539 \n    0.627680352407105 \n    -0.0206828796782751 \n  \n  \n    eq3 \n    0.29722368237889 \n    -0.00177049925158063 \n    0.62739169422544 \n  \n  \n    eq16 \n    0.268239998089082 \n    0.168294312535106 \n    0.592732439474545 \n  \n  \n    eq19 \n    0.35758009314024 \n    0.183400506448371 \n    0.552243103025108 \n  \n  \n    eq33 \n    0.159850007854409 \n    0.302368129555882 \n    0.665487951247779 \n  \n  \n    eq39 \n    0.349069094803534 \n    0.0110077011019322 \n    0.690638221783324 \n  \n\n\n\n\n\nThis is quite dense, but in broad terms, we want to cluster items that correlate with each other into one component (AKA factor AKA subscale).\nIf we use a package in R we can start identifying the top 3 components and check if the questions map on to what we would expect for each of the three subscales:\n\neq_pca <- prcomp(eq_processed)\nsort(abs(eq_pca$rotation[,1]),decreasing = T)[1:5]\n\n     eq16      eq20      eq22      eq18      eq13 \n0.2752500 0.2436235 0.2428536 0.2329153 0.2315572 \n\nsort(eq_pca$rotation[,1],decreasing = T)[1:5]\n\n       eq37        eq11        eq21         eq4        eq38 \n 0.03183132 -0.02479062 -0.03616440 -0.04118648 -0.04219994 \n\n\nComponent 1 involves questions 20, 33, 31, 16 and 5. These items are: - 20: I am very blunt, which some people take to be rudeness, even though this is unintentional. - 33: I usually stay emotionally detached when watching a film. - 31: Other people often say that I am insensitive, though I donâ€™t always see why. - 16: If I say something that someone else is offended by, I think that thatâ€™s their problem, not mine. - 5: People often tell me that I went too far in driving my point home in a discussion."
  },
  {
    "objectID": "itemAnalyses/pca.html#learning-from-httpsuc-r.github.iopca",
    "href": "itemAnalyses/pca.html#learning-from-httpsuc-r.github.iopca",
    "title": "Principle Component Analysis (R)",
    "section": "learning from https://uc-r.github.io/pca",
    "text": "learning from https://uc-r.github.io/pca\n\nlibrary(tidyverse)  # data manipulation and visualization\nlibrary(gridExtra)  # plot arrangement\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndata(\"USArrests\")\nhead(USArrests, 10)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58 21.2\nAlaska        10.0     263       48 44.5\nArizona        8.1     294       80 31.0\nArkansas       8.8     190       50 19.5\nCalifornia     9.0     276       91 40.6\nColorado       7.9     204       78 38.7\nConnecticut    3.3     110       77 11.1\nDelaware       5.9     238       72 15.8\nFlorida       15.4     335       80 31.9\nGeorgia       17.4     211       60 25.8\n\nscaled_df <- apply(USArrests, 2, scale)\n\narrests.cov <- cov(scaled_df)\narrests.eigen <- eigen(arrests.cov)\n\narrests.cov\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\ntemp_1 <- lm(scaled_df[,1] ~ scaled_df[,2])\ncov(temp_1$residuals, scaled_df[,1])\n\n[1] 0.3569992\n\ncov(temp_1$residuals, scaled_df[,2])\n\n[1] -1.663918e-18\n\nstr(arrests.eigen)\n\nList of 2\n $ values : num [1:4] 2.48 0.99 0.357 0.173\n $ vectors: num [1:4, 1:4] 0.536 0.583 0.278 0.543 0.418 ...\n - attr(*, \"class\")= chr \"eigen\"\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\narrests.eigen$values\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\narrests.eigen$vectors\n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\n(beep <- arrests.eigen$vectors[,1:2])\n\n          [,1]       [,2]\n[1,] 0.5358995  0.4181809\n[2,] 0.5831836  0.1879856\n[3,] 0.2781909 -0.8728062\n[4,] 0.5434321 -0.1673186\n\nbeep <- prcomp(scaled_df)\nbeep$rotation <- beep$rotation * -1\nbeep$rotation <- beep$x * -1\n\nbiplot(beep, scale = 0)\n\n\n\n\nTo start"
  },
  {
    "objectID": "itemAnalyses/cronbachAlphaQuestions.html",
    "href": "itemAnalyses/cronbachAlphaQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nCronbachâ€™s Alpha is useful to check whether items in a measure areâ€¦\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbachâ€™s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\n[1] TRUE\n\n\nGreat! What youâ€™re more likely to want to do is to compare a vector to a value. So letâ€™s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Letâ€™s compare that vector to the word â€œcorrectâ€:\n\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Letâ€™s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. Weâ€™re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n  \n\n\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes youâ€™ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n!correct_vector == \"correct\" \n\n[1] FALSE  TRUE FALSE\n\n# which is the same as\n!(correct_vector == \"correct\")\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the â€œ&â€ operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e.Â less than 1000ms:\n\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "rBasics/fundamentals.html",
    "href": "rBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "R allows you to complete calculations, so lets start with that. Type into your markdown or script\nOften, you can use R as a calculator, for example, if you want to know what two peopleâ€™s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nItâ€™s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g.Â how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. â€œmy objectâ€ would be an invalid object name.\nObject names are case-sensitive, so if your object is called â€œMyObjectâ€ you cannot refer to it as â€œmyobjectâ€.\nâ€œ.â€ and â€œ_â€ are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou canâ€™t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing â€œt.teâ€ and pressing the tab will give you â€œt.testâ€\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you donâ€™t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe â€œcâ€ above is short for â€œcombineâ€ as youâ€™re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the â€œtypeofâ€ function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (â€œheightsâ€) is a â€œdoubleâ€ vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers canâ€™t always be integers. Strings are identified as â€œcharacterâ€ objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the â€œmpgâ€ dataframe. To look at it, just type in itâ€™s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below wonâ€™t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see whatâ€™s in the â€œmanufacturerâ€ column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about â€œintelligent copy and pasteâ€ to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming youâ€™ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code weâ€™ve copied so that you can apply it to your own scripts. Thereâ€™s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\nggplot(             # R will keep looking at your code until all the open brackets have been closed)\n  mpg,              #\n  aes(              #\n    displ,          #\n    hwy,            #\n    colour = class  #\n  )\n) +                 # R will look to the next line if you end a line with +\ngeom_point()        #\n\n\n\n\nHereâ€™s how I would comment it out:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    displ,          # x-axis\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\n\n\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    cty,            # x-axis - updated\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\n\n\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "rBasics/filetypes.html",
    "href": "rBasics/filetypes.html",
    "title": "Types of Scripts",
    "section": "",
    "text": "Welcome to using R. This subsection will explain the basics about R. First, lets discuss the different ways to write R code, as there are (at least) 5:\n\ntype it directly into the console (generally not recommended)\nsave it as a script (better) note that script can refer to any of the below, but in this case is being used to describe a script that doesnâ€™t generate a notebook\nsave it as an R Markdown (better still) - this allows you to make beautiful documents\nsave it as an R Notebook (arguably better than R Markdown) - this allows you to make beautiful documents, and is quicker\n\n\nThe Console\nAt the bottom left of RStudio you should have a console that looks something like whatâ€™s highlighted in red below:\n\n\n\nconsole\n\n\nYou can type straight into the console, to get a result. You can scroll through your previous commands by pressing the up arrow in the console. Each time code is run in the console it updates the environment in the top right of R-Studio:\n\n\n\nenvironment\n\n\n\n\nScripts\nThe word â€œscriptâ€ can be interpreted specifically, to refer to a type of R file that includes a lot of code, or generally to refer to any file that includes both R code and code that allows you make a nice looking report. In this subsection, we will be focusing on â€œscriptâ€ as a particular type of file. To create a script, click on File -> New File -> R Script\n\n\n\nnewScript\n\n\nYou will then be shown a blank script, in which you can write a series of functions, and then run them. To run the lines of code, select a line, and then press CTRL-ENTER, or highlight a chunk of code and then press CTRL-ENTER. In either case, the code will be sent to the console and run there.\nAn advantage of a script over just using the console is that you can analyse your data in both structured and complex ways which is difficult if you are typing code directly into the console.\n\n\nR Markdown\nAs highlighted above, R Markdown is a type of â€œscriptâ€ in the general sense of the word, but allows you to create beautiful .html notebooks (.html files are what internet pages are based on). You are in fact reading an example of what can be produced by R Markdown (and R Notebooks). To make an R Markdown file, click on File -> New File -> R Markdown. You will be asked for a title, author and what output you would like. I would suggest â€œfirst markdownâ€, your name and â€œhtmlâ€ as the respective answers. You should then see something like:\n\n\n\nmarkdown\n\n\nThe following points apply to both R Markdown and R Notebooks\nIf you look above, you may notice that there are 2 types of code: Markdown (to write a nice looking report) and R (in grey chunks). I think these are well explained here: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf so Iâ€™ll just explain that R Markdowns run all the code in the chunks each time they generate the output file (e.g.Â html file). This is important to know, because R Notebooks do not run all the code in all the chunks when you generate them (see below for more on this).\n\n\nR Notebooks\nR Notebooks can be created by clicking on File -> New File -> R Notebook. They look quite similar to R Markdowns, but automatically generate the .html output each time you save the notebook. The output file will be a .nb.html file in the same folder as your notebook.\nVery importantly - the .nb.html file will be built based on what happened the last time you run each R chunk. If you never ran the R Chunk, then the nb.html file will not use the output from that chunk. This makes R Notebooks quicker than R Markdowns, because you donâ€™t have to generate the output from scratch each time, as it will just use whatever was generated the last time the chunk was run. However, this means that thereâ€™s a risk your .html output will not be what you expect if you failed to run all of your chunks before saving your file. To address this risk (when youâ€™ve finished editing your file), you can select the â€œrun allâ€ option.\n\n\n\nrunAllChunks"
  },
  {
    "objectID": "installing.html",
    "href": "installing.html",
    "title": "Installing R(Studio)",
    "section": "",
    "text": "Installing R\nDownload and install the following depending on your operating system:\n\nWindows: https://cran.r-project.org/bin/windows/base/\nMac: and select the R-x.x.x.pkg notarized and signed option\n\n\n\nInstalling RStudio\n\nhttps://www.rstudio.com/products/rstudio/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "href": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "title": "R-squared vs.Â Adjusted R-squared",
    "section": "",
    "text": "When completing a regression, thereâ€™s always a risk of â€œoverfittingâ€ the data, i.e.Â creating a model that includes predictors that have no meaningful association with the outcome variable. One reason that overfitting the data is a problem is that it is almost impossible for a predictor to have no association with an outcome variable. For that to happen you would need any data points that suggested a positive association between the outcome and the predictor to be equally balanced out by data points that suggest a negative association:\n\nRPython\n\n\n\nlibrary(ggplot2)\nno_association_df <- data.frame(\n  predictor = c(1,1,1,2,2,2,3,3,3),\n  outcome   = c(1,2,3,1,2,3,1,2,3)\n)\n\nggplot(no_association_df, aes(x = predictor, y = outcome)) + geom_point() + geom_smooth(method=lm, formula = 'y ~ x')\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sample participants in pairs\nno_association_df = {\n  'predictor': [1,1,1,2,2,2,3,3,3],\n  'outcome': [1,2,3,1,2,3,1,2,3]\n}\n\nno_association_df = pd.DataFrame(no_association_df)\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\ncoef = np.polyfit(no_association_df[\"predictor\"],no_association_df[\"outcome\"],1)\npoly1d_fn = np.poly1d(coef) \n# poly1d_fn is now a function which takes in x and returns an estimate for y\n\nplt.plot(no_association_df[\"predictor\"],no_association_df[\"outcome\"], 'ko', no_association_df[\"predictor\"], poly1d_fn(no_association_df[\"predictor\"]), '-b') #'--k'=black dashed line, 'yo' = yellow circle marker\n\n# add title on the x-axis\nplt.xlabel(\"predictor\")\n\n# add title on the y-axis\nplt.ylabel(\"outcome\")\n\nplt.show()\n\n\n\n\n\n\n\nUnrealistic Data Distribution\n\n\nFig. X. An example of how unrealistically balanced your data needs to be to find no association. As this (almost) never happens in reality, samples are biased towards finding associations between predictor and outcome variables even when there arenâ€™t any in the population. For example, letâ€™s generate some random data, and see what R-Values we find. Remember, random data really shouldnâ€™t have any association between predictor and outcome variables.\n\nRPython\n\n\n\nrandom_df = data.frame(\n  random_iv_1 = runif(100),\n  random_iv_2 = runif(100),\n  random_iv_3 = runif(100),\n  random_dv = runif(100)\n)\nrmarkdown::paged_table(random_df)\n\n\n\n  \n\n\n\n\n\n\nimport random\nfrom tabulate import tabulate\n\nrandom_df = {\n    'random_iv_1': np.random.random_sample(size = 100),\n    'random_iv_2': np.random.random_sample(size = 100),\n    'random_iv_3': np.random.random_sample(size = 100),\n    'random_dv': np.random.random_sample(size = 100)\n}\n \n# convert it to a data frame\nrandom_df = pd.DataFrame(random_df)\n\n# print the table\nprint(tabulate(random_df[:10], headers=random_df.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\n\nRPython\n\n\n\nrandom_lm <- lm(random_dv ~ random_iv_1, random_df)\nrandom_summary <- summary(random_lm)\nrandom_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1, data = random_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52452 -0.22320  0.01342  0.25408  0.47637 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.53494    0.05880   9.098  1.1e-14 ***\nrandom_iv_1 -0.02233    0.10492  -0.213    0.832    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2876 on 98 degrees of freedom\nMultiple R-squared:  0.0004618, Adjusted R-squared:  -0.009738 \nF-statistic: 0.04528 on 1 and 98 DF,  p-value: 0.8319\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_lm = smf.ols(formula='random_dv ~ random_iv_1', data=random_df).fit()\nrandom_lm.summary()\n\n\n\n\n\n\n\nTable\n\n\nNote that the above output is generated each time this page is rendered (generated), and so by chance may happen to look like the random predictor is significant. If so, thereâ€™s a 95% chance that this predictor will not be significant next time the page is rendered.\nLooking at the output above, we can see that 0.0462% of the variance of random_dv was explained by random_iv_1 before correction. Considering that these were randomly generated numbers, thatâ€™s 0.0462% too much. However, the Adjusted R-squared is only -0.0097. Note that Adjusted R-squared can be a negative number, and a negative number suggests that based on the sample, the predictor(s) has(/have) no association with the outcome variable in the population.\nA formula for the adjusted r-squared is:\n\\[\n\\bar{R^2} = 1-\\frac{SS_{res}/df_{res}}{SS_{tot}/df_{tot}}\n\\] \\(\\bar{R^2}\\) is the Adjusted R-Squared \\(SS_{total}\\) is the Sum of Squares of the total (i.e.Â how much total variance there is around the mean to explain) \\(SS_{res}\\) is the Sum of Squares of the residuals (i.e.Â how much isnâ€™t explained by the model) \\(df_{total}\\) is the Degrees of Freedom of the total. This is the number of data points - 1, so is N - 1 \\(df_{res}\\) is the Degrees of Freedom of the residuals. The degrees of freedom for the residuals takes into account the number of data points and the number of predictors, and so is N - 1 - 1\nLetâ€™s use the above formula to manually calculate the Adjusted R Squared\n\nRPython\n\n\n\nss_res <- sum(random_lm$residuals^2)\nss_total <- sum(\n  (\n    random_df$random_dv - mean(random_df$random_dv)\n  )^2\n)\n\n\nrandom_r_square = ss_total - ss_res\ndf_total <- length(random_lm$residuals) - 1\ndf_res <- length(random_lm$residuals) -\n  1 - # remove 1 from the number of data points\n  1 # remove another 1 to reflect there being 1 predictor\nadjusted_random_r_square = 1 - (ss_res/df_res)/(ss_total/df_total)\n\nadjusted_random_r_square\n\n[1] -0.009737557\n\n\n\n\n\nss_res = sum(random_lm.resid**2)\nss_total = sum((random_df[\"random_dv\"] - random_df[\"random_dv\"].mean())**2)\n\nrandom_r_square = ss_total-ss_res\ndf_total = len(random_lm.resid)-1\ndf_res = len(random_lm.resid)-1 - 1\n\nadjusted_random_r_square = 1-(ss_res/df_res)/(ss_total/df_total)\nadjusted_random_r_square\n\n\n\n\n-0.009973456268681513\nThe number above should match the Adjusted R-Squared from the multiple regression above. Letâ€™s explore what happens when we have multiple predictors:\n\nRPython\n\n\n\nrandom_lm_multiple <- lm(random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, random_df)\nrandom_multiple_summary <- summary(random_lm_multiple)\nrandom_multiple_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, \n    data = random_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53576 -0.21104  0.01043  0.20669  0.47170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.67088    0.10118   6.631 1.96e-09 ***\nrandom_iv_1 -0.02506    0.10550  -0.238  0.81274    \nrandom_iv_2  0.02023    0.10144   0.199  0.84232    \nrandom_iv_3 -0.26391    0.09877  -2.672  0.00886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2802 on 96 degrees of freedom\nMultiple R-squared:  0.07054,   Adjusted R-squared:  0.0415 \nF-statistic: 2.429 on 3 and 96 DF,  p-value: 0.07004\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_multiple_summary = smf.ols(formula='random_dv ~ random_iv_1 + random_iv_2 + random_iv_3', data=random_df).fit()\nrandom_multiple_summary.summary()\n\n\n\n\n\n\n\nTable\n\n\nTwo things to look for from the above: - The model with 3 predictors has higher (Multiple) R-Squared than the model with only 1 predictor. This reflects problems with over-fitting the model: the more predictors you include in your sample, the more variance in the outcome that will be explained by the predictors, even if those associations between the predictors are arbitrary (i.e.Â donâ€™t reflect anything about the general population). - Adjusted R-squared values are less susceptible to this bias of overfitting the data (but is not completely invulnerable to it). All statistical tests are vulnerable to false positives and including Adjusted R-squared values.\nRemember, the adjusted r-square is necessary for us to make claims about the general population. If we just wanted to make a claim about our sample, we would just use the r-squared, as we donâ€™t need to correct our estimate."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html",
    "href": "GeneralLinearModels/generalLinearModels.html",
    "title": "General Linear Models and Sum of Squares",
    "section": "",
    "text": "General linear models allow you to analyse data in which the dependent variable is continuous. For example, if you are analysing the height of a group of individuals, you might use one of the following analyses:\n\nt-test, comparisons between two conditions e.g.Â are males taller than females?\nregression, one or more predictors of a single outcome e.g.Â does foot size, weight etc. predict height? (Note that correlations are equivalent to a regression with a single predictor)\nANOVA, comparisons between 3 or more conditions or between multiple categorical factors, e.g.Â are there differences in height between sexes and nationalities?\n\nLinear refers to the dependent variable being continuous.\nGeneral refers to the fact that the independent variables can both be continuous (e.g.Â regression) or categorical (e.g.Â t-test or ANOVA).\nIn general linear models all analyses involve creating a model, and capturing what is and isnâ€™t explained by the model (i.e.Â the error of the model). All analyses in general linear models can be formulated as:\n\\[\nData = Model + Error\n\\]\nData: The dependent variable in your analysis Model: A model which predicts a phenomenon. This could be multiple independent variables. Error: What data isnâ€™t explained by the model."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "href": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "title": "General Linear Models and Sum of Squares",
    "section": "Dummy vs.Â effect coding for categorical variables in a model",
    "text": "Dummy vs.Â effect coding for categorical variables in a model\nGeneral Linear Models need numerical values for the predictors. As categorical variables (e.g.Â Sex) donâ€™t have a numeric value by default, we have to substitute the categories with numbers:\n\nEffect coding can be used when you have a binary categorical variable, and you allocate one level 1 and the other -1. For example, you could allocate all females the score 1, and all non-female participants -1. A disadvantage of this approach is that it works best when you have binary categorical variable, but doesnâ€™t work as well when you have 3 or more levels. For example, coding female, male and non-binary sex doesnâ€™t work well with effect coding.\nDummy coding involves allocating a 1 if someone is in a cateogory, and 0 if they are outside of the category. For example, you could allocate 1 to all your female participants, and 0 to all participants who arenâ€™t female to a variable â€œsex_femaleâ€. An advantage of this approach is that you have flexibility to have more than 2 levels, such as having â€œsex_femaleâ€, â€œsex_maleâ€ and â€œsex_nonbinaryâ€ as variables that are all either 1 or 0."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "href": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "title": "General Linear Models and Sum of Squares",
    "section": "Mean as the simplest model of data",
    "text": "Mean as the simplest model of data\nIf you want to estimate what someoneâ€™s life expectancy would be in 2007, you could look at the mean life expectancy using the gapminder data. In terms of how this corresponds to the above model:\n\\[\nData = Model + Error\n\\]\n\\[\nestimatedLifeExpectancy = mean(lifeExpectancy) + Error\n\\]\n\nRPython\n\n\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].mean()\n\n\n\n\n67.00742253521126\n\\[\nestimatedLifeExpectancy = 67.01 + Error\n\\]\nWhich could be visualised as:\n\nRPython\n\n\n\nlibrary(ggplot2)\nggplot(\n  gapminder_2007, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_jitter() +\n  geom_hline(yintercept = mean(gapminder_2007$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], gapminder_2007[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\nplt.show()\n\n\n\n\n\n\n\nPlot of Mean rank(lifeExp) and residuals\n\n\nFig. 1.\nIn English, the above model and figure allow you to predict that anyoneâ€™s life expectancy will be 67 years. However, as you can also see, thereâ€™s a huge amount of error, i.e.Â variance in life expectancy that is not explained by the model. These errors can be squared and summed to give the sum of squares, a statistic of how much error there is around the model:\n\\[\nSS = \\sum(Y_i-\\bar{Y})^2\n\\]\nWhich can be visualised as follows:\n\nRPython\n\n\n\nggplot(\n  gapminder_2007, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], (gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, color='black', s=10)\n\n# vertical lines\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nPlot of squared residuals\n\n\nFig. 2.\nYou can directly compare fig.Â 1. and fig.Â 2. to see how much error is associated with each data point compared to the model. Fig. 2. is positive because it is the distance of the data-point from the mean squared. If you added together all the squares (pink lines) in fig.Â 2. that would give you the sum of squares.\nAs you may have guessed, it is possible to have more precise models that have less error, and thus a smaller sum of squares. The sum of squares around the mean is also the total sum of squares, and the total variance. When we calculate the proportion of the variance that a model explains, we are comparing it to this variance around the mean.\nLetâ€™s explore those possibilities now."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "href": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "title": "General Linear Models and Sum of Squares",
    "section": "T-Tests",
    "text": "T-Tests\nT-tests are restricted to comparisons between 2 conditions/groups, so we will restrict the Gapminder data to allow a comparison between 2 continents. To see if life expectancy was different if you are born in Europe compared to the Americas, letâ€™s first check what the sum of squares is when you just use the mean as the model of life expectancy across these contents:\n\nRPython\n\n\n\ngapminder_americas_europe <- subset(\n  gapminder_2007,   # the data set\n  continent == \"Europe\" | continent == \"Americas\"\n)\n\nggplot(\n  gapminder_americas_europe, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_point() +\n  geom_hline(yintercept = mean(gapminder_americas_europe$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\ngapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == \"Europe\") | (gapminder_2007['continent'] == \"Americas\")]\n\ngapminder_americas_europe[\"lifeExp_rank\"] = gapminder_americas_europe[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], gapminder_americas_europe[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_americas_europe[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=gapminder_americas_europe[\"lifeExp\"], ymax=gapminder_americas_europe[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"lifeExp\")\n\nplt.show()\n\n\n\n\nFig. 3. The errors around the mean of life expectancy across Europe and American countries.\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\nOnce we square the errors in the pink lines above, weâ€™ll get the squares:\n\nRPython\n\n\n\nggplot(\n  gapminder_americas_europe, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\nsum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\n\n[1] 953.4478\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], (gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, color='black', s=10)\n# only one line may be specified; full height\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\nplt.show()\n\nsum((gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2)\n\n\n\n\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\n953.4477649818183\nAnd when you add all of these together:\n\\[\nSumOfSquares = \\sum(Y_i-\\bar{Y})^2 = 953.4478\n\\]\nSo if the model we create for a t-test would result in a smaller sum of squares then that suggests itâ€™s a more precise model for estimating life expectancy than simply using the mean as a model. This is because this would mean thereâ€™s less error in this model. Letâ€™s model this using a t-test. For this we will need to dummy code country:\n\nRPython\n\n\n\n# create a column to place 1 or -1 for each row dependent on the country\ncontEffect = NA\ncontEffect[gapminder_americas_europe$continent == \"Europe\"] = 1\ncontEffect[gapminder_americas_europe$continent == \"Americas\"] = -1\ngapminder_americas_europe = cbind(contEffect,gapminder_americas_europe)\nrmarkdown::paged_table(head(gapminder_americas_europe))\n\n\n\n  \n\n\n\n\n\n\ngapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == \"Europe\") | (gapminder_2007['continent'] == \"Americas\")]\ngapminder_americas_europe[\"contEffect\"]=0\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Europe\")]=1\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Americas\")]=-1\ncols = list(gapminder_americas_europe.columns)\ncols = list(gapminder_americas_europe.columns)\ncols = cols[len(cols)-1:len(cols):1]+cols[0:-1:1]\ngapminder_americas_europe = gapminder_americas_europe[cols]\nprint(tabulate(gapminder_americas_europe[:6], headers=gapminder_americas_europe.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\nNow that we have dummy coded the continent, we can create a new model to try to predict an individualâ€™s life expectancy based on which continent they are from\n\\[\nY = intercept + \\beta * dummyVariable + Error\n\\]\n\\[\nlifeExp = mean(lifeExp) + \\beta * contEffect + Error\n\\]\n\nY being the predicted life expectancy.\n\\(\\bar{Y}\\) being the mean life expectancy regardless of continent. For a t-test this is also the \\(intercept\\).\n\\(\\beta\\) being how much to adjust the prediction based on which continent the person is from\n\\(contEffect\\) being 1 (Europe) or -1 (Americas) to reflect which continent the participant is from\n\\(Error\\) being any error in the prediction not captured by the model\n\nTo get the \\(intercept\\) and \\(\\beta\\) for the above formula letâ€™s use the lm function in R:\n\nRPython\n\n\n\ncontinent_ttest <- lm(lifeExp ~ contEffect, gapminder_americas_europe)\n\ncontinent_ttest$coefficients[1] \n\n(Intercept) \n   75.62836 \n\ncontinent_ttest$coefficients[2]\n\ncontEffect \n   2.02024 \n\ngapminder_americas_europe$t_fit = continent_ttest$coefficients[1] + # intercept\n  continent_ttest$coefficients[2]                       * # gradient\n  gapminder_americas_europe$contEffect\n\n\nggplot(gapminder_americas_europe, aes(x = contEffect, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = contEffect,\n      yend = t_fit,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = -1.9, \n    xend = -.1, \n    y = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n    yend = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = 0.1, \n      xend = 1.9, \n      y = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n      yend = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]\n    ),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = - 1.9,\n      xend = 1.9,\n      y = mean(gapminder_americas_europe$lifeExp),\n      yend = mean(gapminder_americas_europe$lifeExp)\n    ),\n    color = \"dark green\"\n  )\n\nWarning: Use of `gapminder_americas_europe$lifeExp` is discouraged.\nâ„¹ Use `lifeExp` instead.\nUse of `gapminder_americas_europe$lifeExp` is discouraged.\nâ„¹ Use `lifeExp` instead.\n\n\n\n\n\n\n\n\nfrom scipy import stats\n\n# convert 'contEffect' to type category\ngapminder_americas_europe['contEffect'] = gapminder_americas_europe['contEffect'].astype('category')\n\n# lm 'contEffect' ~ 'lifeExp'\ncontinent_ttest = stats.linregress(gapminder_americas_europe['contEffect'],gapminder_americas_europe['lifeExp'])\n\n# show results of lm\ncontinent_ttest\n\ngapminder_americas_europe[\"contEffect\"]=0\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Europe\")]=1\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Americas\")]=-1\n\ngapminder_americas_europe['t_fit'] = continent_ttest.intercept +continent_ttest.slope * gapminder_americas_europe['contEffect']\n\ngapminder_americas_europe['t_res_square'] = (gapminder_americas_europe['lifeExp'] - gapminder_americas_europe['t_fit'])**2\n\n# calculate 'lifeExp' mean for 'contEffect' ==-1\nm1 = gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1].mean()\n\n# calculate 'lifeExp' mean for 'contEffect' ==1\nm2 = gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1].mean()\n\n# repeat lifeExp' mean for 'contEffect' ==-1\nm11=np.repeat(m1, len(gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1]), axis=0)\n\n# repeat lifeExp' mean for 'contEffect' ==1\nm22=np.repeat(m2, len(gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1]), axis=0)\n\n# create x coordinates for 'contEffect' ==-1\nx1 = np.arange(-1.98, -.05, 0.08)\n\n# create x coordinates for 'contEffect' ==1\nx2 = np.arange(0.05, 1.98, 0.065)\n\n\nfig, ax = plt.subplots(figsize =(10, 7))\nax.set_ylim([60, 80])\nplt.axhline(y=gapminder_americas_europe[\"lifeExp\"].mean(), color='green', ls='-')\nplt.hlines(y=m1,xmin=-1.99, xmax=-.04, colors='blue', lw=0.5)\nplt.hlines(y=m2,xmin=1.99, xmax=.04, colors='blue', lw=0.5)\nplt.vlines(x= x1,ymin=gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1], ymax=m11, colors='red', lw=0.5)\nplt.vlines(x= x2,ymin=gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1], ymax=m22, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"contEffect\")\n\n# add title on the y-axis\nplt.ylabel(\"lifeExp\")\n\nplt.show()\n\n\n\n\nLinregressResult(slope=2.020240000000001, intercept=75.62836, rvalue=0.4832076439158285, pvalue=0.00018637488941351192, stderr=0.502794193121279, intercept_stderr=0.5027941931212789)\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\nFig. X. Countries in the americas are dummy coded as -1 and countries in Europe are dummy coded as 1. Note that jittering has been used to help visualise variation within continents, and so all countries in Americas had a \\(contEffect\\) score of -1, even if the jittering above makes it look like participants from Europe had slightly different \\(contEffect\\) values to each other.\nSo now that weâ€™ve visualised the predictions and the error, lets summarise these errors with their sum of squares:\n\nRPython\n\n\n\n#temp_summary <- summary(lm(lifeExp ~ contEffect, data = gapminder_americas_europe))\nsummary(aov(lifeExp ~ contEffect, data = gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# between\noverall_mean <- mean(gapminder_americas_europe$lifeExp)\neurope_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1])\namerica_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1])\nss_between <- \n  sum(gapminder_americas_europe$contEffect == 1) * (europe_mean - overall_mean)^2 +\n  sum(gapminder_americas_europe$contEffect == -1) * (america_mean - overall_mean)^2\n  \ntop_half = ss_between\n\nss_within = (\n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] - europe_mean)^2) + \n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1] - america_mean)^2)\n)\n  \nbottom_half = (ss_within/(length(gapminder_americas_europe$lifeExp) - 2))\n\ntop_half/bottom_half\n\n[1] 16.14453\n\n# Compare with a t-test\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\n4.018^2\n\n[1] 16.14432\n\n# look at a t-distribution compared to an f-distribution\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nmodel = ols('lifeExp ~ contEffect', data = gapminder_americas_europe).fit()\naov_table = sm.stats.anova_lm(model, typ=2)\naov_table\n\n# between\noverall_mean = gapminder_americas_europe['lifeExp'].mean()\neurope_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1].mean()\n\namerica_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1].mean()\n\nss_between =(sum(gapminder_americas_europe['contEffect'] == 1) * (europe_mean - overall_mean)**2) + (sum(gapminder_americas_europe['contEffect'] == -1) * (america_mean - overall_mean)**2)\n\ntop_half = ss_between\n\nss_within = (sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1] - europe_mean)**2)+ sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1] - america_mean)**2))\n\nbottom_half = (ss_within/(len(gapminder_americas_europe['lifeExp']) - 2))\n\ntop_half/bottom_half\n\n# Compare with a t-test\nfrom scipy.stats import ttest_ind\n\nt_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\nt_test\nt_test.statistic **2\n\n\n\n\n\n\n\nTable\n\n\nTtest_indResult(statistic=4.018025720342183, pvalue=0.00018637488941352037)\n\n16.144530689331322\nSo the new sum of squares is 730.8276, which is smaller than it was when we just used the mean regardless of continent (953.4478) which also summarises the total variance (around the mean). In fact, we can use these 2 numbers to calculate the \\(r^2\\) value (i.e.Â what proportion of the variance around the mean is explained by the model). The amount of variance explained by the model can be calculated by:\n\\[\ntotalSumOfSquares - modelSumOfSquares = totalError - modelError\n\\]\nThis allows us to calculate an r-value and thus a p-value:\n\nRPython\n\n\n\nthis_r2 = 1 - sum(gapminder_americas_europe$t_res_squared)/sum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\nthis_r = sqrt(this_r2)\nthis_r\n\n[1] 1\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nthis_r2\n\n[1] 1\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport math\n\nthis_r2 = 1- sum(gapminder_americas_europe['t_res_square'])/ (sum((gapminder_americas_europe['lifeExp'] - gapminder_americas_europe['lifeExp'].mean())**2))\nthis_r=math.sqrt(this_r2)\nthis_r\nt_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\nt_test\n\nmodel = ols('lifeExp ~ contEffect', data = gapminder_americas_europe).fit()\naov_table = sm.stats.anova_lm(model, typ=2)\naov_table\n\nthis_r2\n\n\n\n\nYou may notice above that the manually calculated \\(r^2\\) value is identical to the â€œMultiple R-Squaredâ€, rather than the â€œAdjusted R-squaredâ€. So whatâ€™s the difference between r-squared and adjusted r-squared?\n\nEffect sizes (eta-squared and partial eta-squared)\n\nRPython\n\n\n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\n\nCall:\nlm(formula = lifeExp ~ contEffect, data = gapminder_americas_europe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6921  -2.1364   0.4494   2.5671   7.0449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  75.6284     0.5028 150.416  < 2e-16 ***\ncontEffect    2.0202     0.5028   4.018 0.000186 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.713 on 53 degrees of freedom\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.219 \nF-statistic: 16.14 on 1 and 53 DF,  p-value: 0.0001864\n\n222.62 /(222.62 +13.79)\n\n[1] 0.9416691\n\n\n\n\n\nt_test\naov_table\n222.62 /(222.62 +13.79)\n\n\n\n\n0.941669134131382\n\nRPython\n\n\n\nmale_female_height <- data.frame(\n  sex = c(\"male\",\"male\",\"female\",\"female\",\"female\",\"female\",\"female\",\"female\"),\n  height = c(2.5,2.2,1.5,1.5,1.4,1.4,1.3,1.3),\n  sex_dummy = c(-1,-1,1,1,1,1,1,1)\n)\nmean(male_female_height$height[male_female_height$sex == \"male\"])\n\n[1] 2.35\n\nmean(male_female_height$height[male_female_height$sex == \"female\"])\n\n[1] 1.4\n\nsummary(lm(height ~ sex_dummy, data = male_female_height))\n\n\nCall:\nlm(formula = height ~ sex_dummy, data = male_female_height)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.15  -0.10   0.00   0.10   0.15 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.87500    0.04859  38.587 2.02e-08 ***\nsex_dummy   -0.47500    0.04859  -9.775 6.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.119 on 6 degrees of freedom\nMultiple R-squared:  0.9409,    Adjusted R-squared:  0.9311 \nF-statistic: 95.56 on 1 and 6 DF,  p-value: 6.592e-05\n\n\n\n\n\nmale_female_height = {\n    'sex': [\"male\",\"male\",\"female\",\"female\",\"female\",\"female\",\"female\",\"female\"],\n    'height': [2.5,2.2,1.5,1.5,1.4,1.4,1.3,1.3],\n    'sex_dummy': [-1,-1,1,1,1,1,1,1]\n}\nmale_female_height = pd.DataFrame(male_female_height)\nmale_female_height['height'][male_female_height['sex'] == \"male\"].mean()\nmale_female_height['height'][male_female_height['sex'] == \"female\"].mean()\n\nimport statsmodels.formula.api as smf\nmodel2 = smf.ols(formula='height ~ sex_dummy', data=male_female_height).fit()\nmodel2.summary()\n\n\n\n\n2.35\n\n1.4000000000000001\n\n\n\nTale\n\n\nTo show that weâ€™ve achieved the same as a t-test, letâ€™s run a between subjects t-test that assumes the variance is equal between the groups (which is an assumption of a general linear model), and see if the p-values are the same:\n\nRPython\n\n\n\n#953.4478/730.8276\ncontinent_ttest <- t.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  # general linear models assume the variance between conditions is equal\n  var.equal = T\n)\ncontinent_model <- summary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\ncontinent_ttest$p.value\n\n[1] 0.0001863749\n\ncontinent_model$coefficients[2,4] # p-value for the continent as a predictor\n\n[1] 0.0001863749\n\n\n\n\n\ncontinent_ttest = t_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\n\ncontinent_ttest.pvalue\n\n\n\n\nThere are some advantages of conducting a t-test using the â€œlmâ€ functionality:\n\nYou can capture residuals\nYou have more flexibility to make more complex models\n\nLetâ€™s now see how we can proceed if we have a more complex design, i.e.Â 3 or more levels and/or more than 1 factor, using ANOVAs.\n\nRPython\n\n\n\n## Automatic calculation\ngapminder_3_continents <- subset(\n  gapminder_2007, \n  continent == \"Europe\" | continent == \"Americas\" | continent == \"Africa\"\n)\n\nsummary(aov(lifeExp ~ factor(continent), data = gapminder_3_continents))\n\n                   Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(continent)   2  12017    6008   114.4 <2e-16 ***\nResiduals         104   5461      53                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Manual calculation\nss_between = (\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Europe\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Americas\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Africa\") \n)\n\ntotalSS = sum((gapminder_3_continents$lifeExp - mean(gapminder_3_continents$lifeExp))^2)\n\nshortcut_ss_within = totalSS - ss_between\n\nss_within_long = sum(\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]))^2\n)\n\n(ss_between/2)/(ss_within_long/(length(gapminder_3_continents$lifeExp)-3))\n\n[1] 114.4212\n\n\n\n\n\ngapminder_3_continents['continent'] = gapminder_3_continents['continent'].astype('category')\nmodel3 = ols('lifeExp ~ continent', data = gapminder_3_continents).fit()\naov_table3 = sm.stats.anova_lm(model3, typ=2)\naov_table3\n\nss_between = (gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Europe\")+(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Americas\")+(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Africa\")\n\ntotalSS = sum((gapminder_3_continents['lifeExp'] - gapminder_3_continents['lifeExp'].mean())**2)\n\nwithin1_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"].mean())**2\nwithin1= sum(within1_all)\n\nwithin2_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"].mean())**2\nwithin2= sum(within2_all)\n\nwithin3_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"].mean())**2\nwithin3= sum(within3_all)\n\nss_within_long = within1 + within2 + within3\n\n(ss_between/2)/(ss_within_long/(len(gapminder_3_continents['lifeExp'])-3))\n\n\n\n\n114.42116407168957\nTo visualise this\n\nRPython\n\n\n\nlm_3_continents <- summary(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\nlm_3_continents$coefficients\n\n                          Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)               54.80604   1.004904 54.53856 2.466786e-78\nfactor(continent)Americas 18.80208   1.763600 10.66119 2.243512e-18\nfactor(continent)Europe   22.84256   1.661388 13.74908 3.900175e-25\n\nmean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])\n\n[1] 77.6486\n\ngapminder_3_continents$continent_mean = lm_3_continents$coefficients[1,1] # intercept, which is mean for Africa as there are only \"Americas\" and \"Europe\" factors.\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] + lm_3_continents$coefficients[2,1]\n\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] + lm_3_continents$coefficients[3,1]\n\n\n\nggplot(gapminder_3_continents, aes(x = continent, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = continent,\n      yend = continent_mean,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = 0.55, \n    xend = 1.45, \n    y = lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 1.55, \n    xend = 2.45, \n    y = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 2.55, \n    xend = 3.45, \n    y = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) +\n  geom_segment(\n    aes(\n      x = 0.55,\n      xend = 3.45,\n      y = mean(gapminder_3_continents$lifeExp),\n      yend = mean(gapminder_3_continents$lifeExp),\n      color = \"Overall Mean\"\n    )\n  )\n\nWarning: Use of `gapminder_3_continents$lifeExp` is discouraged.\nâ„¹ Use `lifeExp` instead.\nUse of `gapminder_3_continents$lifeExp` is discouraged.\nâ„¹ Use `lifeExp` instead.\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nspec = dict(x=\"continent\", y=\"lifeExp\", data=gapminder_3_continents)\nfig, ax = plt.subplots(figsize =(7, 5))\nsns.stripplot(**spec, size=4, palette=\"deep\")\nsns.pointplot(**spec, join=False, ci=0, capsize=.7, scale=0, palette=\"deep\")\nplt.axhline(y=gapminder_3_continents[\"lifeExp\"].mean(), color='red', ls='-')\nplt.show()\n\n\n\n\nFig. XXX. Our model in the figure above considers the distance from each of the continentâ€™s mean to the overall mean as part of the explained variance. For each data point, the squared distance from the the mean line to the overall mean line is part of the sum of squares at the top of the formula.\n\n\n2 x 2 ANOVA\nLetâ€™s create data to allow us to compare between 2 years, and between Europe and Americas\n\nRPython\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 <- subset(\n  gapminder,   # the data set\n  year == 2002 & continent == \"Africa\" |\n  year == 2007 & continent == \"Africa\" |\n  year == 2002 & continent == \"Europe\" |\n  year == 2007 & continent == \"Europe\"\n)\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 = gapminder.loc[(gapminder['year'] == 2002) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2002) & (gapminder['continent'] == \"Europe\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Europe\") ]\n\nfrom statsmodels.formula.api import ols\n\nfit = ols('lifeExp ~ C(year) + C(continent)', data=gapminder_2_by_2).fit() \n\nfit.summary()\n\nlm_4_continents_aov_table = sm.stats.anova_lm(fit, typ=2)\nlm_4_continents_aov_table\n\n\n\n\nmanual calculation of f-value for 2 x 2\n\nRPython\n\n\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâœ” purrr   0.3.5      \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\noverallMeanLifeExp = mean(gapminder_2_by_2$lifeExp)\ntotalVar = sum((gapminder_2_by_2$lifeExp - mean(gapminder_2_by_2$lifeExp))^2)\ngapminder_2_by_2 %>%\n  group_by(continent, year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> year_continent_means\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nyear_continent_means\n\n# A tibble: 4 Ã— 5\n# Groups:   continent [2]\n  continent  year mean_lifeExp countries betweenSS\n  <fct>     <int>        <dbl>     <int>     <dbl>\n1 Africa     2002         53.3        52     4396.\n2 Africa     2007         54.8        52     3094.\n3 Europe     2002         76.7        30     6033.\n4 Europe     2007         77.6        30     6866.\n\nsum(year_continent_means$betweenSS)\n\n[1] 20389.47\n\ntotalVar\n\n[1] 30311.89\n\nsum(year_continent_means$betweenSS)/totalVar\n\n[1] 0.6726556\n\n(sum(year_continent_means$betweenSS))/totalVar\n\n[1] 0.6726556\n\ndf_total <- length(gapminder_2_by_2$country) - 1\ndf_res <- length(gapminder_2_by_2$country) - \n  1 - #data points\n  3   # predictors\n\nss_res = totalVar - sum(year_continent_means$betweenSS)\n\n1 - (ss_res/df_res)/(totalVar/df_total)\n\n[1] 0.6665179\n\n##\n# break down of types of variance\n##\ncontinent_df <- gapminder_2_by_2 %>%\n  group_by(continent) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(continent_df$betweenSS)\n\n[1] 20318.97\n\nyear_df <- gapminder_2_by_2 %>%\n  group_by(year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(year_df$betweenSS)\n\n[1] 67.79278\n\n##\n# interaction\n##\nsum(year_continent_means$betweenSS) - sum(continent_df$betweenSS) - sum(year_df$betweenSS)\n\n[1] 2.70036\n\ngapminder\n\n# A tibble: 1,704 Ã— 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# â€¦ with 1,694 more rows\n\n20319 +\n68 +\n3\n\n[1] 20390\n\n\n\n\n\nfrom siuba import group_by, summarize, _\n\noverallMeanLifeExp = gapminder_2_by_2['lifeExp'].mean()\n\ntotalVar = sum((gapminder_2_by_2['lifeExp'] - gapminder_2_by_2['lifeExp'].mean())**2)\n\n\nyear_continent_means=(gapminder_2_by_2\n  >> group_by(_.continent, _.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_continent_means['betweenSS'] = year_continent_means['countries'] * ((overallMeanLifeExp - year_continent_means['mean_lifeExp'])**2)\n\nprint(tabulate(year_continent_means, headers=year_continent_means.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nsum(year_continent_means['betweenSS'])\n\ntotalVar\n\nsum(year_continent_means['betweenSS'])/totalVar\n\ndf_total = len(gapminder_2_by_2['country']) - 1\ndf_res = len(gapminder_2_by_2['country']) - 1 - 3\nss_res = totalVar - sum(year_continent_means['betweenSS'])\n1 - (ss_res/df_res)/(totalVar/df_total)\n\ncontinent_df=(gapminder_2_by_2\n  >> group_by(_.continent)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\ncontinent_df['betweenSS'] = continent_df['countries'] * ((overallMeanLifeExp - continent_df['mean_lifeExp'])**2)\nsum(continent_df['betweenSS'])\n\nyear_df=(gapminder_2_by_2\n  >> group_by(_.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_df['betweenSS'] = year_df['countries'] * ((overallMeanLifeExp - year_df['mean_lifeExp'])**2)\nsum(year_df['betweenSS'])\n\nsum(year_continent_means['betweenSS']) - sum(continent_df['betweenSS']) - sum(year_df['betweenSS'])\n\nprint(tabulate(gapminder[:10], headers=gapminder.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n20319 +68 +3\n\n\n\n\n\n\n3 way ANOVA\n\nRPython\n\n\n\ngapminder_2_by_2$pop_split = \"high\"\ngapminder_2_by_2$pop_split[gapminder_2_by_2$pop < median(gapminder_2_by_2$pop)] = \"low\"\n\ngapminder_2_by_2 %>%\n  group_by(continent, year, pop_split) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> three_way_summary\n\n`summarise()` has grouped output by 'continent', 'year'. You can override using\nthe `.groups` argument.\n\nsum(three_way_summary$betweenSS)\n\n[1] 20403.63\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n                                                  Df Sum Sq Mean Sq F value\nfactor(year)                                       1     68      68   1.067\nfactor(continent)                                  1  20319   20319 319.911\nfactor(pop_split)                                  1     13      13   0.212\nfactor(year):factor(continent)                     1      3       3   0.046\nfactor(year):factor(pop_split)                     1      0       0   0.000\nfactor(continent):factor(pop_split)                1      0       0   0.007\nfactor(year):factor(continent):factor(pop_split)   1      0       0   0.000\nResiduals                                        156   9908      64        \n                                                 Pr(>F)    \nfactor(year)                                      0.303    \nfactor(continent)                                <2e-16 ***\nfactor(pop_split)                                 0.646    \nfactor(year):factor(continent)                    0.830    \nfactor(year):factor(pop_split)                    0.989    \nfactor(continent):factor(pop_split)               0.931    \nfactor(year):factor(continent):factor(pop_split)  0.988    \nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent) * factor(pop_split), \n    data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5189  -4.8868  -0.3127   3.1208  22.0864 \n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   52.96628\nfactor(year)2007                                               1.53805\nfactor(continent)Europe                                       23.52019\nfactor(pop_split)low                                           0.69131\nfactor(year)2007:factor(continent)Europe                      -0.59779\nfactor(year)2007:factor(pop_split)low                         -0.06377\nfactor(continent)Europe:factor(pop_split)low                  -0.26305\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low  0.07923\n                                                              Std. Error\n(Intercept)                                                      1.59392\nfactor(year)2007                                                 2.21201\nfactor(continent)Europe                                          2.60286\nfactor(pop_split)low                                             2.21201\nfactor(year)2007:factor(continent)Europe                         3.65535\nfactor(year)2007:factor(pop_split)low                            3.12825\nfactor(continent)Europe:factor(pop_split)low                     3.65535\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    5.16944\n                                                              t value Pr(>|t|)\n(Intercept)                                                    33.230  < 2e-16\nfactor(year)2007                                                0.695    0.488\nfactor(continent)Europe                                         9.036 5.91e-16\nfactor(pop_split)low                                            0.313    0.755\nfactor(year)2007:factor(continent)Europe                       -0.164    0.870\nfactor(year)2007:factor(pop_split)low                          -0.020    0.984\nfactor(continent)Europe:factor(pop_split)low                   -0.072    0.943\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low   0.015    0.988\n                                                                 \n(Intercept)                                                   ***\nfactor(year)2007                                                 \nfactor(continent)Europe                                       ***\nfactor(pop_split)low                                             \nfactor(year)2007:factor(continent)Europe                         \nfactor(year)2007:factor(pop_split)low                            \nfactor(continent)Europe:factor(pop_split)low                     \nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.97 on 156 degrees of freedom\nMultiple R-squared:  0.6731,    Adjusted R-squared:  0.6585 \nF-statistic: 45.89 on 7 and 156 DF,  p-value: < 2.2e-16\n\n20319 +\n68 +\n3 + \n  13\n\n[1] 20403\n\n20403.63\n\n[1] 20403.63\n\n\n\n\n\ngapminder_2_by_2['pop_split']=0\n\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']<np.median(gapminder_2_by_2['pop'])]=\"low\"\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']>=np.median(gapminder_2_by_2['pop'])]=\"high\"\n\nthree_way_summary = (gapminder_2_by_2\n  >> group_by(_.continent, _.year, _.pop_split)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nthree_way_summary['betweenSS'] = three_way_summary['countries'] * ((overallMeanLifeExp - three_way_summary['mean_lifeExp'])**2)\nsum(three_way_summary['betweenSS'])\n\n\n\nfit2 = ols('lifeExp ~ C(year) + C(continent) + C(pop_split)', data=gapminder_2_by_2).fit() \n\nfit2.summary()"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html",
    "href": "GeneralLinearModels/TTests.html",
    "title": "T-Tests(incomplete)",
    "section": "",
    "text": "In all general linear models you are trying to compare how much of the variance is explained by a model compared to whatâ€™s not being explained by a model. In short\n\\[\n\\frac{var_{explained}}{var_{unexplained}} = \\frac{SS_{explained}}{SS_{unexplained}}\n\\]\nFor each type of t-test, the way we calculate this is slightly different:"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "href": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "One-Sample t-tests",
    "text": "One-Sample t-tests\n\nGLM approach\nOne sample t-tests try to explain whether variance of data is better explained around one specific value (sample mean) compared to another (previously assumed value). For example, imagine that you wanted to test whether life expectancy is higher than 55 across the world:\n\nYour \\(\\mu\\) would be 55. This can be thought of as the assumed population mean that we want to use our sample to test.\nYour \\(\\bar{x}\\) would be the sample mean.\n\nLetâ€™s visualise these values using gapminder data from 2007:\n\nlibrary(ggplot2)\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007\n)\nggplot(gapminder_2007, aes(x=year,y=lifeExp)) + \n  geom_jitter() + \n  xlab(\"\") + \n  theme(axis.text.x = element_blank()) +\n  theme(axis.ticks.x = element_blank()) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = 55,\n      yend = 55,\n      color = \"Mu\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = mean(lifeExp),\n      yend = mean(lifeExp),\n      color = \"Sample Mean\"\n    )\n  )\n\n\n\n\nWe want to create a model that explains any variance around the population mean (\\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\nY is the data point value you are trying to predict. Note that for this formula you will always have the same prediction.\n\\(\\bar{y}\\) is mean of y. You are only interested in whether predicting y based on yâ€™s mean captures a significant amount of the variance of the y-values around the \\(\\mu\\).\n\\(e\\) is the error, i.e.Â the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the â€œpopulationâ€ mean (and will also suggested that there is significant reason to reject the population mean). The total variance using sum of squares is thus:\n\\[\nSS_{total} = \\sum(x_i-\\mu)^2\n\\]\nWhich for the above data would give us:\n\nsum((gapminder_2007$lifeExp - 55)^2)\n\n[1] 41025.16\n\n\nSo your explained variance by this model is any difference between the Mu (\\(\\mu\\)) and the sample mean (\\(\\bar{x}\\)). To summarise this using sum of squares, for each data point you subtract the two from each other and square them, as this difference is what we can explain of variance away from the MU:\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n[1] 20473.3\n\n\nUnexplained variance would be the residuals around the sample mean, as this is variance that is not explained by the model. Conveniently, we can calculate the sum of squared around the sample mean quite elegantly:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n[1] 20551.85\n\n\nSo the F-value should be:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n[1] 140.4611\n\n\nF-values are squares of t-values, so letâ€™s see if this is true here also:\n\nsqrt(f_value)\n\n[1] 11.85163\n\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\nGreat. So now that weâ€™ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\nT is the t-value\nF is the f-value\n\\(SS_{explained}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexplained}\\) is the sum of squares of the data not explained by the model (i.e.Â the residuals)\n\\(df_{explained}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and itâ€™s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so itâ€™s less intuitive that there are 2 levels)."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Paired samples t-tests",
    "text": "Paired samples t-tests\nPaired samples t-tests can be approached like 1-sample t-tests, but you first of all need to collapse the data to have a single variable to compare to a \\(\\mu\\) of zero. Letâ€™s do this for gapminder data, comparing life expectancies between 2002 and 2007:\n\ngapminder_2002_2007_life_exp <- gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]\nt.test(gapminder_2002_2007_life_exp, mu = 0)\n\n\n    One Sample t-test\n\ndata:  gapminder_2002_2007_life_exp\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean of x \n   1.3125 \n\n\nThe above suggests that life expectancy was significanctly different. Letâ€™s see if we get the exact same value when we use a paired t-test in R:\n\nt.test(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002], paired=T)\n\n\n    Paired t-test\n\ndata:  gapminder$lifeExp[gapminder$year == 2007] and gapminder$lifeExp[gapminder$year == 2002]\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean difference \n         1.3125 \n\n\nLooks identical. Letâ€™s compare formulas to see why this is:\n\\[\nt_{paired} = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sigma_{pooled}/\\sqrt{N}} = \\frac{\\bar{x_3}}{\\sigma_{pooled}/\\sqrt{N}}\n\\]\nWhere\n\n\\(\\bar{x_1}\\) is the mean of condition 1\n\\(\\bar{x_2}\\) is the mean of condition 2\n\\(\\bar{x_3}\\) is the mean of the result you get when you subtract condition 2 from condition 1 for each participant, i.e.Â \\(mean(x_1-x_2)\\).\n\\[\n\\sigma_{pooled}  = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}} OR \\frac{\\sum(x_1 - x_2)^2}{N-1}\n\\] One way effectively gets the average of the standard deviations of condition and 1. The second way gets the standard deviation of the differences between conditions 1 and 2. Both give you the same outcome.\n\\(N\\) is the number of participants\n\nYou can rewrite the above formula to compare \\(\\bar{x_3}\\) to \\(\\mu\\), as we know \\(\\mu\\) is zero, which would make this formula (effectively) identical to the one above for one-sample t-tests:\n\\[\n\\frac{\\bar{x_3} - \\mu}{\\sigma_{pooled}/\\sqrt{N}}\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Independent Samples t-tests",
    "text": "Independent Samples t-tests\n\nGLM approach\nFor an independent samples t-test we can create a simple model based on the means of the two groups. You can either dummy or effect code the groups, so weâ€™ll do both to look at how the output is slightly different each way. Weâ€™ll use the gapminder data to see if there are differences in life expectancies between the Americas and Europe in 2007 to illustrate these:\n\ngapminder_2007_Am_Eu <- subset(\n  gapminder,   # the data set\n  year == 2007 & continent == \"Americas\" | \n  year == 2007 & continent == \"Europe\"\n)\n\n\nDummy coding\nOne way to make a model for a t-test is to have a variable that is 1 for one level, and 0 for the other level (note that this gets more complicated if you are going an ANOVA with 3 or more levels). Letâ€™s create a new variable for continent that is 1 if the country is in the Americas, and 0 if itâ€™s not:\n\ngapminder_2007_Am_Eu$americas_dummy = ifelse(gapminder_2007_Am_Eu$continent == \"Americas\", 1,0)\nrmarkdown::paged_table(gapminder_2007_Am_Eu)\n\n\n\n  \n\n\n\nNow that we have added our dummy code, we can write a model for what we expect life expectancy to be for each country:\n\\[\nlifeExp = \\beta_{americas}*mean(lifeExp_{americas}) + \\beta_{europe}*mean(lifeExp_{europe}) + e\n\\]\n\n\nEffect coding"
  },
  {
    "objectID": "categorical/ contingency.html",
    "href": "categorical/ contingency.html",
    "title": "Contingency(incomplete)",
    "section": "",
    "text": "library(psych)\ncontingency_data <- matrix(\n  data = c(45,55,120,80),\n  nrow = 2,\n  ncol = 2\n)\ncontingency_data\n\n     [,1] [,2]\n[1,]   45  120\n[2,]   55   80\n\nmanual_phi = (\n  contingency_data[1,1] * contingency_data[2,2] - \n    contingency_data[1,2] * contingency_data[2,1]\n  )/sqrt(\n    sum(contingency_data[1,]) *\n    sum(contingency_data[2,]) *\n    sum(contingency_data[,1]) *\n    sum(contingency_data[,2])\n  )\nphi(contingency_data, digits = 7)\n\n[1] -0.1421338\n\nmanual_phi\n\n[1] -0.1421338\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics",
    "section": "",
    "text": "In statistics, a variable is any (measurable) attribute that describes any organism or object. Itâ€™s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-hypothesis",
    "href": "statsBasics/statsBasics.html#what-is-a-hypothesis",
    "title": "Statistics Basics",
    "section": "What is a hypothesis?",
    "text": "What is a hypothesis?\nA(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e.Â randomly) if your main hypothesis were not true. We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability.\n\n\n\n\n\n\n\nNote\n\n\n\nIn terms of whether your research conclusions are valid, there are 4 broad possible outcomes:\n\n\n\n\n\n\n\n\n\nTrue\nFalse\n\n\n\n\nPositive\nYou conclude there is an effect in your sample and this reflects the general population.\n(i.e.Â the effect you found was real)\ne.g.Â You conclude that big dogs have more fur than small dogs (presumably this is true as there is more dog to put fur on in big dogs, right?)\nYou conclude there is an effect in your sample, but this does not reflect the general population.\n(i.e.Â the effect you found was not real)\ne.g.Â You conclude that half-full glasses have more water than half-empty glasses\nThis is a type 1 error\n\n\nNegative\nYou conclude that there is no effect in your sample, and this reflects the general population.\n(i.e.Â you are correct to say that there is no effect)\ne.g.Â You conclude that half-full glasses have as much water as a half-empty glasses\nYou conclude that there is no effect in your sample, but this does not reflect the general population.\n(i.e.Â you are incorrect to say that there is no effect)\ne.g.Â You conclude that big dogs do not have more fur than small dogs\nThis is a type 2 error"
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\n\\(Alpha\\) (âº) is the p-value threshold that identifies if a result is â€œsignificantâ€ or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is â€œsignificantâ€ (i.e.Â so unlikely that this would have happened by chance that we conclude this didnâ€™t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e.Â we say there is an effect when there isnâ€™t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "href": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "title": "Statistics Basics",
    "section": "What is power (and what is the beta value)",
    "text": "What is power (and what is the beta value)\nPower tells you how likely you are to get a significant result (assuming the effect you are investigating is real) based on:\n\nthe sample size of participants you have. The more participants you have, the more powerful your analysis will be (i.e.Â the more likely you are to find a significant result).\nthe effect size (e.g.Â Cohenâ€™s d). The bigger the effect size, the more powerful your analysis will be. Effect sizes in power calculations are generally based on previous studies with similar paradigms.\nthe \\(alpha\\) (âº) threshold. A smaller \\(alpha\\) threshold requires a higher powered analysis to get a significant result.\n\nTo explore this, you might find the following interactive tool helpful to see what happens when you change the above:\nhttps://observablehq.com/@patrickmineault/interactive-demo-in-pure-js\n\nQuestion 1\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\nAlpha values areâ€¦\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/probability.html",
    "href": "statsBasics/probability.html",
    "title": "Probability",
    "section": "",
    "text": "This concept is somewhat foundational to almost all the other content in this website, but is often overlooked in textbooks. An advantage about understanding it is that will give you a more complete understanding later when we go through concepts such as probability distributions such as binomial and normal distributions."
  },
  {
    "objectID": "statsBasics/probability.html#probability-of-single-outcomes",
    "href": "statsBasics/probability.html#probability-of-single-outcomes",
    "title": "Probability",
    "section": "Probability of single outcomes",
    "text": "Probability of single outcomes\nIn informal language we might talk about there being a 50% chance of something happening, such as a 50% chance of getting â€œheadsâ€ when flipping a coin. At the risk of telling you what you already know, if thereâ€™s a 50% chance of getting â€œheadsâ€, then we assume there is a 50% chance of getting â€œtailsâ€, because the likelihood of all outcomes put together must equal 100% or else it suggests thereâ€™s another outcome you havenâ€™t considered. For example, if a flipped coin lands on heads 45% of the time, and on tails 45% of the time, then (45% + 45% =) 90% of the time it will land on heads or tails, which means 10% of the time it will do one or more other things (e.g.Â land on neither side). Going forward we will assume weâ€™re using coins that can only land on heads or tails.\nWithin statistics we tend to use decimalised numbers rather than percentages, so a 10% chance is written as .1, a 50% chance is written as .5 and 100% chance is written as 1. This is important as when you have considered all possible outcomes you should be able to add their likelihoods up to make 1."
  },
  {
    "objectID": "statsBasics/probability.html#probability-of-combinations-of-outcomes",
    "href": "statsBasics/probability.html#probability-of-combinations-of-outcomes",
    "title": "Probability",
    "section": "Probability of combinations of outcomes",
    "text": "Probability of combinations of outcomes\nImagine that you want to predict the likelihood of flipping a coin on heads or tails a certain number of times in a row. The outcome of each coin flip is binary, i.e.Â there is only 1 possible outcome out of 2 options. If this isnâ€™t a biased coin, we can calculate some basic expectations about what will happen after 2 flips of the coin:\n\nEach flip of the coin has a 0.5 chance of landing on Heads.\nIf we wanted to calculate the likelihood of 2 flips of heads, thereâ€™s a 0.5 chance we get the first heads flip, and then another 0.5 chance weâ€™ll get the second head flip. To summarise this, we can multiply both 0.5 chances together to get 0.25.\nTo summarise the likelihood of all combinations we could make the following table:\n\n\n\n\n\n\n\n\n\nFirst flip and likelihood\nSecond flip and likelihood\nOverall likelihood\n\n\nHeads (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nHeads (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\n\nLikelihood of any of the above happening\n.25 + .25 + .25 + .25 = 1\n\n\n\nTo achieve what you have above, you need to consider every combination of possible outcomes and calculate their likelihood. A useful quality check is to make sure that when you add the overall likelihoods together you get 1, otherwiseâ€¦\n\nIf you have less than 1 it suggests you have overlooked an outcome\nIf you have more than 1 it suggests you have either\n\nOverestimated the likelihood of a specific outcome\nTreated overlapping outcomes as if they are mutually exclusive\n\n\n\nMutually exclusive outcomes\nIn the above example each of the four combinations are mutually exclusive, as they are specific and distinct. It is impossible that your flip of your coins was both:\n\nheads and then tails\nheads and then heads\n\nEven though the first flip is heads in both outcomes, the second flip is distinct and so the two flips could not be both outcomes as they are exclusive.\nHowever, there are some scenarios in which itâ€™s less clear whether two outcomes are mutually exclusive. The chance of rolling any side of a die is 1 in 6. The chance of rolling a certain number and above can be calculated by counting the number of valid sides there are and then dividing by 6. For example, the chance of rolling 3 and above is 4 in 6 because there are 4 valid sides (3,4,5 and 6).\nImagine that you are rolling dice twice in a row - what is the likelihood that one role will be at least 3 and the other will be at least 4? Letâ€™s start with an incorrect answer:\n\ndice_outcomes <- data.frame(\n  Description <- c(\n    \"First is 3 or more, second is 4 or more\", # valid outcome\n    \"First is 4 or more, second is 3 or more\", # valid outcome\n    \"First is 3 or more, second is 3 or LESS\", # invalid outcome\n    \"First is 4 or more, second is 2 or LESS\", # invalid outcome\n    \"First is 3 or LESS, second is 4 or more\", # invalid outcome\n    \"First is 2 or LESS, second is 3 or more\", # invalid outcome\n    \"First is 2 or LESS, second is 2 or LESS\"  # invalid outcome\n  ),\n  likelihood = c(\n    (4/6) * (3/6), # \"First is 3 or more, second is 4 or more\", # valid outcome\n    (3/6) * (4/6), # \"First is 4 or more, second is 3 or more\", # valid outcome\n    (3/6) * (3/6), # \"First is 3 or more, second is 3 or LESS\", # invalid outcome\n    (3/6) * (2/6), # \"First is 4 or more, second is 2 or LESS\", # invalid outcome\n    (3/6) * (3/6), # \"First is 3 or LESS, second is 4 or more\", # invalid outcome\n    (2/6) * (4/6), # \"First is 2 or LESS, second is 3 or more\", # invalid outcome\n    (2/6) * (2/6)  # \"First is 2 or LESS, second is 2 or LESS\"  # invalid outcome\n  )\n)\nknitr::kable(dice_outcomes)\n\n\n\n\n\n\n\n\nDescriptionâ€¦.c..First.is.3.or.more..second.is.4.or.moreâ€¦.First.is.4.or.more..second.is.3.or.moreâ€¦\nlikelihood\n\n\n\n\nFirst is 3 or more, second is 4 or more\n0.3333333\n\n\nFirst is 4 or more, second is 3 or more\n0.3333333\n\n\nFirst is 3 or more, second is 3 or LESS\n0.2500000\n\n\nFirst is 4 or more, second is 2 or LESS\n0.1666667\n\n\nFirst is 3 or LESS, second is 4 or more\n0.2500000\n\n\nFirst is 2 or LESS, second is 3 or more\n0.2222222\n\n\nFirst is 2 or LESS, second is 2 or LESS\n0.1111111\n\n\n\n\nsum(dice_outcomes$likelihood)\n\n[1] 1.666667\n\n\nWhen we add all the outcome likelihoods together we get more than 1 ğŸ˜’. This reflects us (or me, you didnâ€™t ask me to make the above table) treating some outcomes as if they are exclusive when they are not. For example, the two valid outcomes have overlap as they both include the likelihoods that both dice are of 4 or more. This has inflated the likelihood of the valid outcomes when you add these outcomes together, as you have doubled up on the overlap. To visualise this:\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_rect(\n    mapping = aes(\n      xmin = 2.5,\n      xmax = 6.5,\n      ymin = 3.5,\n      ymax = 6.5\n    ),\n    fill = \"blue\",\n    alpha = .5\n  ) +\n  geom_rect(\n    mapping = aes(\n      xmin = 3.5,\n      xmax = 6.5,\n      ymin = 2.5,\n      ymax = 6.5\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) +\n  theme_bw() +\n  theme(\n    plot.background = element_blank(),\n    #panel.grid.minor = element_line(color = \"black\", linewidth =  2),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank()\n  ) + \n  coord_fixed() +\n  xlab(\"First roll\") +\n  ylab(\"Second roll\") + \n  geom_vline(\n    xintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),\n    color=\"white\"\n  ) + \n  geom_hline(\n    yintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),\n    color=\"white\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 6, by = 1)) +\n  scale_y_continuous(breaks = seq(0, 6, by = 1)) \n\n\n\n\nIn the above visualisation of all possible dice role combinations, the proportion of the area covered reflects the likelihood of each valid combination highlighted: â€œFirst is 3 or more, second is 4 or moreâ€ in blue and â€œFirst is 4 or more, second is 3 or moreâ€ in red. As you can see, there is a lot of overlap (purple/pink), reflecting that these were not mutually exclusive possibilities, and so adding them together will inflate the estimated likelihood of either of them happening.\nAn elegant solution is to subtract the likelihood associated with repetition from your original calculation:\n\n(4/6) * (3/6) + # \"First is 3 or more, second is 4 or more\", # valid outcome\n(3/6) * (4/6) - # \"First is 4 or more, second is 3 or more\", # valid outcome\n(3/6) * 3/6     # overlap between the two valid outcomes above \n\n[1] 0.4166667\n\n\nTo confirm this is correct, we can use R to count how many times out of 36 the dice will be valid:\n\nvalid_dice <- matrix(FALSE,nrow = 6,ncol = 6)\nfor(i in 1:6){\n  for(j in 1:6){\n    if(i >= 3 & j >= 4){\n      valid_dice[i,j] = TRUE\n    }\n    if(i >= 4 & j >= 3){\n      valid_dice[i,j] = TRUE\n    }\n  }\n}\nsum(valid_dice)/ # number of valid roll combinations\n36               # total number of roll combinations\n\n[1] 0.4166667\n\n\nWe get the same output."
  },
  {
    "objectID": "statsBasics/probability.html#the-likelihood-of-something-not-happening-is-often-a-useful-shortcut",
    "href": "statsBasics/probability.html#the-likelihood-of-something-not-happening-is-often-a-useful-shortcut",
    "title": "Probability",
    "section": "The likelihood of something NOT happening is often a useful shortcut",
    "text": "The likelihood of something NOT happening is often a useful shortcut\nAs shown above, calculating of the probabilities can be done in more and less reliable and elegant ways. Generally speaking, an elegant calculation is less likely to result in error. One common example of how to be more elegant is how we address the chance of avoiding an undesirable outcome, such as a false-positive. If we ran three tests on random data, and accepted an \\(\\alpha\\) threshold of .05 (i.e.Â a 5% chance that we would incorrectly identify an effect), then there are (at least) 2 ways you could calculate the likelihood of at least 1 false positive. The slower way:\n\\[\nfp_1 + fp_2 + fp_3  + fp_{1,2} + fp_{1,3} + fp_{2,3} + fp_{1,2,3}\n\\]\n\n\\(fp_1\\) refers to a false positive only for test 1\n\\(fp_2\\) refers to a false positive only for test 2\n\\(fp_3\\) refers to a false positive only for test 3\n\\(fp_{1,2}\\) refers to a false positive only for tests 1 and 2\n\\(fp_{1,3}\\) refers to a false positive only for tests 1 and 3\n\\(fp_{2,3}\\) refers to a false positive only for tests 2 and 3\n\\(fp_{1,2,3}\\) refers to a false positive for all three tests\n\n\nat_least_1_fp = \n.05 * .95 * .95 + # false positive only for test 1\n.95 * .05 * .95 + # false positive only for test 2\n.95 * .95 * .05 + # false positive only for test 3\n.05 * .05 * .95 + # false positive only for test 1 and 2\n.05 * .95 * .05 + # false positive only for test 1 and 3\n.95 * .05 * .05 + # false positive only for test 2 and 3\n.05 * .05 * .05   # false positive only for test all tests\nat_least_1_fp\n\n[1] 0.142625\n\n\nTo check this is correct, when we add the above number to the likelihood of 0 false-positives we should get 1:\n\n.95 * .95 * .95 + # zero false positives\n  at_least_1_fp\n\n[1] 1\n\n\nBut, we could simply subtract the likelihood of zero false positives from 1:\n\n1 - (.95 * .95 * .95)\n\n[1] 0.142625\n\n\nand get the same result.\n\n\n\n\n\n\nInconsistent p-value thresholds\n\n\n\nNote that itâ€™s easier to do simple calculations like above if the p-value thresholds are consistent (e.g.Â .05 in this case), whereas inconsistent p-value thresholds between outcomes can lead to false assumptions about mutual exclusivity as described above.\n\n\n\nQuestion 1\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/basicsQuestions.html",
    "href": "statsBasics/basicsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\nAlpha values areâ€¦\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Scientific notation is used to express very large or very small numbers in a concise format using exponents of 10 (\\(10^n\\)). So, rather than having to show all of the digits for \\(4,600,000,000\\) (4.6 Trillion) it can be expressed in scientific notation: \\(4.6 \\times 10^9\\). Likewise, very small numbers can be expressed using the same format, e.g.Â \\(0.0000005\\) = \\(5.0 \\times 10^{-7}\\).\nNote: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n3.1e3 == 3.1 * 10^3\n\n[1] TRUE\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n2.5e-3 == 2.5 * 10^(-3)\n\n[1] TRUE\n\n\nHowever, if you would like to turn off scientific notation in R you can type:\n\noptions(scipen=999)\n\n\nQuestion 1\nWhich is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/eNumbersQuestions.html",
    "href": "statsBasics/eNumbersQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/probabilityQuestions.html",
    "href": "statsBasics/probabilityQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "templates/ojs.html",
    "href": "templates/ojs.html",
    "title": "ojs",
    "section": "",
    "text": "Note that you wonâ€™t be able to preview figures created using this\n\ndata = FileAttachment(\"palmerpenguins.csv\").csv({ typed: true })\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\nathletes = FileAttachment(\"athletes.csv\").csv({typed: true})\n\n\ndotplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndotplot = Plot.dot(athletes, {x: \"weight\", y: \"height\", stroke: \"sex\"}).plot()\n\n\n//dotplot.legend(\"color\")\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"bill_depth_mm\", stroke: \"species\"}).plot()\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"count\", stroke: \"sex\"}).plot()\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  )\n).plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "templates/tabsets.html",
    "href": "templates/tabsets.html",
    "title": "tabsets template",
    "section": "",
    "text": "You can use Python and R within Quarto files\n\nRPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n\n1 + 2\n\n3"
  },
  {
    "objectID": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "href": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "title": "Multi-collinearity (incomplete)",
    "section": "Measuring multi-collinearity using Variance Inflation Factor (VIF)",
    "text": "Measuring multi-collinearity using Variance Inflation Factor (VIF)\n\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\ngdp_pop_predict_lifeExp <- lm(\n  formula = lifeExp ~ pop + gdpPercap,\n  data = gapminder_2007\n    \n)\n\n\ngdp_pop_predict_lifeExp$coefficients\n\n (Intercept)          pop    gdpPercap \n5.920520e+01 7.000961e-09 6.416085e-04 \n\npred_lm <- lm(lifeExp ~ pop, gapminder_2007)\npred_lm$coefficients[2]\n\n         pop \n3.889069e-09 \n\n1-sqrt(pred_lm$coefficients[2])\n\n      pop \n0.9999376 \n\nvif(gdp_pop_predict_lifeExp)\n\n      pop gdpPercap \n 1.003109  1.003109"
  },
  {
    "objectID": "regressions/simpleRegressions.html",
    "href": "regressions/simpleRegressions.html",
    "title": "Simple regression (incomplete)",
    "section": "",
    "text": "Simple regression, also known as linear regression, builds on correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression canâ€™t tell us exactly what is influencing what (i.e.Â whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\nGradient reflects how steep the line is\nIntercept is the point at which the regression line crosses the y-axis\n\nLetâ€™s use coding magic to find out the intercept and the gradient (AKA slope):\n\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nThe above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more useful terms, for every extra $10,000 dollars per person, the life expectancy goes up by 6 years).\nFor the above equation we will always retrieve values from the graph, except residuals, which is the â€˜errorâ€™ and so a more complete formula for the outcome can be represented by the following formula\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nResidual reflects whatâ€™s left over, and is not represented in the line of best fit formula because you canâ€™t predict whatâ€™s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\nThese residuals can be thought of the error, i.e.Â what the model failed to predict. In more mathematical terms, the model would be:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "regressions/simpleRegressions.html#notes-for-anthony",
    "href": "regressions/simpleRegressions.html#notes-for-anthony",
    "title": "Simple regression (incomplete)",
    "section": "Notes for Anthony",
    "text": "Notes for Anthony\nR2 = SSR/SSTO = 1 - SSE/SSTO\n\nsse  = sum((gapminder_2007$lifeExp - gapminder_2007$fitted)^2)\nssto = sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\nrsqr = 1-(sse/ssto)\n\nIf your model is minimising the error, then what happens if you have 2 predictors:\n\\[\nY = a + b_1X_1 + b_2X_2 + e\n\\]\nSave the residuals after 1 correlation:\n\nres_gdp = gapminder_2007$lifeExp - gapminder_2007$fitted\ncor.test(gapminder_2007$pop, res_gdp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and res_gdp\nt = 1.3842, df = 140, p-value = 0.1685\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.04948127  0.27564447\nsample estimates:\n      cor \n0.1161931"
  },
  {
    "objectID": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "href": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "title": "Simple regression (incomplete)",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\nIn correlations we discussed how the strength of association is the proportion of variance of y explained by x. For simple regression, this is also the case:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply the above formula to see what R is for \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap))\n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) \n  )\n\n[1] 0.6786624\n\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)\n\n[1] 0.6786624\n\n# r^2\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)^2 \n\n[1] 0.4605827\n\n\nWe can confirm that squaring R gives r^2\n\nsummary(life_expectancy_model)\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.828  -6.316   1.922   6.898  13.128 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(>|t|)    \n(Intercept) 59.56565008  1.01040864   58.95 <0.0000000000000002 ***\ngdpPercap    0.00063713  0.00005827   10.93 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.899 on 140 degrees of freedom\nMultiple R-squared:  0.4606,    Adjusted R-squared:  0.4567 \nF-statistic: 119.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nNow lets see if the same logic works when there are 2 or more predictors that interact to predict each other:\n\\[\nr = \\frac{var_{xyz}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})(z_i-\\bar{z})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2 * \\sum(z_i-\\bar{z})^2}}\n\\]\n\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap)) *\n  (gapminder_2007$pop-mean(gapminder_2007$pop))  \n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n    sum((gapminder_2007$pop - mean(gapminder_2007$pop))^2) \n  )\n\n[1] -0.007330224\n\n\n\nsummary(lm(lifeExp ~  gdpPercap + pop, data = gapminder_2007))\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap + pop, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                   Estimate      Std. Error t value            Pr(>|t|)    \n(Intercept) 59.205198140717  1.040398672164  56.906 <0.0000000000000002 ***\ngdpPercap    0.000641608517  0.000058176209  11.029 <0.0000000000000002 ***\npop          0.000000007001  0.000000005068   1.381               0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.87 on 139 degrees of freedom\nMultiple R-squared:  0.4679,    Adjusted R-squared:  0.4602 \nF-statistic: 61.11 on 2 and 139 DF,  p-value: < 0.00000000000000022\n\n\n\nWhy use regression?\nRegression builds on correlation by providing a more detailed view of your data and with this provides an equation that can be used for any future predicting and optimizing of your data.\n\n\n[1] 4\n\n\nThe differences between regression and correlation"
  },
  {
    "objectID": "excelIntro/averageIfs.html",
    "href": "excelIntro/averageIfs.html",
    "title": "Averageifs",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the averageifs tab at the bottom of excel.\nSometimes it will be helpful to average only specific values in a column. One way to do this is using an â€œaverageifsâ€ formula.\nLet us imagine we want separate average response times for male and female participants. We can use a formula that lets us use one column to identify which rows we want to average in another column. So if we want to average the response time for females, weâ€™re only interested in the rows with female data:\n\nSo hereâ€™s how an â€œaverageifsâ€ formula could look:\n\nSo letâ€™s break this down. This averageifs formula has three inputs:\n\nThe cells that will be averaged - B2:B11\nA set of cells which will be used to determine which rows are selected: C2:C11\nA value that is compared with the previous set of cells to determine which rows are selected: â€œFemaleâ€\n\nIn practice, the above formula does this:\n\nSo you now have a formula for calculating the mean scores based on one criterion, in this case gender. Letâ€™s consolidate this by you answering the following questions\n\nviewof question_1_response = Inputs.number([0,500], {label: \"Mean female ms\", step:1});\ncorrect_female_ms = 413;\n\nquestion_1_result = { \n  if(question_1_response == correct_female_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ \n\nviewof question_2_response = Inputs.number([0,500], {label: \"Mean male ms\", step:1});\ncorrect_male_ms = 408;\n\nquestion_2_result = { \n  if(question_2_response == correct_male_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ \nNow that you have calculated the average score based on one criterion, we can calculate it based on two criterion! Itâ€™s like the original formula, but we add another range of cells, and another value to compare the cells to.\nSo let us imagine we want the average response time for females with a mobile phone? We would use our original formula (see above), but before closing it we would add the cells referring to whether a mobile phone is present (B2:B11), and then compare these cells to the word â€œyesâ€. Letâ€™s do that:\n\nJust to really consolidate what is going on, like before, we focussed only on female participants:\n\nBut are now also focusing on rows in which a mobile phone is present (as indicated by â€œyesâ€)\n\nSo now you should be able to calculate the response times for all four groups of participants:\n\nviewof question_3_response = Inputs.number([0,700], {label: \"Mean female using phones\", step:1});\ncorrect_female_yes_ms = 385;\n\nquestion_3_result = { \n  if(question_3_response == correct_female_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ \n\nviewof question_4_response = Inputs.number([0,700], {label: \"Mean female NOT using phones\", step:1});\ncorrect_female_no_ms = 457;\n\nquestion_4_result = { \n  if(question_4_response == correct_female_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ \n\nviewof question_5_response = Inputs.number([0,700], {label: \"Mean male using phones\", step:1});\ncorrect_male_yes_ms = 612;\n\nquestion_5_result = { \n  if(question_5_response == correct_male_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ \n\nviewof question_6_response = Inputs.number([0,700], {label: \"Mean male NOT using phones\", step:1});\ncorrect_male_no_ms = 103;\n\nquestion_6_result = { \n  if(question_6_response == correct_male_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦"
  },
  {
    "objectID": "excelIntro/formulas.html",
    "href": "excelIntro/formulas.html",
    "title": "Formulas",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here.\nBy using formulas we can automate a lot of processing in excel.\nAll you need to do to make a cell a formula, is to put an â€œ=â€ sign at the start of it.\nLetâ€™s start by going to the worksheet Formulas (you might already be there, or need to scroll left in the tabs at the bottom of the page):\n\nYouâ€™ll see that thereâ€™s a hypothetical situation in which someone has done two runs, and wants to calculate the total distance theyâ€™ve run. We can use the following formula to add the values in each of the cells together:\n\nYou can see that you can just add cell B2 to cell B3 to get the total added together.\n\nYour turn\nDo this yourself, and complete the following:\nThe total distance run wasâ€¦\n\nviewof question_0_response = Inputs.number([0,100], {label: \"\", step:.01});\ncorrect_q0 = 33.37;\n\nquestion_0_result = { \n  if(question_0_response == correct_q0){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow letâ€™s use this logic to work out some other values on that sheet:\nThe total distance run by James wasâ€¦\n\nviewof question_1_response = Inputs.number([0,100], {label: \"\", step:.0005});\ncorrect_q1 = 58.3535;\n\nquestion_1_result = { \n  if(question_1_response == correct_q1){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElaine ranâ€¦\n\nviewof question_2_response = Inputs.number([0,100], {label: \"\", step:.333});\ncorrect_q2 = 16.333;\n\nquestion_2_result = { \n  if(question_2_response == correct_q2){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe total distance run by Dianna wasâ€¦\n\nviewof question_3_response = Inputs.number([0,100], {label: \"\", step:.002});\ncorrect_q3 = 32.184;\n\nquestion_3_result = { \n  if(question_3_response == correct_q3){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeel free to play around with these formulas a bit more. A really good way to learn excel is to play around with what youâ€™ve learned to build up confidence with it."
  },
  {
    "objectID": "excelIntro/anchoring.html",
    "href": "excelIntro/anchoring.html",
    "title": "Anchoring",
    "section": "",
    "text": "This section will keep using the averageifs tab in excel. There are a couple of tricks thatâ€™ll allow you to copy and paste the formulas in a way that saves you having to write them out again with minor tinkering. The first is to make your formula as dynamic as possible. This means to refer to other cells where possible to make your formula do more for you.\nThis might be still be a bit abstract, so letâ€™s do something concrete. Youâ€™ll see that we structured our output into a table:\n\nThis formula doesnâ€™t yet make use of the information in the table. Hereâ€™s an example of one way that it could:\n\nSee how weâ€™re referring to the cell with the word â€œFemaleâ€ in, rather than having to write â€œFemaleâ€. In the Final assignment this sort of trick will be very helpful. Letâ€™s do the same to look at whether thereâ€™s a mobile phone present:\n\nNow weâ€™re nearly ready to start copying and pasting this cell into the other 3 cells. However, if we did that now, the cells would only be partially correctly aligned, as the relative locations would work a little, but not entirely. Letâ€™s see that in practice. First, letâ€™s copy the cell one up from itâ€™s original location:\n\nSee that whilst it is good that one of the cells moved up to refer to â€œmaleâ€ rather than â€œfemaleâ€, another of the cells moved to refer to â€œmobile phone presentâ€ instead of â€œyesâ€, which is what we would have wanted.\nSimilarly, if we just copy the original cell one to the right instead, you might already guess whatâ€™s going to go wrong:\n\nWhilst it was good that one referred to cell moved to the right to refer to the â€œnoâ€ value, we didnâ€™t want the other cell to move to the right to refer to â€œ384.5503â€, we wanted it to stay on â€œfemaleâ€. You might also notice that all the columns we are referring to have moved over also, and are also misaligned!\nSo to prevent cells moving in ways you do not want them to, you anchor them. You anchor them by putting a dollar sign ($) before the row and/or column you want to stop changing. So letâ€™s fix the original cell to be appropriately anchored:\n\nIn particular, notice the new dollar signs ($) next to their respective rows and columns\n\nThis will require practice to become confident with, but it will be an extremely useful excel skill."
  },
  {
    "objectID": "excelIntro/sum.html",
    "href": "excelIntro/sum.html",
    "title": "Sum and SumIf",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the sum tab at the bottom of excel:\n\nIn this example, we would like to calculate the total response times, known as sum. To start with, letâ€™s calculate the SUM for all participants in this spreadsheet:\n\nSo to calculate the SUM (i.e.Â total of the selected cells added together), you write a formula as follows:\n=sum(START:END)\nNow because everything is lined up to have all the mobile users first, and then the non-mobile users, so you can just use the sum function to select the rows where it is â€œyesâ€ for mobile phone users. Itâ€™s not always practical to align your data column by column, so sumifs is a helpful function. Hereâ€™s a formula you could run to calculate the sum for females:\n\nThe excel formulaâ€™s structure:\n=sumifs(cells_you_want_to_sum, selection_crition_cells_1, selction_criterion_1)\nYou can have as many criterion as you like. Letâ€™s look at females with mobile phones:\n\nYou may have noticed 2 things:\n\nYou just add an extra criteria column (in this case column D) and a new criteria to compare it to (â€œyesâ€) to allow you to have an extra criteria to select your rows on\nThis example compares column C to I7 rather than the word â€œFemalesâ€. Referring to a specific cell can help make your formulas more efficient, as you donâ€™t have to repeatedly type the same word again and again. In fact, arguably it would be better to have anchored I$7 so that you could copy the formula down."
  },
  {
    "objectID": "excelIntro/if.html",
    "href": "excelIntro/if.html",
    "title": "If function",
    "section": "",
    "text": "If formulas are very helpful for tidying data. Let us imagine in our experiment we want to remove all participants who were too slow, and we assume they just werenâ€™t paying attention. Letâ€™s remove all participants who took longer than 800ms to respond. We can do this participant by participant using â€œifâ€ formulas.\nIn English, what weâ€™re going to do is write a formula which looks at the participantâ€™s response time, and if it is too slow, write â€œTOO SLOWâ€. If itâ€™s not too slow, weâ€™ll write down the respective response time. Letâ€™s do this for one row:\n\nLetâ€™s relate this to the structure of an If formula generally. An â€œifâ€ formulaâ€™s structure goes like so:\nâ€œ=if(logical test,result if true, result if false)â€\nThe logical test in our case was whether the cell B2 was greater than (>) 800: Â Â Â B2 > 800\nThe result we wanted if B2 was greater than 800 was the text â€œTOO SLOWâ€: Â Â Â Â Â  â€œTOO SLOWâ€\nThe result we wanted if B2 was not greater than 800 was the value in B2: Â Â Â Â Â Â Â B2\nSo as a next step, make this formula for each row and then complete the following statement:\nThere were _____ participants who took more than 800ms, making them too slow.Â \n\nviewof if_q1_response = Inputs.number({label: ''});\ncorrect_if_q1 = '3';\nif_q1_result = {\n  if(if_q1_response == correct_if_q1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow letâ€™s do another formula to remove participants who were too fast. We donâ€™t think a participant can react quicker than 100ms in reality, so in the Too Fast column, write a formula for each row that will write â€œTOO FASTâ€ if the response time is less than 100ms, or write the response time if it is not â€œTOO FASTâ€.\nAfter writing these formulas, complete the following statement:\nThere were _____ participants who took less than 100ms, making them too fast.\n\nviewof if_q2_response = Inputs.number({label: ''});\ncorrect_if_q2 = '2';\nif_q2_result = {\n  if(if_q2_response == correct_if_q2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "excelIntro/count.html",
    "href": "excelIntro/count.html",
    "title": "Count and countifs function",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. You can find the relevant tab below:\n\nThis function allows you to count the number of cells in a selection.\nLike all formulas, it begins with an â€œ=â€ sign.\nLetâ€™s use this to count the number of participants you have (even though in this case we already know it is 10). The general formula is:\n=count(START:END)\nSo to calculate the number of participants, we would write:\n\nAn important thing to know about count formulas is that they only count cells with numbers in them. So the following would get zero:\n\nIf you want to count the number of phone and non-phone users, you can use countifs. This allows you to count how many occurrences there are of a value you are looking for. You can do this for just one column looking for just one value, or multiple columns looking for multiple values.\nThe general formula if you are just looking for one value in one column:\n=countifs(column_1,value_1)\nIf you wanted to know the number of phone users, you could type:\n=countifs(D2:D11,\"yes\")\nHowever, as there is nothing else in column D, it would be more elegant to refer to the whole column instead:\n=countifs(D:D,\"yes\")\nNow if we wanted to calculate how many females had a phone, you could use add a second column and compare a second value to it:\n=countifs(D:D,\"yes\",C:C,\"Female\")\n\nYour turn\nUse countif formulas to calculate the following. Your forumas should get you the same numbers as listed below:\n\n\n\nSex\nPhones\nNo Phones\nTotal\n\n\n\n\nFemales\n3\n2\n5\n\n\nMales\n3\n2\n5"
  },
  {
    "objectID": "jast_setup.html",
    "href": "jast_setup.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "distributions/normal_questions.html",
    "href": "distributions/normal_questions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamieâ€™s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  },
  {
    "objectID": "distributions/transformingQuestions.html",
    "href": "distributions/transformingQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/skewnessQuestions.html",
    "href": "distributions/skewnessQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is"
  },
  {
    "objectID": "distributions/skewness.html",
    "href": "distributions/skewness.html",
    "title": "Skewness (R,Python)",
    "section": "",
    "text": "Parametric analyses are based on the assumption that the data you are analysing is normally distributed (see below):\nIf your data fits a normal distribution, then you can draw conclusions based on certain facts about this distribution, e.g.Â the fact that 97.7% of your population should have a score that is more negative than +2 standard deviations above the mean (because Z-scores represent standard deviations from the mean). As a result, if your data is skewed:\nWhite represents the median in the figure above. As you can see from the above skewed distribution, the median is below the mean, consistent with the data being skewed. Importantly, the assumptions that we can make about what proportion of the population are 1 standard deviation above and below the mean are no longer valid, as more than half the population are below the mean in this case. This would suggest that non-parametric analyses could be more appropriate if your data is skewed.\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is.\nThe next section is a breakdown of the formula for those interested in it (but this is not crucial)."
  },
  {
    "objectID": "distributions/skewness.html#consolidation-questions",
    "href": "distributions/skewness.html#consolidation-questions",
    "title": "Skewness (R,Python)",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "Kurtosis refers to how influenced a distribution is by its tails.\n\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\nIs Platykurtic vs.Â leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "distributions/binomial.html",
    "href": "distributions/binomial.html",
    "title": "Binomial Distribution (R) - incomplete",
    "section": "",
    "text": "Imagine that you want to predict the likelihood of flipping a coin on heads or tails a certain number of times in a row. The outcome of each coin flip is binary, i.e.Â there is only 1 possible outcome out of 2 options. If this isnâ€™t a biased coin, we can calculate some basic expectations about what will happen after 2 flips of the coin:\n\nEach flip of the coin has a 50% or 0.5 chance of landing on Heads.\nIf we wanted to calculate the likelihood of 2 flips of heads, thereâ€™s a 0.5 chance we get the first heads flip, and then another 0.5 chance weâ€™ll get the second head flip. To summarise this, we can multiply both 0.5 chances together to get .025.\nTo summarise the likelihood of all combinations we could make the following table:\n\n\n\n\n\n\n\n\n\nFirst flip and likelihood\nSecond flip and likelihood\nOverall likelihood\n\n\nHeads (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nHeads (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\n\nLikelihood of one of the above happening\n.25 + .25 + .25 + .25 = 1\n\n\n\nOne way to visualise this is as follows"
  },
  {
    "objectID": "distributions/binomial.html#what-if-the-odds-of-each-option-are-not-equal-to-each-other-e.g.-you-have-a-biased-coin",
    "href": "distributions/binomial.html#what-if-the-odds-of-each-option-are-not-equal-to-each-other-e.g.-you-have-a-biased-coin",
    "title": "Binomial Distribution (R) - incomplete",
    "section": "What if the odds of each option are not equal to each other (e.g.Â you have a biased coin)",
    "text": "What if the odds of each option are not equal to each other (e.g.Â you have a biased coin)\nLetâ€™s now imagine that we have a coin that will flip heads 75% of the time, and tails 25% of the time. Itâ€™s a bit more complicated to draw a distribution that captures this as the likelihood of either side is no longer equal.\n\nbiased_coin_flips <- data.frame(\n  first  = c(\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"heads\",\"tails\",\"tails\",\"tails\",\"tails\"),\n  second = c(\"heads\",\"heads\",\"heads\",\"tails\",\"heads\",\"heads\",\"heads\",\"tails\",\"heads\",\"heads\",\"heads\",\"tails\",\"heads\",\"heads\",\"heads\",\"tails\")\n)\n\nbiased_coin_flips$heads = apply(biased_coin_flips, 1, function(x) length(which(x==\"heads\")))\n\nknitr::kable(biased_coin_flips)\n\n\n\n\nfirst\nsecond\nheads\n\n\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\ntails\n1\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\ntails\n1\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\nheads\n2\n\n\nheads\ntails\n1\n\n\ntails\nheads\n1\n\n\ntails\nheads\n1\n\n\ntails\nheads\n1\n\n\ntails\ntails\n0\n\n\n\n\nggplot(biased_coin_flips, aes(heads)) + geom_histogram(bins = 3)\n\n\n\n\nHow likely am I to get at least 1 head flip with this biased coin?\n\nsum(biased_coin_flips$heads >= 1) / length(biased_coin_flips$heads)\n\n[1] 0.9375\n\n\nWhich can be visualised as:\n\nggplot(biased_coin_flips, aes(heads, fill = heads >= 1)) + geom_histogram(bins = 3)\n\n\n\n\nWith binomial distributions you can calculate the likelihood of each event happening with multiplications\n\nhead.head <- .75 * .75\nhead.tail <- .75 * .25\ntail.head <- .25 * .75\ntail.tail <- .25 * .25\n\nhead.head + head.tail + tail.head + tail.tail\n\n[1] 1\n\n\nDoes the above calculations confirm a .9375 chance of a flipping at least one heads\n\nhead.head + head.tail + tail.head\n\n[1] 0.9375\n\n\nGreat!"
  },
  {
    "objectID": "distributions/transforming.html",
    "href": "distributions/transforming.html",
    "title": "Transforming Data (R)",
    "section": "",
    "text": "A lot of analyses is dependent on data being normally distributed. One problem with your data might be that it is skewed. Lets focus on the gapminder data from 2007 to see if the gdp and life expectancy data is skewed, and how this could be addressed.\nSo it looks like both the gdp and life expectancy are skewed (as their z-scores are greater than 1.96). Lets double check with a quick plot:\nItâ€™s relatively easy to see the skewness of gdp, but life expectancy is a bit more subtle. As the data is skewed, we may want to transform it to make it less skewed.\nWe can complete a logarithmic transformation to reduce the skewness, so lets do that to both variables and then replot the data:\nLets check if the skewness has changed for the gdp:\nSo, transforming the gdp did reduce skewness but increased kurtsosis, so beware that applying a transformation may cause other problems! Lets check whether the log transformation reduced skewness for life expectancy:\nSeems like the answer is no.\nAn important question is whether the associations between your variables change after transformation, so letâ€™s check that next:\nThe log transformed data is more strongly associated with each other than the original data. However, not all transformations will change associations between variables."
  },
  {
    "objectID": "distributions/transforming.html#linear-vs.-non-linear-transformations",
    "href": "distributions/transforming.html#linear-vs.-non-linear-transformations",
    "title": "Transforming Data (R)",
    "section": "Linear vs.Â non-linear transformations",
    "text": "Linear vs.Â non-linear transformations\nLinear transformation includes adding, subtracting from, multiplying or dividing variables. These transformations change the absolute value, but not pattern of the distribution of the variable. Letâ€™s use life expectancy to illustrate how linear transformations change the absolute values without changing the distribution.\n\nAdditive transformations\nIf you added 100 to the life expectancy for all countries, you would change the absolute value:\n\n# before transformation\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n# after transformation\nmean(gapminder_2007$lifeExp + 100)\n\n[1] 167.0074\n\n\nThereâ€™s a big difference between the means, but all weâ€™ve done is shift the distribution up 100, we havenâ€™t made it wider or thinner:\n\n# before transformation\nsd(gapminder_2007$lifeExp)\n\n[1] 12.07302\n\n# after transformation\nsd(gapminder_2007$lifeExp + 100)\n\n[1] 12.07302\n\n\nIf we were to visualise this transformation\n\nlife_exp_before_after <- data.frame(\n  life_exp = c(gapminder_2007$lifeExp, gapminder_2007$lifeExp + 100),\n  tranformed = c(rep(\"before\", each = 142), rep(\"after\", each =142))\n)\n\nggplot(life_exp_before_after, aes(x=life_exp, fill=tranformed)) +\n  geom_histogram(binwidth = 2, alpha=.5, position = \"identity\") +\n  ggtitle(\"Before vs. after additive transformation\") \n\n\n\n\nWe can see above that there is no difference in the shape of the distributions, but a shift. You would get the same pattern shifted also if you had subtracted from the original data. As a result, any association between the transformed variable and another variable will be the same as it was before the transformation as the shapes of the distributions are still the same.\n\n\nMultiplicative transformations\nIf you multiplied the life expectancy by 1.5 then you would change both the mean\n\n# before transformation\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n# after transformation\nmean(gapminder_2007$lifeExp * 1.5)\n\n[1] 100.5111\n\n\nand SD of life expectancy\n\n# before transformation\nsd(gapminder_2007$lifeExp)\n\n[1] 12.07302\n\n# after transformation\nsd(gapminder_2007$lifeExp * 1.5)\n\n[1] 18.10953\n\n\nWe established above that changing the mean isnâ€™t sufficient to change the shape of a distribution, but would changing the standard deviation change the shape of the distribution. Letâ€™s put two histograms of each distribution side by side to evaluate this:\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$lifeExp, breaks = seq(min(gapminder_2007$lifeExp), max(gapminder_2007$lifeExp), length.out = 11), main = \"Original\")\nhist(gapminder_2007$lifeExp*1.5, breaks = seq(min(gapminder_2007$lifeExp*1.5), max(gapminder_2007$lifeExp*1.5), length.out = 11), main = \"Original * 1.5\")\n\n\n\n\nWe can see that the shape/pattern of the distribution is the same, and so the association between the transformed variable and other variables will stay the same after transformation. This is because associations between variables ignore the scale of either variable.\n\n\nNon-linear transformations\nUnlike linear transformations, non-linear transformations change the shape of distributions. There are a wide variety of non-linear transformations you could apply to a variable, such asâ€¦\n\nSquare (\\(^2\\))\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(gapminder_2007$gdpPercap^2, main = \"Squared\")\n\n\n\n\nSquaring data is likely to make the distributions more extreme, and so isnâ€™t often a pragmatic solution to try to make your data less skewed.\n\n\nSquare root (\\(\\sqrt{}\\))\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(sqrt(gapminder_2007$gdpPercap), main = \"Square root\")\n\n\n\n\nThis transformation appears to have reduced the skewness of the distribution. Calculating the square root of a variable will disproportionately reduce extreme values compared to less extreme values. This might be more clearly shown by looking at the change in the individual data points:\n\n# focusing on 5 countries to make it visually easier\ngapminder_sqrt <- data.frame(\n  country = gapminder_2007$country[1:5],\n  transformed = c(\n    rep(\"Original\",5),\n    rep(\"Square Root\",5)\n  ),\n  # gdp has been divided by 500 to make the comparisons more visible\n  gdp  = c(gapminder_2007$gdpPercap[1:5]/500, sqrt(gapminder_2007$gdpPercap[1:5]/500))\n)\nggplot(gapminder_sqrt, aes(x=country, y = gdp, color = transformed)) +\n  geom_point(size=5) +\n  xlab(\"Country index\") +\n  ylab(\"GDP (before and after transformation)\") \n\n\n\n\nAs you can see above, the original values (pink) that are higher are much more heavily reduced by square root transforming them than lower original values.\n\n\nLogarithmic (\\(\\log\\))\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(log(gapminder_2007$gdpPercap), main = \"Logarithmic\")\n\n\n\n\nThis transformation seems very successful in changing the distribution shape to be less skewed. Letâ€™s see if the log transformation follows a similar pattern as the sqrt in disproportionately impacting larger values than smaller values.\n\n# focusing on 5 countries to make it visually easier\ngapminder_log <- data.frame(\n  country = gapminder_2007$country[1:5],\n  transformed = c(\n    rep(\"Original\",5),\n    rep(\"Square Root\",5)\n  ),\n  # gdp has been divided by 500 to make the comparisons more visible\n  gdp  = c(gapminder_2007$gdpPercap[1:5]/500, log(gapminder_2007$gdpPercap[1:5]/500))\n)\nggplot(gapminder_log, aes(x=country, y = gdp, color = transformed)) +\n  geom_point(size=5) +\n  xlab(\"Country index\") +\n  ylab(\"GDP (before and after transformation)\") \n\n\n\n\nYep, log also reduces skewness by disproportionately reducing higher values."
  },
  {
    "objectID": "distributions/transforming.html#linear-transformations-will-not-change-the-association-between-variables",
    "href": "distributions/transforming.html#linear-transformations-will-not-change-the-association-between-variables",
    "title": "Transforming Data (R)",
    "section": "Linear transformations will not change the association between variables",
    "text": "Linear transformations will not change the association between variables\nYou can transform a single variable by adding and multiplying it, but as these are linear transformations they do not change the shape of the distributions of the original variables, and thus do not change the association between variables. For example:\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data + 5 to one variable (an additive change)\ncor.test(\n  gapminder_2007$gdpPercap + 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap + 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data - 10 to one variable (an additive change)\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp - 10\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp - 10\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with multiplication of 5 to one variable (multiplicative)\ncor.test(\n  gapminder_2007$gdpPercap * 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap * 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# grid comparing the 4 transformations\npar(mfrow = c(2,2), \n    mar = c(2,2,2,1))\nplot(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, main=\"original correlation\")\nplot(gapminder_2007$gdpPercap + 5, gapminder_2007$lifeExp, main=\"added 5 to gdp\")\nplot(gapminder_2007$gdpPercap - 10, gapminder_2007$lifeExp, main = \"took 10 away from gdp\")\nplot(gapminder_2007$gdpPercap *5, gapminder_2007$lifeExp, main = \"multiplied gdp by 5\")\n\n\n\n\nYou can see that the transformations being linear havenâ€™t changed the nature of the associations."
  },
  {
    "objectID": "distributions/transforming.html#non-linear-transformations-do-change-associations",
    "href": "distributions/transforming.html#non-linear-transformations-do-change-associations",
    "title": "Transforming Data (R)",
    "section": "Non-linear transformations do change associations",
    "text": "Non-linear transformations do change associations\nIf you apply non-linear transformations to one or both variables this will change the direction and strength of the associations. Below are some examples when you transform both variables:\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with log of variables squared\ncor.test(\n  log(gapminder_2007$gdpPercap),\n  log(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  log(gapminder_2007$gdpPercap) and log(gapminder_2007$lifeExp)\nt = 14.752, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7060729 0.8372165\nsample estimates:\n      cor \n0.7800706 \n\n# correlation with both variables squared\ncor.test(\n  gapminder_2007$gdpPercap ^ 2,\n  gapminder_2007$lifeExp ^ 2\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap^2 and gapminder_2007$lifeExp^2\nt = 8.6437, df = 140, p-value = 1.123e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4709220 0.6877841\nsample estimates:\n      cor \n0.5898894 \n\ncor.test(\n  sqrt(gapminder_2007$gdpPercap),\n  sqrt(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(gapminder_2007$gdpPercap) and sqrt(gapminder_2007$lifeExp)\nt = 12.981, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6539524 0.8057025\nsample estimates:\n      cor \n0.7390648 \n\n# grid comparing the 4 transformations\npar(mfrow = c(2,2), \n    mar = c(2,2,2,1))\nplot(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, main = \"original correlation\")\nplot(log(gapminder_2007$gdpPercap),log(gapminder_2007$lifeExp), main = \"log applied\")\nplot(gapminder_2007$gdpPercap ^ 2,gapminder_2007$lifeExp ^ 2, main = \"data squared\")\nplot(sqrt(gapminder_2007$gdpPercap),sqrt(gapminder_2007$lifeExp), main = \"square root of data\")\n\n\n\n\n\n\n\n\n\n\nTransforming is not universally accepted\n\n\n\nWhilst this page describes ways to transform the data and how they impact the distributions, this isnâ€™t universally accepted practice as it is changing your data (perhaps similar to arguments that you shouldnâ€™t remove outliers if they reflect real data points). However, transformation does not necessarily bias your data towards or against hypotheses if done appropriately, and is in fact used in mainstream analyses such Spearmanâ€™s Rank correlations. There are at least two major possible problems from inappropriately transforming your data:\n\nIf it becomes a form of fishing for data through multiple comparisons.\nTreating your data unevenly to create meaningless differences between conditions. For example, if you ran a t-test between 2 conditions, but one of them wasnâ€™t normally distributed, then transforming only one of the conditions could make your comparison less meaningful. Letâ€™s use the gapminder data to illustrate this by comparing gdp per capita between Europe and Africa:\n\n\ngapminder_2007_us_europe <- gapminder_2007[\n  gapminder_2007$continent ==  \"Europe\"| \n  gapminder_2007$year == \"Americas\",\n]\n\nspssSkewKurtosis(gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Europe\"])\n\n           estimate        se     zScore\nskew     -0.1028377 0.4268924 -0.2408985\nkurtosis -1.0441533 0.8327456 -1.2538683\n\nspssSkewKurtosis((gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Africa\"]))\n\n         estimate        se   zScore\nskew     1.707376 0.3304137 5.167389\nkurtosis 1.793325 0.6500932 2.758566\n\n\nSo We can see that thereâ€™s a significant problem with skewness for countries from Africa (zScore > 1.96) but not from Europe. The correct thing to do is to apply any transformation (to address skewness) to Africa to Europe also to avoid differences between the groups reflecting bias from the distortions. Logarithmic transformations can help reduce skewness, so letâ€™s see how the means compare after applying the log transformation to both groups:\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Europe\"])\n\n[1] 9.985978\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n[1] 7.486539\n\n\nThereâ€™s a difference, in which Europe had a higher GDP per capita in 2007. Letâ€™s check if both Europe and Africa are less skewed in their distributions after log-transforming their data:\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Europe\"])\n\n           estimate        se     zScore\nskew     -0.7604293 0.4268924 -1.7813138\nkurtosis -0.6776747 0.8327456 -0.8137835\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n           estimate        se    zScore\nskew      0.5431380 0.3304137  1.643812\nkurtosis -0.6719277 0.6500932 -1.033587\n\n\nLooking good. Now, what would have happened if we had only log-transformed Africaâ€™s data as it had the skewed distribution:\n\nmean(gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Europe\"])\n\n[1] 25054.48\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n[1] 7.486539\n\n\nIt now looks like Europeans have 3346.604 times as much GDP per capita as Africa!?\nThe above example hopefully is quite intuitive how problems arise if you apply transformations mindlessly. If you will be comparing differences in magnitudes between conditions, then it is important that transformations are applied equally to avoid bias. If you are correlating between conditions (or conducting a regression), then you do not have the same issue of bias.\n\n\n\nQuestion 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/normal.html",
    "href": "distributions/normal.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "Parametric statistics often compare values to a normal distribution of expected data, based on the estimated mean and SD. Lets start by showing a (made up) normal distribution of heights in centimeters:\nSo lets say the average personâ€™s height is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:\n\nRPythonExcelJASPSPSS\n\n\n\n# Plot a normal distribution of heights\npopulation_heights_x <- seq(\n  120,    # min\n  180,    # max\n  by = 1  \n)\npopulation_heights_y <- dnorm(\n  population_heights_x,\n  mean = 150,\n  sd   = 10\n)\nplot(\n  population_heights_x,\n  population_heights_y,\n  xlab = \"height\",\n  ylab = \"frequency\"\n)\n# Add line to show mean and median\nabline(\n  v=150,  # where the line for the mean will be \n  lwd=5\n)\n\n\n\n\n\n\n\n# Plot a normal distribution of heights\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\npopulation_heights_x = [x for x in range(120, 181, 1)]\n\n# vector for the y-axis\npopulation_heights_y = norm.pdf(population_heights_x, loc=150, scale=10)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n\n# plot\nplt.scatter(population_heights_x, population_heights_y, color='w', edgecolors='black')\n\n# Add line to show mean and median\nplt.axvline(x=150, color='black', ls='-', lw=5)\n\n# add title on the x-axis\nplt.xlabel(\"height\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n# show the plot\nplt.show()\n\n# show plot\nplt.show()\n\n\n\n\nNormal Distribution\n\n\n\n\nDownload and open the normal.xlsx file in this repository to see data being used to create the below figure:\n(note that making a vertical line to reflect the mean didnâ€™t seem as easy in Excel as other languages, so this hasnâ€™t been added)\n\n\n\nNormal Distribution using Excel\n\n\n\n\nDownload and open height.csv in JASP, and then complete the following steps to generate the figure below:\n\n\nClick on the Descriptives panel\nadd height as a variable\nopen the Basic plots interface and then select Distribution plots and Display density\n\n\n\nDownload and open height.sav in SPSS and then complete the following steps to generate the figure below:\n\n\nSelect Analyze â€“> Descriptive Statistics â€“> Exploreâ€¦\n\n\n\nMove the height variable to the dependent list, then select Plotsâ€¦ and choose Histogram as a descriptive figure\n\n\n\n\n\nYou can see that the above fits a bell-curve, and the middle represents both the mean and the median as the data is symmetrical. In reality, almost no data is a perfect bell-curve, but there are ways to test if the data isnâ€™t sufficiently normal to use parametric tests with.\nNext, we will look at how normal distributions allow you to transform your data to z-scores to compare to a z-distribution."
  },
  {
    "objectID": "distributions/normal.html#z-scores-and-the-z-distribution",
    "href": "distributions/normal.html#z-scores-and-the-z-distribution",
    "title": "Normal Distribution",
    "section": "Z-scores and the z-distribution",
    "text": "Z-scores and the z-distribution\nA z-score is a standardised value that captures how many standard deviations above or below the mean an individual value is. Thus, to calculate the z-score\n\\[\nZ = \\frac{individualScore-meanScore}{StandardDeviation}\n\\]\nOr in formal terminology:\n\\[\nZ = \\frac{x-\\bar{x}}{\\sigma}\n\\]\nThe calculated score can then be applied to a z-distribution, which is parametric/normally distributed. Lets have a look at a z-distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# vector for the x-axis\nz_score_x <- seq(\n  -3,    # min\n  3,    # max\n  by = .1  \n)\n\n# vector for the y-axis\nz_score_y <- dnorm(\n  z_score_x,\n  mean = 0,\n  sd   = 1\n)\n\nplot(\n  z_score_x,\n  z_score_y,\n  xlab = \"z-score (SDs from the mean)\",\n  ylab = \"frequency\"\n)\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\nz_score_x = np.arange(-3.0, 3.1, 0.1)\n\n# vector for the y-axis\nz_score_y = norm.pdf(z_score_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\n# plot\nplt.scatter(z_score_x, z_score_y, color='w', edgecolors='black')\n\n# add title on the x-axis\nplt.xlabel(\"z-score (SDs from the mean)\")\n\n# add title on the y-axis\nplt.ylabel(\"frequency\")\n\n# show plot\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the â€œSelect Dataâ€¦â€ option when right clicking on the chart:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\nItâ€™s not clear that generating a z-distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\nItâ€™s not clear that generating a z-distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\n\nIf you compare the height distribution above to the z-score distribution, you should see that they are identically distributed. This is useful, as we know what percentage of a population fits within each standard deviation of a normal distribution:\n\nRPythonExcelJASPSPSS\n\n\n\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_13.6 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n p +\n   \n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6, \n     aes(\n       x, \n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   \n   annotate(\n     \"text\", \n     x=0.5, \n     y=0.01, \n     label= \"34.1%\"\n   ) + \n   annotate(\n     \"text\", \n     x=1.5, \n     y=0.01, \n     label= \"13.6%\"\n   ) + \n   annotate(\n     \"text\", \n     x=2.3, \n     y=0.01, \n     label= \"2.3%\"\n   ) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='red', alpha=.4)\n\nplt.text(2.05, 0.001, '2.3%', fontsize = 10, color='black',weight='bold')\nplt.text(1.1, 0.001, '13.6%', fontsize = 10, color='black',weight='bold')\nplt.text(0.1, 0.001, '34.1%', fontsize = 10, color='black',weight='bold')\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_0_plus.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the â€œSelect Dataâ€¦â€ option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\nItâ€™s not clear that generating a distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\nItâ€™s not clear that generating a distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\n\nThe above visualises how 34.1% of a populationâ€™s scores will be between 0 and 1 standard deviation from the mean, 13.6% of the populationâ€™s scores will be between 1 to 2 standard deviations above the mean, and 2.3% of the population will be more then 2 standard deviations above the mean. Remember that the normal distribution is symmetrical, so we also know that 34.1% of the populationâ€™s score will be between 0 to 1 standard deviations below the mean (or 0 to -1 SDs), 13.6% of the populationâ€™s score will be between -2 to -1 standard deviations from the mean, and 2.3% of the populationâ€™s score will be more negative than -2 standard deviations from the mean. Lets look at this cumulative distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"2.3%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"15.9%\") + \n   annotate(\"text\", x=-0.4, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (norm_x < -2), color='gold', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-2 <= norm_x)&(norm_x <= -1), color='red', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-1 <= norm_x)&(norm_x <= 0), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='cyan', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='purple', alpha=.4)\n\nplt.text(2.05, 0.001, '100%', fontsize = 10,color='black',weight='bold')\nplt.text(1.2, 0.001, '97.7%', fontsize = 10,color='black',weight='bold')\nplt.text(0.2, 0.001, '84.1%', fontsize = 10,color='black',weight='bold')\nplt.text(-0.8, 0.001, '50%', fontsize = 10,color='black',weight='bold')\nplt.text(-1.85, 0.001, '15.9%', fontsize = 10,color='black',weight='bold')\nplt.text(-2.7, 0.001, '2.3%', fontsize = 10,color='black',weight='bold')\n\nplt.yticks(np.arange(0, 0.5, step=0.1))\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_percentages.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the â€œSelect Dataâ€¦â€ option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\nItâ€™s not clear that generating a distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\nItâ€™s not clear that generating a distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\n\nThe above figure visualises how 13.6% of the population have score that is more negative than -2 standard deviations from the mean, 34.1% of the population have a standard deviation that is more negative than -1 standard deviations from the mean (this also include all the people who are more than -2 standard deviations from the mean), etc.\nWe can now use the above information to identify which percentile an individual is within a distribution.\nFor example, letâ€™s imagine that an individual called Jane wants to know what percentile sheâ€™s at with her height. Lets imagine she is 170cm tall, the mean height of people 150cm, and the SD 10cm. That would make her z-score:\n\\[\nZ_{score} = \\frac{170 - 150}{10} = 2\n\\]\nAs we can see from the figure above, that puts her above 97.7% of the population, putting her in the top 2.3%."
  },
  {
    "objectID": "distributions/normal.html#consolidation-questions",
    "href": "distributions/normal.html#consolidation-questions",
    "title": "Normal Distribution",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamieâ€™s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  },
  {
    "objectID": "advancedR/fancyFigures.html",
    "href": "advancedR/fancyFigures.html",
    "title": "Fancy Figures",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "correlations/partialCorrelations.html",
    "href": "correlations/partialCorrelations.html",
    "title": "Partial Correlations",
    "section": "",
    "text": "As you may have heard, correlation does not equal causation. One possible reason for this is that thereâ€™s a third variable that explains an association. Letâ€™s imagine that we are trying to understand whether life expectancy goes up over time and why. First of all, letâ€™s check if lifeExpectancy is going up over time using the gapminder data:\n\nRPythonJulia\n\n\n\nlibrary(gapminder)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâœ” purrr   0.3.5      \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\ngapminder %>% \n  group_by(year) %>% \n  summarise(\n    lifeExp = mean(lifeExp),\n    gdpPercap = mean(gdpPercap)\n  ) -> gapminder_by_year\n\ncor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$lifeExp\nt = 18.808, df = 10, p-value = 3.91e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9498070 0.9962336\nsample estimates:\n     cor \n0.986158 \n\nggplot(data = gapminder_by_year, aes(x = year, y = lifeExp)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# TBC\n\n\n\n\n# TBC\n\n\n\n\nSo now that weâ€™ve confirmed that there is a positive association between the year and life expectancy, the next question is why? What changes from year to year that could explain increased life expectancy? Letâ€™s investigate whether gdp per capita generally goes up each year, and whether itâ€™s associated with life expectancy. If both of these things are true, then perhaps the increase in gdp per year is an explanation of the association between year and life expectancy.\n\nIs year and gdp per capita associated?\n\nR\n\n\n\ncor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$gdpPercap\nt = 17.039, df = 10, p-value = 1.022e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9393538 0.9954265\nsample estimates:\n      cor \n0.9832101 \n\nggplot(data = gapminder_by_year, aes(x = year, y = gdpPercap)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhilst there are some outliers in earlier years, we seem to have found that gdp per capita has gone up.\n\n\n\n\n\nIs gdp per capita and life expectancy associated?\n\nR\n\n\n\ncor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$gdpPercap and gapminder_by_year$lifeExp\nt = 11.137, df = 10, p-value = 5.875e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8663785 0.9895601\nsample estimates:\n      cor \n0.9619721 \n\nggplot(data = gapminder_by_year, aes(x = gdpPercap, y = lifeExp)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nSo the above analysis suggests that all three variables are incredibly related. (you are unlikely to see such strong associations in real psychology experiments). But letâ€™s now check in whether thereâ€™s still an association between the year and life expectancy once you control for GDP per capita. This partial correlation can be visualised as follows:\n\n\n\n\nflowchart LR\n    Year(X:Year) <---> GDP(Z:GDP per Capita)\n    Year <---> Life(Y:Life Expectancy)\n    GDP <---> Life \n  style Year color:white,fill:#159,stroke:#333,stroke-width:4px\n  style Life color:white,fill:#159,stroke:#333,stroke-width:4px\n  style GDP fill:#571,stroke:#fff,stroke-width:3px,color:#fff,stroke-dasharray: 5 5\n\n\n\n\n\n\n\n\nTo calculate the partial correlation r value you control for the association between the two variables\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{originalCorrelation - varianceExplainedByCovariate}{varianceNotExplainedByCovariate}\n\\]\n\n\\(x\\) = The Year\n\\(y\\) = Life expectancy\n\\(z\\) = GDP per capita\n\\(\\sqrt{1- r^2_{xz}}\\) = variance not explained by correlation between Year(\\(x\\)) and GDP (\\(z\\))\n\\(\\sqrt{1- r^2_{yz}}\\) = variance not explained by correlation between Life Expectancy (\\(y\\)) and GDP (\\(z\\))\n\nOne way to think of the formula above is that: - the top-half represents how much variance is explained by overlap between the two main variables, subtracted by the variance of each variable with the confound - the bottom-half represents how much variance there is left to explain once youâ€™ve removed associations between each main variable and the covariate.\nLetâ€™s see what the r value is after this partial correlation:\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{.986158 - .9832101 * .9619721 }{\\sqrt{(1-.9832101^2)(1-.9619721^2)}} = \\frac{.040354}{.04984321} = .8092841\n\\]\nLetâ€™s check if the manually calculated partial r-value is the same as what R gives us:\n\nlibrary(ppcor)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor(gapminder_by_year)\n\n$estimate\n               year    lifeExp  gdpPercap\nyear      1.0000000  0.8092844  0.7629391\nlifeExp   0.8092844  1.0000000 -0.2521280\ngdpPercap 0.7629391 -0.2521280  1.0000000\n\n$p.value\n                 year    lifeExp   gdpPercap\nyear      0.000000000 0.00254769 0.006309324\nlifeExp   0.002547690 0.00000000 0.454498736\ngdpPercap 0.006309324 0.45449874 0.000000000\n\n$statistic\n              year    lifeExp  gdpPercap\nyear      0.000000  4.1331001  3.5404830\nlifeExp   4.133100  0.0000000 -0.7816358\ngdpPercap 3.540483 -0.7816358  0.0000000\n\n$n\n[1] 12\n\n$gp\n[1] 1\n\n$method\n[1] \"pearson\"\n\n\nYes, the pearson correlation between year and life expectancy is .8092844, so the difference of .0000003 is a rounding error from the manual calculations above. Just to confirm that this is a rounding error, hereâ€™s what you get if you complete the same steps but with the estimate part of the correlation objects instead:\n\nx.y.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\nx.z.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\ny.z.cor <- cor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n(x.y.cor$estimate - x.z.cor$estimate * y.z.cor$estimate)/sqrt((1-x.z.cor$estimate^2) * (1 - y.z.cor$estimate^2))\n\n      cor \n0.8092844 \n\n\nConfirming that the difference in the manual calculation is a rounding error. Note that the numbers you get from R may be rounded numbers, and so your calculations may reflect the rounding.\nTo conclude, as there is still an association between Year and Life Expectancy once controlling for GDP, this partial correlation is consistent with GDP not being the only explanation for why Life Expectancy goes up each year."
  },
  {
    "objectID": "correlations/correlationsQuestions.html",
    "href": "correlations/correlationsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations (R,Python)",
    "section": "",
    "text": "Please make sure youâ€™ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. Weâ€™ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data. Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations (R,Python)",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\n\\(\\sum\\) is saying to add together everything (i.e.Â the sum of everything within the brackets for this formula)\n\\(x_i\\) refers to each individualâ€™s x-score\n\\(y_i\\) refers to each individualâ€™s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\n\nThe variance for life expectancy can be visualised as the sum of the square of the following:\n\nRPython\n\n\n\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    linewidth  = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\"\n  )\n\n# ggsave(\"life_exp_resid.png\", life_exp_resid)\nlife_exp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add horizontal line for the mean of 'lifeExp'\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\nScatterplot with residuals of â€˜Life expectancyâ€™\n\n\n\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\nRPython\n\n\n\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\nggsave(\"gdp_resid.png\", gdp_resid)\n\nSaving 7 x 5 in image\n\ngdp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\nScatterplot with residuals of â€˜GDP per capitaâ€™\n\n\n\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations (R,Python)",
    "section": "Total variance",
    "text": "Total variance\nA correlation captures how much of the total variance is explained by the overlapping variance between the x and y axes. So we first need to capture the total variance. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other."
  },
  {
    "objectID": "correlations/correlations.html#shared-variance-between-x-and-y-aka-covariance",
    "href": "correlations/correlations.html#shared-variance-between-x-and-y-aka-covariance",
    "title": "Correlations (R,Python)",
    "section": "Shared variance between \\(x\\) and \\(y\\) (AKA covariance)",
    "text": "Shared variance between \\(x\\) and \\(y\\) (AKA covariance)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\nRPython\n\n\n\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\nSaving 7 x 5 in image\n\nshared_resid\n\n\n\n\n\n\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n# save the plot\nplt.savefig('shared_resid.png')\n\n\n\n\nScatterplot with shared residualsâ€™\n\n\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearsonâ€™s R correlation we need to compare the amount that \\(x\\) and \\(y\\) vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\nRPython\n\n\n\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n[1] 0.6786624\n\n\n\n\n\n# Import math Library\nimport math\n\nvarxy = sum(\n    (gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())*\n    (gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean()))\n\n\ntotalvar = math.sqrt(\n   sum((gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())**2)*\n   sum((gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean())**2))\nvarxy/totalvar\n\n\n\n\n0.6786623986777583\nIf the above calculation is correct, weâ€™ll get exactly the same value when using the cor.test function:\n\nRPython\n\n\n\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\n\n\n\nimport scipy.stats\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n(0.6786623986777585, 1.6891897969647515e-20)\n\n\n\nAgain, a reminder of how shared variance could be visualised:\n\n\n\n\n\nA question you might have at this point, is whether the above figure of shared variance seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\nRPython\n\n\n\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\ninverted_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], 125-gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=125-gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=125-gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=125-gapminder_2007[\"lifeExp\"], ymax=125-gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('inverted_resid.png')\n\n\n\n\nScatterplot with inverted shared residualsâ€™\n\n\n\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e.Â r=-.679. Notice how thereâ€™s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like itâ€™s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearmanâ€™s Rho (AKA Spearmanâ€™s Rank) instead:\n\nRPython\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\ndef spssSkewKurtosis(x):\n    import pandas as pd\n    import math\n    \n    w=len(x)\n    m1=x.mean()\n    m2=sum((x-m1)**2)\n    m3=sum((x-m1)**3)\n    m4=sum((x-m1)**4)\n    s1=(x).std()\n    skew=w*m3/(w-1)/(w-2)/s1**3\n    sdskew=math.sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n    kurtosis=(w*(w+1)*m4 - 3*m2**2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1**4)\n    sdkurtosis=math.sqrt( 4*(w**2-1) * sdskew**2 / ((w-3)*(w+5)) )\n    \n    ## z-scores added by reading-psych\n    zskew = skew/sdskew\n    zkurtosis = kurtosis/sdkurtosis\n    \n    mat = pd.DataFrame([[skew, sdskew, zskew],[kurtosis, sdkurtosis, zkurtosis]], columns=['estimate','se','zScore'], index=['skew','kurtosis'])\n    \n    return mat\n  \nspssSkewKurtosis(gapminder_2007[\"gdpPercap\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp\"])\n\n\n\n\nSkewness and kurtosis for â€˜gdpPercapâ€™\n\n\n\n\n\nSkewness and kurtosis for â€˜lifeExpâ€™\n\n\n\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e.Â their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearmanâ€™s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations (R,Python)",
    "section": "Spearmanâ€™s Rank (AKA Spearmanâ€™s Rho)",
    "text": "Spearmanâ€™s Rank (AKA Spearmanâ€™s Rho)\nSpearmanâ€™s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e.Â unusually extreme) data points. Letâ€™s now turn the gapminder data weâ€™ve been working with above into ranks and then run a Pearsonâ€™s correlation on it to confirm this:\n\nRPython\n\n\n\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\n\n\ngapminder_2007[\"gdpPercap_rank\"] = gapminder_2007[\"gdpPercap\"].rank()\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\n\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\nRPython\n\n\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\n\n\n\nspssSkewKurtosis(gapminder_2007[\"gdpPercap_rank\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp_rank\"])\n\n\n\n\nSkewness and kurtosis for â€˜gdpPercap_rankâ€™\n\n\n\n\n\nSkewness and kurtosis for â€˜lifeExp_rankâ€™\n\n\n\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e.Â flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e.Â be at a higher risk of false positives. This is evidence that using a Spearmanâ€™s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\nRPython\n\n\n\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\n\n\n\n# Pearson correlation on **ranked** data:\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n# Spearman correlation applied to original data (letting R do the ranking)\nscipy.stats.spearmanr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n(0.8565899189213543, 4.6229745362984015e-42)\n\nSpearmanrResult(correlation=0.8565899189213544, pvalue=4.62297453629821e-42)\n\n\n\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\nRPython\n\n\n\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\nrank_resid\n\n\n\n\n\n\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap_rank\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp_rank\"],xmin=gapminder_2007[\"gdpPercap_rank\"], xmax=gapminder_2007[\"gdpPercap_rank\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp_rank\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap_rank\"],ymin=gapminder_2007[\"lifeExp_rank\"], ymax=gapminder_2007[\"lifeExp_rank\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita (RANK)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy (RANK)\")\n\nred_patch = mpatches.Patch(color='red', label='GDP Residuals')\ngreen_patch = mpatches.Patch(color='green', label='Life Expectancy Residuals')\n\nplt.legend(handles=[red_patch, green_patch],bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# show the plot\nplt.show()\n\n\n\n\nScatterplot for â€˜Life Expectancy (RANK)â€™ and â€˜GDP per capita (RANK)â€™ with residualsâ€™\n\n\n\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\nshared_resid\n\n\n\n\n\n\n\n\n\n\nR values are standardised values\n\n\n\nAs both Pearsonâ€™s R and Spearmanâ€™s Rank are calculations of the proportion of total variance that can be explained by covariance between variabels, they will always be a value between -1 (all variance is explained for a negative association) to 1 (all variance is explained for a positive association). R values are thus standardised, unlike variance and covariance values which have no limit in their values.\n\n\n\nConsolidation questions\n\nQuestion 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/questionMaker.html",
    "href": "extras/questionMaker.html",
    "title": "Question Maker",
    "section": "",
    "text": "If you have any questions you would like to contribute to any of the consolidation questions, this page will help generate the code needed to include them. Alternatively, you can simply suggest a question at:\nhttps://github.com/Reading-Psych/jast/discussions/categories/suggest-content\nIf you would like to generate the code, please complete the following inputs:\nGive your question a unique name. It cannot have a space in it, so use underscores (e.g.Â â€œquestion_1â€ rather than â€œquestion 1â€)\n\nviewof question_name = Inputs.text();\n\n\n\n\n\n\nWrite the question itself here:\n\nviewof question_text = Inputs.text();\n\n\n\n\n\n\nWhat type of question is it?\n\nviewof question_type = Inputs.radio([\n  \"Multiple choice\", \n  \"Numeric\",\n  \"Text\"\n]);\n\n\n\n\n\n\nIf you selected multiple choice, write the options you would like the user to choose from. Please put a | (known as a â€œpipeâ€) character between each option\n\nviewof question_responses = Inputs.text();\n\n\n\n\n\n\nWhat is the correct answer? Be precise in how itâ€™s written.\n\nviewof question_correct   = Inputs.text();\n\n\n\n\n\n\n\noutput_string = {\n  var accuracy_code = \"correct_\" + question_name + \" = '\" + question_correct + \"';\\n\" +\n    question_name + \"_result = {\\n\" +\n    \"  if(\" + question_name + \"_response == correct_\" + question_name + \"){\\n\" +\n    \"    return 'Correct!';\\n\" +\n    \"  } else {\\n\" +\n    \"    return 'Incorrect or incomplete.';\\n\" +\n    \"  };\\n\" +\n    \"}\\n\" +\n    \"```\\n\" +\n    \"${\" + question_name + \"_result}\";\n  switch(question_type){\n    case \"Multiple choice\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.radio(['\" + question_responses.split(\"|\").join(\"','\") + \"']);\\n\" +\n        accuracy_code;\n      break;\n    case \"Numeric\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.number();\\n\" +\n        accuracy_code;\n      break;\n    case \"Text\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.text();\\n\" +\n        accuracy_code;\n      break;\n  }  \n}\n\n\n\n\n\n\n\nQuestion code:"
  },
  {
    "objectID": "extras/allQuestions.html",
    "href": "extras/allQuestions.html",
    "title": "All questions",
    "section": "",
    "text": "Which is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha values areâ€¦\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#describing-data",
    "href": "extras/allQuestions.html#describing-data",
    "title": "All questions",
    "section": "Describing Data",
    "text": "Describing Data\n\nCentral Tendency\n\nQuestion 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDispersion\n\nQuestion 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#distributions",
    "href": "extras/allQuestions.html#distributions",
    "title": "All questions",
    "section": "Distributions",
    "text": "Distributions\n\nNormal\n\nQuestion 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamieâ€™s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer isâ€¦ .\n\nIf you want to practice with different numbers in these questions then please reload the page.\n\n\n\n\nSkewness\n\nQuestion 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is \n\n\n\nTransforming Data\n\nQuestion 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#correlations",
    "href": "extras/allQuestions.html#correlations",
    "title": "All questions",
    "section": "Correlations",
    "text": "Correlations\n\nQuestion 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#item-analysis",
    "href": "extras/allQuestions.html#item-analysis",
    "title": "All questions",
    "section": "Item Analysis",
    "text": "Item Analysis\n\nCronbach Alpha\n\nQuestion 1\nCronbachâ€™s Alpha is useful to check whether items in a measure areâ€¦\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbachâ€™s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#validity-checks",
    "href": "extras/allQuestions.html#validity-checks",
    "title": "All questions",
    "section": "Validity checks",
    "text": "Validity checks\n\nMultiple testing\n\nQuestion 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof multiple_testing_1_response = Inputs.radio(['True','False']);\ncorrect_multiple_testing_1 = 'False';\nmultiple_testing_1_result = {\n  if(multiple_testing_1_response == correct_multiple_testing_1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof multiple_testing_2_response = Inputs.radio(['Bonferroni','Å idÃ¡k']);\ncorrect_multiple_testing_2 = 'Å idÃ¡k';\nmultiple_testing_2_result = {\n  if(multiple_testing_2_response == correct_multiple_testing_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  }
]