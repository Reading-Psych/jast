[
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency (R,Python)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started\n\n\n\nCentral tendancy describes typical values of a variable, such as it’s mean and median.\n\nMean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\nLoading required package: gapminder\n\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n  \n\n\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# import the tabulate\nfrom tabulate import tabulate\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n#display table\nprint(tabulate(gapminder_2007[:6], headers=gapminder_2007.head() , tablefmt=\"fancy_grid\",showindex=False ))\n\n# total of all years\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n\n\nTable\n\n\n67.00742253521126\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n67.00742253521126\n\n\n\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\ngapminder_2007['lifeExp'].median()\n\n67.00742253521126\n71.93549999999999\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\ngapminder_2007['lifeExp'].mode()\n\n\n\n\nMode of ‘lifeExp’\n\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\nlength(gapminder_2007$lifeExp)\n\n[1] 142\n\nlength(unique(gapminder_2007$lifeExp))\n\n[1] 142\n\n\n\n\n\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n142\n142\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n[1] 2 4\n\n\n\n\n\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n\n\nExample of Mode\n\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most.\n\nQuestion 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/centralTendencyQuestions.html",
    "href": "describingData/centralTendencyQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/dispersion.html",
    "href": "describingData/dispersion.html",
    "title": "Dispersion (R,Python)",
    "section": "",
    "text": "To understand distributions such as the normal distribution, it’s helpful to clarify some more basic concepts around how data is dispersed or spread."
  },
  {
    "objectID": "describingData/dispersion.html#range",
    "href": "describingData/dispersion.html#range",
    "title": "Dispersion (R,Python)",
    "section": "Range",
    "text": "Range\nRange simply captures the min(imum) and the max(imum) values. Lets look at the min and max for the life expectancy data from 2007:\n\nRPython\n\n\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmin(gapminder_2007$lifeExp)\n\n[1] 39.613\n\nmax(gapminder_2007$lifeExp)\n\n[1] 82.603\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].min()\n\n82.603\n\ngapminder_2007['lifeExp'].max()\n\n39.613\n\n\n\nSo the range for life expectancy in 2007 was between 39.613 and 82.603."
  },
  {
    "objectID": "describingData/dispersion.html#variance",
    "href": "describingData/dispersion.html#variance",
    "title": "Dispersion (R,Python)",
    "section": "Variance",
    "text": "Variance\n\nPopulation Variance\nVariance is how much the data varies around a mean. To capture this, we compare each individual’s score with the mean, so lets do this with our gapminder data’s life expectancy:\n\nRPython\n\n\n\nlife_expectancy_variance_table <- data.frame(\n  life_expectancy = gapminder_2007$lifeExp,\n  diff_from_mean  = gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp)\n)\n\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nimport pandas as pd\nfrom tabulate import tabulate\n\nlife_expectancy_variance_table = {\n  'life_expectancy' : gapminder_2007['lifeExp'],\n  'diff_from_mean': gapminder_2007['lifeExp']- gapminder_2007['lifeExp'].mean(),\n}\n\n# convert it to a data frame\nlife_expectancy_variance_table = pd.DataFrame(life_expectancy_variance_table)\n\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\nTable\n\n\n\n\n\nSo we know for each country how different their life expectacy is to the mean life expectancy. But ideally we would like a single value to summarise variance. Lets see what would happen if we tried to summarise these differences from the mean by calculating the mean difference from the mean:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_from_mean) \n\n[1] 5.153937e-15\n\n\n\n\n\nlife_expectancy_variance_table['diff_from_mean'].mean()\n\n5.254013186958487e-15\n\n\n\nWe get a number that is effectively zero (go here for an explanation about e-numbers), because all the values above the mean balance out those below the mean. So to address this, we can square the differences to force all the numbers to be positive:\n\nRPython\n\n\n\nlife_expectancy_variance_table$diff_squared = life_expectancy_variance_table$diff_from_mean^2\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'] = life_expectancy_variance_table['diff_from_mean'].pow(2)\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\nTable\n\n\n\n\n\nIf we calculate the average of this, then we get a summary of the variance that is more informative:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_squared)\n\n[1] 144.7314\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'].mean()\n\n144.73136049752028\n\n\n\nHowever, as mean is what you get when you add all the items together and then divide it by the number of items, this can also be done in 2 steps in R (this will help us understand the formula later):\n\nRPython\n\n\n\nsum_of_squares = sum(life_expectancy_variance_table$diff_squared)\nthis_variance  = sum_of_squares/length(life_expectancy_variance_table$diff_squared)\nthis_variance\n\n[1] 144.7314\n\n\n\n\n\nsum_of_squares = life_expectancy_variance_table['diff_squared'].sum()\nthis_variance = sum_of_squares/life_expectancy_variance_table['diff_squared'].count()\nthis_variance\n\n144.73136049752028\n\n\n\nWe can represent the above in the following formula for the population’s (remember, this is when you have everyone from the group you are measuring) variance:\n\\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N}\n\\]\nLet’s break down each of the above symbols: σ^2 is population variance Σ is sum xi refers to the value for each participant x̄ refers to the mean for all participants N refers to the number of participants\n(note that the above is written as if we’re looking at the variance of a group of participants, but the principles still work if looking at non-participant data)\n\n\nSample variance\nTo calculate the variance for a sample of participants, rather than every participant in the group you’re measuring, you need a slightly different formula:\n\\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N - 1}\n\\] Note that Sample variance is represented by S^2 rather than σ^2\nSo why do we divide by N-1 rather than N? This is because the sample variance is an estimate rather than the actual population variance. When estimating the population variance you take into account the actual number of people (N) in the sample, whereas when you are estimating what happens generally in the population based on your sample, you take into account the degrees of freedom (N-1). In broad terms this reduces the risk of you under-estimating the variance of the population. You don’t necessarily need to understand degrees of freedom beyond the idea that you are controlling for the fact that you are analysing a sample rather than the population they represent, so don’t worry if the next section isn’t completely clear."
  },
  {
    "objectID": "describingData/dispersion.html#degrees-of-freedom",
    "href": "describingData/dispersion.html#degrees-of-freedom",
    "title": "Dispersion (R,Python)",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom calculations can be useful to address statistics that are vulnerable to bias within a sample (i.e. the sample being distorted compared to the population). Interestingly, mean is not, but variance is. Lets see this by looking at differences between the population and sample for mean and variance, looking at the height of three people, and combining them into every combination of 2 people possible:\n\nRPython\n\n\n\nthree_heights = c(150,160,170)\npopulation_height_mean = mean(three_heights)\npopulation_height_variance = sum((three_heights - population_height_mean)^2)/3\n#sample participants in pairs\nsample_heights = data.frame(\n  pp1 = c(150,150,NA),\n  pp2 = c(160,NA,160),\n  pp3 = c(NA,170,170),\n  pair = c(\n    \"1 and 2\",\n    \"1 and 3\",\n    \"2 and 3\"\n  )\n)\n\nsample_heights$mean = c(\n  mean(c(three_heights[1], three_heights[2])),\n  mean(c(three_heights[1], three_heights[3])),\n  mean(c(three_heights[2], three_heights[3]))\n)\n\nsample_heights$pop_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/3,\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/3,\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/3\n)\n\nsample_heights$sample_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/(3-1),\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/(3-1),\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/(3-1)\n)\n\n\nrmarkdown::paged_table(sample_heights)\n\n\n\n  \n\n\nmean_sample_mean <- mean(sample_heights$mean)\nmean_sample_variance <- mean(sample_heights$sample_var)\nmean_population_variance <- mean(sample_heights$pop_var)\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nthree_heights = [150,160,170]\nthree_heights = pd.DataFrame(three_heights)\npopulation_height_mean = three_heights.mean()\npopulation_height_variance = (three_heights - population_height_mean).pow(2).sum()/3\n\n#sample participants in pairs\nsample_heights = {\n  'pp1': [150,150, np.nan],\n  'pp2': [160, np.nan, 160],\n  'pp3': [np.nan, 170, 170],\n  'pair': [\"1 and 2\", \"1 and 3\", \"2 and 3\"]\n}\n\nsample_heights = pd.DataFrame(sample_heights)\nmean = [three_heights.iloc[0:2].mean(),three_heights.iloc[[0,2],].mean(),three_heights.iloc[1:3].mean()]\nmean = pd.DataFrame(mean)\nsample_heights['mean']=mean\n\npop_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/3,((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/3,((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/3]\npop_var = pd.DataFrame(pop_var)\nsample_heights['pop_var']= pop_var\n\nsample_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/(3-1),((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/(3-1),((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/(3-1)]\nsample_var=pd.DataFrame(sample_var)\nsample_heights['sample_var']=sample_var\n\n\nsample_heights = pd.DataFrame(sample_heights)\n#print(markdownTable(sample_heights.to_dict(orient='records')).getMarkdown())\nprint(tabulate(sample_heights, headers=sample_heights.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nmean_sample_mean = sample_heights['mean'].mean()\nmean_sample_variance = sample_heights['sample_var'].mean()\nmean_population_variance = sample_heights['pop_var'].mean()\n\n\n\n\nTable\n\n\n\n\n\nWhen comparing the population mean to the mean sample mean (i.e., what is the typical mean for any sample), they’re identical (i.e. NOT biased):\n\nRPython\n\n\n\npopulation_height_mean\n\n[1] 160\n\nmean_sample_mean\n\n[1] 160\n\n\n\n\n\npopulation_height_mean\nmean_sample_mean\n\n0    160.0\ndtype: float64\n\n160.0\n\n\n\nWhereas when comparing the actual population variance (population_height_variance) to the mean (to identify what is a typical) estimate of variance using the population formula that should not be used for samples (mean_population_variance) finds the estimate of variance is typically smaller than the actual variance in the population:\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_population_variance\n\n[1] 33.33333\n\n\n\n\n\npopulation_height_variance\nmean_population_variance\n\n0    66.666667\ndtype: float64\n\n33.333333333333336\n\n\n\nAs this bias (almost) always underestimates the population variance, degrees of freedom is a useful correction to address this within calculations of sample variance. Lets compare the actual population height variance (population_height_variance) to the mean estimate using degrees of freedom that should be used for samples (mean_sample_variance).\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_sample_variance\n\n[1] 50\n\n\n\n\n\npopulation_height_variance\nmean_sample_variance\n\n0    66.666667\ndtype: float64\n\n50.0\n\n\n\nSo, not perfect, but this is less under-representative of the variance.\nOne thing to bear in mind is that calculation of some statistics does not require use of the degrees of freedom to correct for bias (as seen above, mean was not susceptible to bias).\nIf you would like to understand how degrees of freedom are determined, and what the thinking is behind this term, read on for a brief description of this (otherwise, feel free to skip to the next section).\nDegrees of freedom refers to how many values could change in your variable once you know what the outcome of the relevant statistic is. For example, if you’re interested in the variance of the height of the three people, then you only have 2 degrees of freedom, because once you know the height of 2 of the participants AND the variance of the height, then there the remaining participant only has a 2 possible heights (so their height isn’t free to change)."
  },
  {
    "objectID": "describingData/dispersion.html#standard-deviation-sd",
    "href": "describingData/dispersion.html#standard-deviation-sd",
    "title": "Dispersion (R,Python)",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is the square root of the variance. This takes into account that that the variance includes the square of the difference between the individual values and the mean:\n\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nVariance\n\\[                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                               \\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}                \n                                                                                                                                                                                                                                                                                                                                   \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     S^2 = \\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}                \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\]\n\n\nSD\n\\[                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                              \\sigma = \\sqrt\\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}              \n                                                                                                                                                                                                                                                                                                                                   \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    S = \\sqrt\\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}              \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\]"
  },
  {
    "objectID": "describingData/dispersion.html#effect-size",
    "href": "describingData/dispersion.html#effect-size",
    "title": "Dispersion (R,Python)",
    "section": "Effect size",
    "text": "Effect size\nEffect size is basically what it sounds like, it is a measure of how big the effect you are investigating is. You may find a difference between participants or conditions, but effect size calculations give you a sense of whether these are big or small effects. We will use Cohen’s D as an example of an effect size calculation to illustrate this issue.\nCohen’s \\(d\\) is used to capture the effect size in three situations:\n\nComparing a set of data against a single value\nComparing two conditions (within-subject)\nComparing two groups of participants (between-subject)\n\nLet’s do a within-subject comparison of life expectancy between 2002 and 2007 across the world to illustrate the above.\nThe general formula for Cohen’s D when you are comparing within subject is:\n\\[\nd = \\frac{M_{difference}}{SD_{difference}}\n\\]\nThe general benchmarks for how big or small a Cohen’s \\(d\\) value are as follows:\n\n.01 is very small (Sawilowsky 2009)\n.2 and below is small (Cohen 2013)\n.5 is medium (Cohen 2013)\n.8 is large (Cohen 2013)\n1.2 is very large (Sawilowsky 2009)\n2 is huge (Sawilowsky 2009)\n\n\nRPython\n\n\n\n# Manual calculation\nlife_exp_diff = gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]\nmean(life_exp_diff)/sd(life_exp_diff)\n\n[1] 1.230619\n\n# Function to confirm\nlibrary(lsr)\ncohensD(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002],method = \"paired\")\n\n[1] 1.230619\n\n\n\n\n\n# Manually calculated\ngapminder_2002 = gapminder.loc[gapminder['year'] == 2002]\nlife_exp_diff = np.array(gapminder_2007['lifeExp']) - np.array(gapminder_2002['lifeExp'])\nlife_exp_diff.mean()\nlife_exp_diff.std(ddof=1)\nlife_exp_diff.mean()/life_exp_diff.std(ddof=1)\n\n1.3124999999999996\n1.0665367479612573\n1.230618637856515\n\n\n\nNow, you may be thinking that we already had insight into the effect size simply by comparing means, in which we can see that life expectancy has gone up by 1.31 years between 2002 and 2007. That’s true, and depending on your research question you may want to acknowledge whether the change is meaningful (i.e. is one year’s higher life expectancy a big deal?). However, you might not say an average increase of 1.31 years in life expectancy is a large effect if the change of life expectancy is very inconsistent between countries (e.g. some countries life expectancy decreased). Effect size calculation takes into consideration the how consistent the data is (e.g. how small the SD is), and so gives you further insight than a simple mean difference. The large effect size (1.23) confirms that there is a large effect of time on life expectancy, that there is a big difference relative to the general variation in the data.\nNote that many tests described later have their own effect size calculations associated with them:\n\n\n\nTest\nEffect Size Unit/Test\n\n\n\n\nT-tests\nCohen’s \\(d\\)\n\n\nCorrelations\n\\(r\\)\n\n\nRegression\n\\(R^2\\)\n\n\nANOVAs\n\\(\\eta\\)\n\n\n\n\nQuestion 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "describingData/dispersionQuestions.html",
    "href": "describingData/dispersionQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Show/Hide all\n\n\n\n\n\n\n\nCourse Overview\n\n\n\n\n\n\n\n\n\n\n\nRed means that the page does not exist yet\nGray means that the page doesn’t yet have separation of different levels of understanding\nOrange means that the page is started\nIn this website you can choose to expand or shrink the page to match the level of understanding you want.\n\nIf you do not expand any (green) subsections then you will only see the most superficial level of description about the statistics. If you expand the green subsections you will get details that are required to complete the tests, but perhaps not all the explanations for why the statistics work.\n\n\n\n\n\n\n\nAn example of a green subsection\n\n\n\n\n\n\n\nIf you expand the blue subsections you will also see some explanations that will give you a more complete understanding. If you are completing MSc-level statistics you would be expected to understand all the blue subsections.\n\n\n\n\n\n\n\nAn example of a blue subsection\n\n\n\n\n\n\n\nRed subsections will go deeper than what is expected at MSc level, such as testing higher level concepts.\n\n\n\n\n\n\n\nAn example of a red subsection"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html",
    "href": "itemAnalyses/cronbachsAlpha.html",
    "title": "Cronbach’s Alpha (R,Python)",
    "section": "",
    "text": "If you want to address whether a questionnaire or scale’s items are measuring the same underlying variable, it can be helpful to assess how much overlap there is between the items.\nLet’s use some publicly available data to investigate this issue. The Empathy Quotient (Lawrence et al. 2004) is a self-report measure of how empathic someone is. There are a variety of data sets from people who have used this measure, so we’ll use some data from the following repository:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\nWhilst this is a general measure of empathy, empathy is a multifacted concept, so let’s focus on one of the types of empathy, cognitive empathy. This will involve focusing on just items associated with this facet. Note that the items for this are 14, 15, 29, 34 and 35. Let’s see how much covariance (see here for a reminder on covariance or shared variance) there is between the items:\n\neq <- read.csv(\"EQ_Data.csv\")\ncog_eq <- eq[,c(\"Q14.processed\", \"Q15.processed\", \"Q29.processed\", \"Q34.processed\", \"Q35.processed\")]\n\n# to improve readability, relabel the variables\ncolnames(cog_eq) <-c(\"cog_1\",\"cog_2\",\"cog_3\",\"cog_4\",\"cog_5\")\nknitr::kable(data.frame(cov(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.3687127\n0.1106138\n0.0976130\n0.1628303\n0.1012361\n\n\ncog_2\n0.1106138\n0.2787724\n0.1086957\n0.0812020\n0.0230179\n\n\ncog_3\n0.0976130\n0.1086957\n0.3060529\n0.0829071\n0.0967604\n\n\ncog_4\n0.1628303\n0.0812020\n0.0829071\n0.3687127\n0.0865303\n\n\ncog_5\n0.1012361\n0.0230179\n0.0967604\n0.0865303\n0.3674339\n\n\n\n\n\nNote that covariance of an item with itself is just variance, e.g. cog_1 with itself:\n\nvar(cog_eq$cog_1)\n\n[1] 0.3687127\n\n\nThe above covariance matrix suggests all the items have some overlap - all associations are positive. To get a sense of how strongly they overlap a correlation matrix could be more useful, so let’s quickly look at that:\n\nknitr::kable(data.frame(cor(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n1.0000000\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\n1.0000000\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\n1.0000000\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\n1.0000000\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\n1.0000000\n\n\n\n\n\nTo create an aggregate of whether these all seem to reliably be measuring the same construct, we can calculate the Cronbach’s Alpha, so let’s do that next. Cronbach’s Alpha can be calculated in a few ways. One way to conceptualise the cronbach’s alpha is:\n\\[\n\\frac{average Covariance Between Variables}{average(co)Variance}\n\\]\nUsing the table above: this is the average of all the cells that capture covariance between items:\n\n(which captures how much the variables overlap) divided by the average of all variance of items with both with other items and with themselves\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.3687127\n0.1106138\n0.0976130\n0.1628303\n0.1012361\n\n\ncog_2\n0.1106138\n0.2787724\n0.1086957\n0.0812020\n0.0230179\n\n\ncog_3\n0.0976130\n0.1086957\n0.3060529\n0.0829071\n0.0967604\n\n\ncog_4\n0.1628303\n0.0812020\n0.0829071\n0.3687127\n0.0865303\n\n\ncog_5\n0.1012361\n0.0230179\n0.0967604\n0.0865303\n0.3674339\n\n\n\n\n\nWhich can be summarised as follows:\n\\[\n\\alpha = \\frac{\\bar{COV}}{(\\sum{s_i^2} + \\sum{COV_i})/N^2}\n\\]\nBased on a formula from (Field 2013) (Fourth edition, page 708)\n\n\\(N\\) is the number of items\n\\(\\bar{COV}\\) is the average covariance between items (note that this does not include an items covariance with itself)\n\\(s_i\\) is the standard deviation for a single item (remember that squaring it give\n\\(COV_i\\) is the covariance for an item with another item\nThe bottom half together is the sum of a complete covariance matrix like the one directly above.\n\nWhat’s nice about this formula, is that it’s quite easy to implement:\n\n\\(N^2\\) is \\(5^2\\) which makes 25\n\\(\\bar{COV}\\) is the mean for the above covariance matrix after removing comparisons of items with themselves, so the mean of:\n\n\n# we know that a correlation matrix will have 1 where an item is correlating with itself, so we'll create an index to skip those items:\ncov_itself = cor(cog_eq) == 1\n\n# now let's calculate the mean of the valid parts of the covariance matrix\ncov_df = data.frame(cov(cog_eq))\ncov_df[cov_itself] = \"\"\nknitr::kable(cov_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n\n0.110613810741688\n0.097612958226769\n0.162830349531117\n0.101236146632566\n\n\ncog_2\n0.110613810741688\n\n0.108695652173913\n0.0812020460358057\n0.0230179028132992\n\n\ncog_3\n0.097612958226769\n0.108695652173913\n\n0.0829070758738278\n0.096760443307758\n\n\ncog_4\n0.162830349531117\n0.0812020460358057\n0.0829070758738278\n\n0.0865302642796247\n\n\ncog_5\n0.101236146632566\n0.0230179028132992\n0.096760443307758\n0.0865302642796247\n\n\n\n\n\nmean_cov_df = mean(cov(cog_eq)[!cov_itself])\n\nwhich would make 0.0951407\n\n\\(\\sum{s_i^2}\\) is the sum of the variance of each item. In our case, we already have that information when we calculated the “covariance” of each item with itself:\n\n\nmean_var_df <- data.frame(cov(cog_eq))\nmean_var_df[!cov_itself] = \"\"\nknitr::kable(mean_var_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n0.368712702472293\n\n\n\n\n\n\ncog_2\n\n0.278772378516624\n\n\n\n\n\ncog_3\n\n\n0.306052855924979\n\n\n\n\ncog_4\n\n\n\n0.368712702472293\n\n\n\ncog_5\n\n\n\n\n0.367433930093777\n\n\n\n\nsum_var_df = mean(cov(cog_eq)[cov_itself])\n\n\n\\(\\sum{COV_i}\\) is just the sum of the all the covariances, which actually are the same values we focus on for the mean:\n\n\nknitr::kable(cov_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n\n0.110613810741688\n0.097612958226769\n0.162830349531117\n0.101236146632566\n\n\ncog_2\n0.110613810741688\n\n0.108695652173913\n0.0812020460358057\n0.0230179028132992\n\n\ncog_3\n0.097612958226769\n0.108695652173913\n\n0.0829070758738278\n0.096760443307758\n\n\ncog_4\n0.162830349531117\n0.0812020460358057\n0.0829070758738278\n\n0.0865302642796247\n\n\ncog_5\n0.101236146632566\n0.0230179028132992\n0.096760443307758\n0.0865302642796247\n\n\n\n\n\n\nThis makes it quite simple to calculate the whole of the bottom row, as we can sum the entire covariance matrix (that includes the variances of each item with itself):\n\nsum(cov(cog_eq))\n\n[1] 3.592498\n\n\nPut together, we can calculate \\(alpha\\):\n\nmean(cov(cog_eq)[cor(cog_eq) != 1])/ # mean covariance\nmean(cov(cog_eq))                    # mean of covariance and variance combined\n\n[1] 0.6620788\n\n\nLet’s compare this to the psych packages alpha function (note that it is likely that RStudio will have another alpha function active that is not cronbach’s alpha if you haven’t loaded the psych package!):\n\neq_alpha_output <- psych::alpha(cog_eq)\nknitr::kable(eq_alpha_output$total) # note that there other outputs available\n\n\n\nTable 1: Psych Package’s Cronbach Alpha calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_alpha\nstd.alpha\nG6(smc)\naverage_r\nS/N\nase\nmean\nsd\nmedian_r\n\n\n\n\n\n0.6620788\n0.662596\n0.6330763\n0.2820018\n1.963806\n0.0642007\n1.22029\n0.3790777\n0.2817932"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#standardised-alpha",
    "href": "itemAnalyses/cronbachsAlpha.html#standardised-alpha",
    "title": "Cronbach’s Alpha (R,Python)",
    "section": "Standardised Alpha",
    "text": "Standardised Alpha\nVery similar logic to above, but instead of dividing the average covariate between items by the average covariate between and within items, we divide the average r value between items by the average r-value for both between and within items:\n\\[\n\\alpha = \\frac{\\bar{r_{b}}}{\\bar{r}}\n\\]\n\n\\(\\bar{r_b}\\) being the average r-value between items\n\\(\\bar{r}\\) being the average r-value both between and within items\n\n\n\n\n\n\n\nCronbach’s Alpha vs. Standardised Cronbach’s Alpha\n\n\n\nRemember that r-values are comparisons of covariates to the total variance (see here), and are thus standardised compared to covariates that are not divided by the total variance. One way to think about this is that correlations are standardised between -1 to 1, whereas variance has no lower or upper limit.\n\n\nThis is dividing the mean of:\n\ncor_df = data.frame(cor(cog_eq))\ncor_df[cov_itself] = NA\nknitr::kable(cor_df)\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\nNA\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\nNA\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\nNA\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\nNA\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\nNA\n\n\n\n\n\nby the mean of:\n\nknitr::kable(data.frame(cor(cog_eq)))\n\n\n\n\n\ncog_1\ncog_2\ncog_3\ncog_4\ncog_5\n\n\n\n\ncog_1\n1.0000000\n0.3450170\n0.2905797\n0.4416185\n0.2750438\n\n\ncog_2\n0.3450170\n1.0000000\n0.3721252\n0.2532783\n0.0719203\n\n\ncog_3\n0.2905797\n0.3721252\n1.0000000\n0.2468024\n0.2885426\n\n\ncog_4\n0.4416185\n0.2532783\n0.2468024\n1.0000000\n0.2350901\n\n\ncog_5\n0.2750438\n0.0719203\n0.2885426\n0.2350901\n1.0000000\n\n\n\n\n\nThis can be calculated quite elegantly:\n\nmean(cor(cog_eq)[cor(cog_eq) != 1])/ # mean correlation between items\nmean(cor(cog_eq))                    # mean of all correlations\n\n[1] 0.662596\n\n\nWhich matches the value of std.alpha in the output in Table 1."
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#general-benchmarks-for-cronbachs-alpha",
    "href": "itemAnalyses/cronbachsAlpha.html#general-benchmarks-for-cronbachs-alpha",
    "title": "Cronbach’s Alpha (R,Python)",
    "section": "General benchmarks for Cronbach’s alpha",
    "text": "General benchmarks for Cronbach’s alpha\nThere are caveats to these benchmarks, and weird things that can happen with Cronbach’s alpha, but here are some broad benchmarks about how reliable your items are based on their \\(\\alpha\\) value:\n\nLess than .6 is unacceptable\n.6 to .7 is mediocre\nGreater than .7 is acceptable reliability\nGreater than .9 suggests that your measure includes redundant items"
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#some-warnings",
    "href": "itemAnalyses/cronbachsAlpha.html#some-warnings",
    "title": "Cronbach’s Alpha (R,Python)",
    "section": "Some warnings",
    "text": "Some warnings\nMore items in a measure generally means a higher CA, even if the associations between the items is consistent. For example, imagine we had the following correlation matrix:\n\nexample_1_small <- data.frame(\n  item_1 = c(1,.1,.1,.1),\n  item_2 = c(.1,1,.1,.1),\n  item_3 = c(.1,.1,1,.1),\n  item_4 = c(.1,.1,.1,1)\n)\nknitr::kable(example_1_small)\n\n\n\n\nitem_1\nitem_2\nitem_3\nitem_4\n\n\n\n\n1.0\n0.1\n0.1\n0.1\n\n\n0.1\n1.0\n0.1\n0.1\n\n\n0.1\n0.1\n1.0\n0.1\n\n\n0.1\n0.1\n0.1\n1.0\n\n\n\n\n\nWe would get the following standardised \\(\\alpha\\):\n\nmean(example_1_small[example_1_small != 1])/ # mean correlation between items\nmean(as.matrix(example_1_small))             # mean of all correlations\n\n[1] 0.3076923\n\n\nLet’s throw in a few more items with exactly the same strength of association to everything else:\n\nexample_1_large <- data.frame(\n  item_1 = c(1,.1,.1,.1,.1,.1,.1,.1),\n  item_2 = c(.1,1,.1,.1,.1,.1,.1,.1),\n  item_3 = c(.1,.1,1,.1,.1,.1,.1,.1),\n  item_4 = c(.1,.1,.1,1,.1,.1,.1,.1),\n  item_5 = c(.1,.1,.1,.1,1,.1,.1,.1),\n  item_6 = c(.1,.1,.1,.1,.1,1,.1,.1),\n  item_7 = c(.1,.1,.1,.1,.1,.1,1,.1),\n  item_8 = c(.1,.1,.1,.1,.1,.1,.1,1)\n)\nexample_1_large\n\n  item_1 item_2 item_3 item_4 item_5 item_6 item_7 item_8\n1    1.0    0.1    0.1    0.1    0.1    0.1    0.1    0.1\n2    0.1    1.0    0.1    0.1    0.1    0.1    0.1    0.1\n3    0.1    0.1    1.0    0.1    0.1    0.1    0.1    0.1\n4    0.1    0.1    0.1    1.0    0.1    0.1    0.1    0.1\n5    0.1    0.1    0.1    0.1    1.0    0.1    0.1    0.1\n6    0.1    0.1    0.1    0.1    0.1    1.0    0.1    0.1\n7    0.1    0.1    0.1    0.1    0.1    0.1    1.0    0.1\n8    0.1    0.1    0.1    0.1    0.1    0.1    0.1    1.0\n\n\n\nmean(example_1_large[example_1_large != 1])/ # mean correlation between items\nmean(as.matrix(example_1_large))             # mean of all correlations\n\n[1] 0.4705882\n\n\n\nDoes this inflation of alpha get worse if the average associations between items are stronger or weaker?\nTo visualise how rapidly the alpha values increase with the number of items, and if this is a bigger or smaller problems if the average association is stronger or weaker, let’s calculate the alpha values you would get for a range of r-values (.1 to .9) and number of items (5 to 100):\n\nlibrary(ggplot2)\nalpha_inflation <- data.frame(\n  item_n  = rep(5:100,9),\n  r_value = (rep(c(.025, .05, .1, .2, .3, .4, .5, .6, .7), each = 96)),\n  alpha   = NA\n)\nfor(i in 1:length(alpha_inflation$item_n)){\n  this_r_value = alpha_inflation$r_value[i]\n  this_r_value = alpha_inflation$item_n[i]\n  this_example_cor_matrix = matrix(\n    alpha_inflation$r_value[i],\n    alpha_inflation$item_n[i],\n    alpha_inflation$item_n[i]\n  )\n  for(j in 1:alpha_inflation$item_n[i]){\n    this_example_cor_matrix[j,j] = 1\n  }\n  alpha_inflation$alpha[i] = mean(this_example_cor_matrix[this_example_cor_matrix != 1])/ # mean correlation between items\nmean(as.matrix(this_example_cor_matrix))             # mean of all correlations\n}\n\nggplot(data=alpha_inflation, aes(x=item_n, y=alpha, group=r_value, color=factor(r_value))) +\n  geom_line()+\n  geom_point() +\n  xlab(\"Number of Items\")  +\n  ylab(\"Standardised Cronbach's Alpha\") +\n  scale_color_discrete(name=\"Average \\nr-value\") + \n  geom_hline(yintercept = .7, color = \"black\") \n\n\n\n\nFigure 1: Visualisation of how an increased number of items inflates Cronbach’s Alpha even if the average association between items is kept consistent. Note that whilst this has been done on standardised Cronbach’s Alpha, the principle applies even for calculation of non-standardised cronbach’s Alpha. The black line is at a value of .7.\n\n\n\n\nWe can see above that the alpha value isn’t inflated hugely when the average correlation is already high, which makes sense as there isn’t much scope for it to be inflated. Measures with items with only weak associations are heavily influenced though. If you have a measure with 100 items, a Standardised Cronbach’s Alpha of .7 will be achieved even if the average \\(r\\) value is as low as .025. In this sort of situation, it might be more helpful to identify sub-factors within your measure using analysis like Principle Components Analysis or Confirmatory Factor Analysis."
  },
  {
    "objectID": "itemAnalyses/cronbachsAlpha.html#consolidation-questions",
    "href": "itemAnalyses/cronbachsAlpha.html#consolidation-questions",
    "title": "Cronbach’s Alpha (R,Python)",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\nCronbach’s Alpha is useful to check whether items in a measure are…\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbach’s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "itemAnalyses/cfa.html",
    "href": "itemAnalyses/cfa.html",
    "title": "Confirmatory Factor Analysis",
    "section": "",
    "text": "Questionnaires and surveys often have items that reflect multiple sub-scales or multiple (possibly related) factors (AKA components or sub-scales), in a single measure. In Confirmatory Factor Analysis you already have allocated items to specific factors (e.g. based on literature), but need to confirm whether these are valid allocations.\nFor example, the empathy quotient [@Lawrence2004] has multiple types of empathy within it:\n\ncognitive empathy\nemotional reactivity\nsocial skills\n\nLet’s use one publicly available data set to check if we can confirm that the allocations of items to each of these subscales is consistent with the current data:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\n\neq <- read.csv(\"EQ_Data.csv\")\n# focusing on q1.raw to q40.raw\neq_processed <- eq[,c(paste(\"Q\",1:40,\".processed\",sep=\"\"))]\nrmarkdown::paged_table(eq_processed)\n\n\n\n  \n\n\ncolnames(eq_processed) <- paste(\"eq\",1:40, sep=\"\")\n\n\nrmarkdown::paged_table(data.frame(cor(eq_processed)))"
  },
  {
    "objectID": "itemAnalyses/cfa.html#confirmatory-factor-analysis",
    "href": "itemAnalyses/cfa.html#confirmatory-factor-analysis",
    "title": "Confirmatory Factor Analysis",
    "section": "Confirmatory factor analysis",
    "text": "Confirmatory factor analysis\nWe already have three subscales for the empathy quotient, so let’s confirm whether our current data is consistent with them. The subscales are:\n\n\n\n\n\n\nSome of the items below are reverse scored\n\n\n\nThe items that would go against the component, e.g. “I find it hard to know what to do in a social situation” if the participant agreed are reverse scored, meaning that strongly disagreeing increases their score for the relevant subscale (in this case “social skills”).\n\n\nCognitive:\n\n14: I am good at predicting how someone will feel.\n15: I am quick to spot when someone in a group is feeling awkward or uncomfortable.\n29: I can sense if I am intruding, even if the other person doesn’t tell me.\n34: I can tune into how someone else feels rapidly and intuitively.\n35: I can easily work out what another person might want to talk about.\n\nSocial Skills:\n\n2: I find it difficult to explain to others things that I understand easily, when they don’t understand it first time.\n4: I find it hard to know what to do in a social situation.\n7: Friendships and relationships are just too difficult, so I tend not to bother with them.\n8: I often find it difficult to judge if something is rude or polite.\n21: I don’t tend to find social situations confusing.\n\nEmotion:\n\n3: I really enjoy caring for other people.\n16: If I say something that someone else is offended by, I think that that’s their problem, not mine.\n19: Seeing people cry doesn’t really upset me.\n33: I usually stay emotionally detached when watching a film.\n39: I tend to get emotionally involved with a friend’s problems.\n\nLet’s start by looking at correlation matrices to see if the items tend to correlate with each other in the current data set:\n\nCognitive empathy\n\ncor(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")])\n\n          eq14       eq15      eq29      eq34       eq35\neq14 1.0000000 0.34501695 0.2905797 0.4416185 0.27504384\neq15 0.3450170 1.00000000 0.3721252 0.2532783 0.07192026\neq29 0.2905797 0.37212520 1.0000000 0.2468024 0.28854264\neq34 0.4416185 0.25327834 0.2468024 1.0000000 0.23509011\neq35 0.2750438 0.07192026 0.2885426 0.2350901 1.00000000\n\n\nSo far looking good, as everything positively correlates - mostly with r-values greater than .25.\n\n\nSocial Skills\n\ncor(eq_processed[,c(\"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\")])\n\n             eq2       eq4         eq7         eq8        eq21\neq2   1.00000000 0.2067016 -0.06714029  0.06086131  0.30817434\neq4   0.20670163 1.0000000  0.16596510  0.16674209  0.25085525\neq7  -0.06714029 0.1659651  1.00000000 -0.02137924  0.21971081\neq8   0.06086131 0.1667421 -0.02137924  1.00000000 -0.04748266\neq21  0.30817434 0.2508553  0.21971081 -0.04748266  1.00000000\n\n\nThis is a bit less convincing, as the 8th item doesn’t consistently correlate with other items in this subscale.\n\n\nEmotional empathy\n\ncor(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n\n           eq3       eq16       eq19      eq33      eq39\neq3  1.0000000 0.19886409 0.23743638 0.2444680 0.3494996\neq16 0.1988641 1.00000000 0.09384386 0.2801024 0.2080836\neq19 0.2374364 0.09384386 1.00000000 0.1884394 0.3160680\neq33 0.2444680 0.28010241 0.18843936 1.0000000 0.2923009\neq39 0.3494996 0.20808361 0.31606800 0.2923009 1.0000000\n\n\nEmotional empathy seems similarly consistent to cognitive empathy.\nLet’s now see how well each item loads onto the total of these scores:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\neq_subscales_r <- data.frame(\n  cor(\n    eq_processed[,c(\n      # Cog\n      \"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\",\n      \n      # Social Skills\n      \"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\",\n      \n      # Emotion\n      \"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\"\n      )],\n    matrix(data = c(\n      rowSums(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")]),\n      rowSums(eq_processed[,c(\"eq2\",\"eq4\", \"eq7\", \"eq8\", \"eq21\")]),\n      rowSums(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n      ), ncol = 3\n    )\n  )\n) \n\ntrying to create a proper table…\n\npsych::fa(eq_processed, nfactors = 3, rotate=\"oblimin\")\n\nLoading required namespace: GPArotation\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : I\nam sorry, to do these rotations requires the GPArotation package to be\ninstalled\n\n\nFactor Analysis using method =  minres\nCall: psych::fa(r = eq_processed, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n       MR1   MR2   MR3    h2   u2 com\neq1   0.53 -0.11  0.03 0.297 0.70 1.1\neq2   0.13  0.24  0.24 0.130 0.87 2.5\neq3   0.49  0.11 -0.20 0.287 0.71 1.4\neq4   0.08 -0.03  0.56 0.325 0.68 1.0\neq5   0.22  0.59  0.01 0.397 0.60 1.3\neq6   0.25  0.05 -0.13 0.085 0.92 1.6\neq7   0.15  0.26  0.24 0.145 0.85 2.6\neq8   0.31  0.13  0.16 0.141 0.86 1.9\neq9   0.28  0.38  0.24 0.282 0.72 2.6\neq10  0.26  0.05 -0.27 0.142 0.86 2.1\neq11  0.21 -0.42  0.11 0.237 0.76 1.6\neq12  0.38  0.23  0.45 0.398 0.60 2.5\neq13  0.57 -0.05 -0.06 0.330 0.67 1.0\neq14  0.61 -0.23 -0.05 0.430 0.57 1.3\neq15  0.43 -0.22  0.13 0.246 0.75 1.7\neq16  0.42  0.42 -0.03 0.347 0.65 2.0\neq17  0.20  0.29 -0.22 0.170 0.83 2.7\neq18  0.46  0.22  0.36 0.394 0.61 2.4\neq19  0.36 -0.12  0.04 0.150 0.85 1.3\neq20  0.26  0.55  0.15 0.390 0.61 1.6\neq21  0.12 -0.08  0.48 0.249 0.75 1.2\neq22  0.62  0.03 -0.27 0.454 0.55 1.4\neq23  0.10  0.11 -0.18 0.055 0.95 2.3\neq24  0.20 -0.18 -0.27 0.146 0.85 2.6\neq25  0.19  0.28 -0.42 0.286 0.71 2.2\neq26  0.42 -0.42  0.12 0.370 0.63 2.2\neq27  0.24 -0.06 -0.01 0.060 0.94 1.1\neq28  0.53  0.18 -0.31 0.413 0.59 1.9\neq29  0.54 -0.19 -0.08 0.337 0.66 1.3\neq30  0.16  0.40 -0.04 0.184 0.82 1.3\neq31  0.35  0.60  0.01 0.483 0.52 1.6\neq32  0.29  0.19  0.20 0.162 0.84 2.6\neq33  0.33  0.30  0.08 0.209 0.79 2.1\neq34  0.59 -0.22 -0.12 0.414 0.59 1.4\neq35  0.41 -0.25  0.23 0.283 0.72 2.3\neq36  0.52 -0.49  0.09 0.516 0.48 2.1\neq37 -0.06 -0.04  0.08 0.013 0.99 2.3\neq38  0.30 -0.54  0.08 0.386 0.61 1.6\neq39  0.47  0.00 -0.31 0.319 0.68 1.7\neq40  0.58 -0.28  0.00 0.419 0.58 1.4\n\n                       MR1  MR2  MR3\nSS loadings           5.70 3.36 2.02\nProportion Var        0.14 0.08 0.05\nCumulative Var        0.14 0.23 0.28\nProportion Explained  0.51 0.30 0.18\nCumulative Proportion 0.51 0.82 1.00\n\nMean item complexity =  1.8\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  780  with the objective function =  22.46 with Chi Square =  1209.17\ndf of  the model are 663  and the objective function was  14.34 \n\nThe root mean square of the residuals (RMSR) is  0.09 \nThe df corrected root mean square of the residuals is  0.1 \n\nThe harmonic n.obs is  69 with the empirical chi square  833.94  with prob <  6.5e-06 \nThe total n.obs was  69  with Likelihood Chi Square =  743.36  with prob <  0.016 \n\nTucker Lewis Index of factoring reliability =  0.754\nRMSEA index =  0.039  and the 90 % confidence intervals are  0.02 0.058\nBIC =  -2063.86\nFit based upon off diagonal values = 0.79\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3\nCorrelation of (regression) scores with factors   0.95 0.92 0.87\nMultiple R square of scores with factors          0.90 0.85 0.75\nMinimum correlation of possible factor scores     0.80 0.70 0.50\n\n# from https://www.anthonyschmidt.co/post/2020-09-27-efa-tables-in-r/\nflex <- function(data, title=NULL) {\n  # this grabs the data and converts it to a flextbale\n  flextable(data) %>%\n  # this makes the table fill the page width\n  set_table_properties(layout = \"autofit\", width = 1) %>%\n  # font size\n  fontsize(size=10, part=\"all\") %>%\n    #this adds a ttitlecreates an automatic table number\n      set_caption(title, \n                  autonum = officer::run_autonum(seq_id = \"tab\", \n                                                 pre_label = \"Table \", \n                                                 post_label = \"\\n\", \n                                                 bkm = \"anytable\")) %>%\n  # font type\n  font(fontname=\"Times New Roman\", part=\"all\")\n}\n\nfa_table <- function(x, cut) {\n  #get sorted loadings\n  loadings <- psych::fa.sort(x)$loadings %>% round(3)\n  #supress loadings\n  loadings[loadings < cut] <- \"\"\n  #get additional info\n  add_info <- cbind(x$communalities, \n                    x$uniquenesses,\n                    x$complexity) %>%\n    # make it a data frame\n    as.data.frame() %>%\n    # column names\n    rename(\"Communality\" = V1,\n           \"Uniqueness\" = V2,\n           \"Complexity\" = V3) %>%\n    #get the item names from the vector\n    rownames_to_column(\"item\")\n  #build table\n  loadings %>%\n    unclass() %>%\n    as.data.frame() %>%\n    rownames_to_column(\"item\") %>%\n    left_join(add_info) %>%\n    mutate(across(where(is.numeric), round, 3))\n}\n\nfa_table(\n  psych::fa(eq_processed, nfactors = 3, rotate=\"oblimin\"), \n  .5\n)\n\nLoading required namespace: GPArotation\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : I\nam sorry, to do these rotations requires the GPArotation package to be\ninstalled\n\n\nJoining with `by = join_by(item)`\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n   item   MR1   MR2   MR3 Communality Uniqueness Complexity\n1  eq22 0.617                   0.454      0.546      1.373\n2  eq14 0.613                   0.430      0.570      1.287\n3  eq34 0.591                   0.414      0.586      1.371\n4  eq40 0.584                   0.419      0.581      1.434\n5  eq13 0.569                   0.330      0.670      1.036\n6  eq29 0.544                   0.337      0.663      1.280\n7  eq28 0.535                   0.413      0.587      1.857\n8   eq1 0.532                   0.297      0.703      1.099\n9  eq36 0.515                   0.516      0.484      2.063\n10  eq3                         0.287      0.713      1.437\n11 eq39                         0.319      0.681      1.748\n12 eq18                         0.394      0.606      2.360\n13 eq15                         0.246      0.754      1.679\n14 eq26                         0.370      0.630      2.167\n15 eq16                         0.347      0.653      2.009\n16 eq35                         0.283      0.717      2.294\n17 eq19                         0.150      0.850      1.261\n18 eq33                         0.209      0.791      2.105\n19  eq8                         0.141      0.859      1.885\n20 eq32                         0.162      0.838      2.589\n21  eq6                         0.085      0.915      1.623\n22 eq27                         0.060      0.940      1.117\n23 eq31       0.602             0.483      0.517      1.598\n24  eq5       0.589             0.397      0.603      1.285\n25 eq20       0.548             0.390      0.610      1.602\n26 eq38                         0.386      0.614      1.612\n27 eq11                         0.237      0.763      1.637\n28 eq30                         0.184      0.816      1.338\n29  eq9                         0.282      0.718      2.558\n30 eq17                         0.170      0.830      2.708\n31  eq7                         0.145      0.855      2.578\n32  eq4             0.564       0.325      0.675      1.041\n33 eq21                         0.249      0.751      1.176\n34 eq12                         0.398      0.602      2.493\n35 eq25                         0.286      0.714      2.195\n36 eq24                         0.146      0.854      2.639\n37 eq10                         0.142      0.858      2.085\n38  eq2                         0.130      0.870      2.510\n39 eq23                         0.055      0.945      2.305\n40 eq37                         0.013      0.987      2.339\n\n\n\ncog_eq_sum = (\n  eq_processed$eq14 +\n  eq_processed$eq15 +\n  eq_processed$eq29 +\n  eq_processed$eq34 +\n  eq_processed$eq35\n)\n\ncog_eq_mean = (\n  eq_processed$eq14 +\n  eq_processed$eq15 +\n  eq_processed$eq29 +\n  eq_processed$eq34 +\n  eq_processed$eq35\n)/5\n\n\nlm(eq14 ~ cog_eq_sum, eq_processed)\n\n\nCall:\nlm(formula = eq14 ~ cog_eq_sum, data = eq_processed)\n\nCoefficients:\n(Intercept)   cog_eq_sum  \n    -0.3124       0.2341  \n\nlm(eq14 ~ cog_eq_mean, eq_processed)\n\n\nCall:\nlm(formula = eq14 ~ cog_eq_mean, data = eq_processed)\n\nCoefficients:\n(Intercept)  cog_eq_mean  \n    -0.3124       1.1705  \n\nbop<-lm(cog_eq_mean ~ eq14 + eq15 + eq29 + eq34 + eq35, eq_processed)\n\nbop$coefficients \n\n(Intercept)        eq14        eq15        eq29        eq34        eq35 \n-1.1501e-15  2.0000e-01  2.0000e-01  2.0000e-01  2.0000e-01  2.0000e-01 \n\ncolnames(eq_subscales_r) <- c( \"Cognitive\", \"Social\", \"Emotion\")\n\neq_subscales_r %>% mutate_all(~cell_spec(\n        .x, \n        color = ifelse(.x < .4, \"black\", \"white\"),\n        background = ifelse(.x > .4, \"red\",\" white\"))) %>%\n    kable(escape = F) %>%\n    kable_styling()\n\n\n\n \n  \n      \n    Cognitive \n    Social \n    Emotion \n  \n \n\n  \n    eq14 \n    0.730729714723979 \n    0.110723735489975 \n    0.31320589104921 \n  \n  \n    eq15 \n    0.60185382395657 \n    0.198874708000659 \n    0.137720182774352 \n  \n  \n    eq29 \n    0.659975490377257 \n    0.0278299656284012 \n    0.364451955577018 \n  \n  \n    eq34 \n    0.679619374819311 \n    0.0226190168052008 \n    0.395730784538071 \n  \n  \n    eq35 \n    0.587492289787915 \n    0.122610871425514 \n    0.242014658835711 \n  \n  \n    eq2 \n    -0.00813200484463844 \n    0.576298173261772 \n    0.133855862092061 \n  \n  \n    eq4 \n    0.0479274896411267 \n    0.662778025977272 \n    0.123591773099612 \n  \n  \n    eq7 \n    -0.0312109379222552 \n    0.438752142983534 \n    0.234889555117118 \n  \n  \n    eq8 \n    0.226816177181639 \n    0.427950397740909 \n    0.134334810841783 \n  \n  \n    eq21 \n    0.161873895295539 \n    0.627680352407105 \n    -0.0206828796782751 \n  \n  \n    eq3 \n    0.29722368237889 \n    -0.00177049925158063 \n    0.62739169422544 \n  \n  \n    eq16 \n    0.268239998089082 \n    0.168294312535106 \n    0.592732439474545 \n  \n  \n    eq19 \n    0.35758009314024 \n    0.183400506448371 \n    0.552243103025108 \n  \n  \n    eq33 \n    0.159850007854409 \n    0.302368129555882 \n    0.665487951247779 \n  \n  \n    eq39 \n    0.349069094803534 \n    0.0110077011019322 \n    0.690638221783324 \n  \n\n\n\n\n\nThis is quite dense, but in broad terms, we want to cluster items that correlate with each other into one component (AKA factor AKA subscale).\nIf we use a package in R we can start identifying the top 3 components and check if the questions map on to what we would expect for each of the three subscales:\n\neq_pca <- prcomp(eq_processed)\nsort(abs(eq_pca$rotation[,1]),decreasing = T)[1:5]\n\n     eq16      eq20      eq22      eq18      eq13 \n0.2752500 0.2436235 0.2428536 0.2329153 0.2315572 \n\nsort(eq_pca$rotation[,1],decreasing = T)[1:5]\n\n       eq37        eq11        eq21         eq4        eq38 \n 0.03183132 -0.02479062 -0.03616440 -0.04118648 -0.04219994 \n\n\nComponent 1 involves questions 20, 33, 31, 16 and 5. These items are: - 20: I am very blunt, which some people take to be rudeness, even though this is unintentional. - 33: I usually stay emotionally detached when watching a film. - 31: Other people often say that I am insensitive, though I don’t always see why. - 16: If I say something that someone else is offended by, I think that that’s their problem, not mine. - 5: People often tell me that I went too far in driving my point home in a discussion."
  },
  {
    "objectID": "itemAnalyses/cfa.html#learning-from-httpsuc-r.github.iopca",
    "href": "itemAnalyses/cfa.html#learning-from-httpsuc-r.github.iopca",
    "title": "Confirmatory Factor Analysis",
    "section": "learning from https://uc-r.github.io/pca",
    "text": "learning from https://uc-r.github.io/pca\n\nlibrary(tidyverse)  # data manipulation and visualization\nlibrary(gridExtra)  # plot arrangement\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndata(\"USArrests\")\nhead(USArrests, 10)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58 21.2\nAlaska        10.0     263       48 44.5\nArizona        8.1     294       80 31.0\nArkansas       8.8     190       50 19.5\nCalifornia     9.0     276       91 40.6\nColorado       7.9     204       78 38.7\nConnecticut    3.3     110       77 11.1\nDelaware       5.9     238       72 15.8\nFlorida       15.4     335       80 31.9\nGeorgia       17.4     211       60 25.8\n\nscaled_df <- apply(USArrests, 2, scale)\n\narrests.cov <- cov(scaled_df)\narrests.eigen <- eigen(arrests.cov)\n\narrests.cov\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\ntemp_1 <- lm(scaled_df[,1] ~ scaled_df[,2])\ncov(temp_1$residuals, scaled_df[,1])\n\n[1] 0.3569992\n\ncov(temp_1$residuals, scaled_df[,2])\n\n[1] 5.328079e-17\n\nstr(arrests.eigen)\n\nList of 2\n $ values : num [1:4] 2.48 0.99 0.357 0.173\n $ vectors: num [1:4, 1:4] 0.536 0.583 0.278 0.543 0.418 ...\n - attr(*, \"class\")= chr \"eigen\"\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\narrests.eigen$values\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\narrests.eigen$vectors\n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\n(beep <- arrests.eigen$vectors[,1:2])\n\n          [,1]       [,2]\n[1,] 0.5358995  0.4181809\n[2,] 0.5831836  0.1879856\n[3,] 0.2781909 -0.8728062\n[4,] 0.5434321 -0.1673186\n\nbeep <- prcomp(scaled_df)\nbeep$rotation <- beep$rotation * -1\nbeep$rotation <- beep$x * -1\n\nbiplot(beep, scale = 0)\n\n\n\n\nTo start"
  },
  {
    "objectID": "itemAnalyses/pca.html",
    "href": "itemAnalyses/pca.html",
    "title": "Principle Component Analysis (R)",
    "section": "",
    "text": "Questionnaires and surveys often have items that reflect multiple sub-scales or multiple related constructs, in a single measure. For example, the empathy quotient (Lawrence et al. 2004) has multiple types of empathy within it:\n\ncognitive empathy\nemotional reactivity\nsocial skills\n\nLet’s use one publicly available data set to check if we can replicate these subscales:\nhttps://github.com/bhismalab/EyeTracking_PlosOne_2017/blob/master/EQ_Data.csv\n\neq <- read.csv(\"EQ_Data.csv\")\n# focusing on q1.raw to q40.raw\neq_processed <- eq[,c(paste(\"Q\",1:40,\".processed\",sep=\"\"))]\nrmarkdown::paged_table(eq_processed)\n\n\n\n  \n\n\ncolnames(eq_processed) <- paste(\"eq\",1:40, sep=\"\")\n\n\nrmarkdown::paged_table(data.frame(cor(eq_processed)))"
  },
  {
    "objectID": "itemAnalyses/pca.html#confirmatory-factor-analysis",
    "href": "itemAnalyses/pca.html#confirmatory-factor-analysis",
    "title": "Principle Component Analysis (R)",
    "section": "Confirmatory factor analysis",
    "text": "Confirmatory factor analysis\nWe already have three subscales for the empathy quotient, so let’s confirm whether our current data is consistent with them. The subscales are:\n\n\n\n\n\n\nSome of the items below are reverse scored\n\n\n\nThe items that would go against the component, e.g. “I find it hard to know what to do in a social situation” if the participant agreed are reverse scored, meaning that strongly disagreeing increases their score for the relevant subscale (in this case “social skills”).\n\n\nCognitive:\n\n14: I am good at predicting how someone will feel.\n15: I am quick to spot when someone in a group is feeling awkward or uncomfortable.\n29: I can sense if I am intruding, even if the other person doesn’t tell me.\n34: I can tune into how someone else feels rapidly and intuitively.\n35: I can easily work out what another person might want to talk about.\n\nSocial Skills:\n\n2: I find it difficult to explain to others things that I understand easily, when they don’t understand it first time.\n4: I find it hard to know what to do in a social situation.\n7: Friendships and relationships are just too difficult, so I tend not to bother with them.\n8: I often find it difficult to judge if something is rude or polite.\n21: I don’t tend to find social situations confusing.\n\nEmotion:\n\n3: I really enjoy caring for other people.\n16: If I say something that someone else is offended by, I think that that’s their problem, not mine.\n19: Seeing people cry doesn’t really upset me.\n33: I usually stay emotionally detached when watching a film.\n39: I tend to get emotionally involved with a friend’s problems.\n\nLet’s start by looking at correlation matrices to see if the items tend to correlate with each other in the current data set:\n\nCognitive empathy\n\ncor(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")])\n\n          eq14       eq15      eq29      eq34       eq35\neq14 1.0000000 0.34501695 0.2905797 0.4416185 0.27504384\neq15 0.3450170 1.00000000 0.3721252 0.2532783 0.07192026\neq29 0.2905797 0.37212520 1.0000000 0.2468024 0.28854264\neq34 0.4416185 0.25327834 0.2468024 1.0000000 0.23509011\neq35 0.2750438 0.07192026 0.2885426 0.2350901 1.00000000\n\n\nSo far looking good, as everything positively correlates - mostly with r-values greater than .25. Let’s now see how well each item loads onto the total of these scores:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\neq_subscales_r <- data.frame(\n  cor(\n    eq_processed[,c(\n      # Cog\n      \"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\",\n      \n      # Social Skills\n      \"eq2\",\"eq4\",\"eq7\",\"eq8\",\"eq21\",\n      \n      # Emotion\n      \"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\"\n      )],\n    matrix(data = c(\n      rowSums(eq_processed[,c(\"eq14\",\"eq15\",\"eq29\",\"eq34\",\"eq35\")]),\n      rowSums(eq_processed[,c(\"eq2\",\"eq4\", \"eq7\", \"eq8\", \"eq21\")]),\n      rowSums(eq_processed[,c(\"eq3\",\"eq16\",\"eq19\",\"eq33\",\"eq39\")])\n      ), ncol = 3\n    )\n  )\n) \n\ncolnames(eq_subscales_r) <- c( \"Cognitive\", \"Social\", \"Emotion\")\n\neq_subscales_r %>% mutate_all(~cell_spec(\n        .x, \n        color = ifelse(.x < .4, \"black\", \"white\"),\n        background = ifelse(.x > .4, \"red\",\" white\"))) %>%\n    kable(escape = F) %>%\n    kable_styling()\n\n\n\n \n  \n      \n    Cognitive \n    Social \n    Emotion \n  \n \n\n  \n    eq14 \n    0.730729714723979 \n    0.110723735489975 \n    0.31320589104921 \n  \n  \n    eq15 \n    0.60185382395657 \n    0.198874708000659 \n    0.137720182774352 \n  \n  \n    eq29 \n    0.659975490377257 \n    0.0278299656284012 \n    0.364451955577018 \n  \n  \n    eq34 \n    0.679619374819311 \n    0.0226190168052008 \n    0.395730784538071 \n  \n  \n    eq35 \n    0.587492289787915 \n    0.122610871425514 \n    0.242014658835711 \n  \n  \n    eq2 \n    -0.00813200484463844 \n    0.576298173261772 \n    0.133855862092061 \n  \n  \n    eq4 \n    0.0479274896411267 \n    0.662778025977272 \n    0.123591773099612 \n  \n  \n    eq7 \n    -0.0312109379222552 \n    0.438752142983534 \n    0.234889555117118 \n  \n  \n    eq8 \n    0.226816177181639 \n    0.427950397740909 \n    0.134334810841783 \n  \n  \n    eq21 \n    0.161873895295539 \n    0.627680352407105 \n    -0.0206828796782751 \n  \n  \n    eq3 \n    0.29722368237889 \n    -0.00177049925158063 \n    0.62739169422544 \n  \n  \n    eq16 \n    0.268239998089082 \n    0.168294312535106 \n    0.592732439474545 \n  \n  \n    eq19 \n    0.35758009314024 \n    0.183400506448371 \n    0.552243103025108 \n  \n  \n    eq33 \n    0.159850007854409 \n    0.302368129555882 \n    0.665487951247779 \n  \n  \n    eq39 \n    0.349069094803534 \n    0.0110077011019322 \n    0.690638221783324 \n  \n\n\n\n\n\nThis is quite dense, but in broad terms, we want to cluster items that correlate with each other into one component (AKA factor AKA subscale).\nIf we use a package in R we can start identifying the top 3 components and check if the questions map on to what we would expect for each of the three subscales:\n\neq_pca <- prcomp(eq_processed)\nsort(abs(eq_pca$rotation[,1]),decreasing = T)[1:5]\n\n     eq16      eq20      eq22      eq18      eq13 \n0.2752500 0.2436235 0.2428536 0.2329153 0.2315572 \n\nsort(eq_pca$rotation[,1],decreasing = T)[1:5]\n\n       eq37        eq11        eq21         eq4        eq38 \n 0.03183132 -0.02479062 -0.03616440 -0.04118648 -0.04219994 \n\n\nComponent 1 involves questions 20, 33, 31, 16 and 5. These items are: - 20: I am very blunt, which some people take to be rudeness, even though this is unintentional. - 33: I usually stay emotionally detached when watching a film. - 31: Other people often say that I am insensitive, though I don’t always see why. - 16: If I say something that someone else is offended by, I think that that’s their problem, not mine. - 5: People often tell me that I went too far in driving my point home in a discussion."
  },
  {
    "objectID": "itemAnalyses/pca.html#learning-from-httpsuc-r.github.iopca",
    "href": "itemAnalyses/pca.html#learning-from-httpsuc-r.github.iopca",
    "title": "Principle Component Analysis (R)",
    "section": "learning from https://uc-r.github.io/pca",
    "text": "learning from https://uc-r.github.io/pca\n\nlibrary(tidyverse)  # data manipulation and visualization\nlibrary(gridExtra)  # plot arrangement\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndata(\"USArrests\")\nhead(USArrests, 10)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58 21.2\nAlaska        10.0     263       48 44.5\nArizona        8.1     294       80 31.0\nArkansas       8.8     190       50 19.5\nCalifornia     9.0     276       91 40.6\nColorado       7.9     204       78 38.7\nConnecticut    3.3     110       77 11.1\nDelaware       5.9     238       72 15.8\nFlorida       15.4     335       80 31.9\nGeorgia       17.4     211       60 25.8\n\nscaled_df <- apply(USArrests, 2, scale)\n\narrests.cov <- cov(scaled_df)\narrests.eigen <- eigen(arrests.cov)\n\narrests.cov\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\ntemp_1 <- lm(scaled_df[,1] ~ scaled_df[,2])\ncov(temp_1$residuals, scaled_df[,1])\n\n[1] 0.3569992\n\ncov(temp_1$residuals, scaled_df[,2])\n\n[1] 5.328079e-17\n\nstr(arrests.eigen)\n\nList of 2\n $ values : num [1:4] 2.48 0.99 0.357 0.173\n $ vectors: num [1:4, 1:4] 0.536 0.583 0.278 0.543 0.418 ...\n - attr(*, \"class\")= chr \"eigen\"\n\narrests.eigen\n\neigen() decomposition\n$values\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n$vectors\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\narrests.eigen$values\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\narrests.eigen$vectors\n\n          [,1]       [,2]       [,3]        [,4]\n[1,] 0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] 0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] 0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] 0.5434321 -0.1673186  0.8177779  0.08902432\n\n(beep <- arrests.eigen$vectors[,1:2])\n\n          [,1]       [,2]\n[1,] 0.5358995  0.4181809\n[2,] 0.5831836  0.1879856\n[3,] 0.2781909 -0.8728062\n[4,] 0.5434321 -0.1673186\n\nbeep <- prcomp(scaled_df)\nbeep$rotation <- beep$rotation * -1\nbeep$rotation <- beep$x * -1\n\nbiplot(beep, scale = 0)\n\n\n\n\nTo start"
  },
  {
    "objectID": "itemAnalyses/cronbachAlphaQuestions.html",
    "href": "itemAnalyses/cronbachAlphaQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nCronbach’s Alpha is useful to check whether items in a measure are…\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbach’s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdr.html",
    "href": "multipleTesting/fwer_vs_fdr.html",
    "title": "FWER, FDR, Positive and Negative effects(R)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdr.html#false-discovery-rate-and-positive-vs.-negative-effects",
    "href": "multipleTesting/fwer_vs_fdr.html#false-discovery-rate-and-positive-vs.-negative-effects",
    "title": "FWER, FDR, Positive and Negative effects(R)",
    "section": "False Discovery Rate and Positive vs. Negative effects",
    "text": "False Discovery Rate and Positive vs. Negative effects\nThe false discovery rate (FDR) is the (expected) proportion of false positives out of true and false positives:\n\\[\nFDR = \\frac{\\color{red}{FalsePositives}}{\\color{red}{FalsePositives} + \\color{green}{TruePositives}}\n\\]\nEstimating this consistently is tricky because \\(\\color{red}{FalsePositives}\\) should be negative effects (hence written in red), as they reflect negative effects in the population that have incorrectly been identified as positive.\nThe fact that all false positives are negatives can be visualised as follows:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(eulerr)\nset.seed(6)\nneg_vd <- list(\n\"Positive\" = 301:1000,\n      \"False Negative\" = 301:335,\n      \"Negative\" = 1:300,\n      \"False Positive\" = 1:15\n)\n\nplot(euler(neg_vd, shape = \"ellipse\"),  \n  fills = list(\n    fill = c(\n      \"dark green\", # Positive\n      \"purple\",      # False Negative\n      \"light blue\",  # Negative\n      \"red\"          # False positive\n    ),\n    alpha = 0.5\n  )\n)\n\n\n\n\nFigure 1: A venn diagram showing how false-negatives are a subset of positive effects in the population, and false-positives are a subset of negatives in the population.\n\n\n\n\nThe above illustration also illustrates that the ratio between positive and negative effects in the population should change your false discovery rate. To illustrate this, let’s create a table of how many false positives and true positives we would expect based on different proportions of positive effects in the population, and what the impact is on the FDR.\n\nfdr_df <- data.frame(\n  pop_pos  = c(700,500,300),\n  pop_neg  = c(300,500,700),\n  power    = c(.95,.95,.95),\n  alpha    = c(.05,.05,.05)\n)\nfdr_df$true_pos  = fdr_df$pop_pos * fdr_df$power\nfdr_df$true_neg  = fdr_df$pop_neg * (1- fdr_df$alpha)\nfdr_df$false_pos = fdr_df$pop_neg * fdr_df$alpha\nfdr_df$false_neg = fdr_df$pop_pos * (1-fdr_df$power)\nfdr_df$FDR       = fdr_df$false_pos/(fdr_df$false_pos + fdr_df$true_pos)\nknitr::kable(fdr_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npop_pos\npop_neg\npower\nalpha\ntrue_pos\ntrue_neg\nfalse_pos\nfalse_neg\nFDR\n\n\n\n\n700\n300\n0.95\n0.05\n665\n285\n15\n35\n0.0220588\n\n\n500\n500\n0.95\n0.05\n475\n475\n25\n25\n0.0500000\n\n\n300\n700\n0.95\n0.05\n285\n665\n35\n15\n0.1093750\n\n\n\n\n\nThe false discovery rate is highest when only 20% of the effects in the population are positive, and lowest when 70% of the effects in the population are positive. Let’s visualise this with some Venn Diagrams:\n\n\n\n\n\n\nExpand to see code for the below Venn diagrams\n\n\n\n\n\n\nlibrary(eulerr)\nset.seed(7)\n\n# Overview\npos_neg_1_plot <- plot(\n  euler(\n    pos_neg_1_vd <- list(\n      \"700\" = 301:1000,\n      \"35\" = 301:335,\n      \"300\" = 1:300,\n      \"15\" = 1:15\n    ),\n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"dark green\", # Positive\n      \"purple\",      # False Negative\n      \"light blue\",  # Negative\n      \"red\"          # False positive\n    ),\n    alpha = 0.5\n  ), \n)\n\n# FDR comparison\nfdr_1_plot <- plot(\n  euler(\n    list(\n      \"True Positives\" = 336:1000,\n      \"False Positives\" = 1:15\n    ), \n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"green\", # Positive\n      \"red\"          # False Positive\n    ),\n    alpha = 0.5\n  ), \n  labels = c(\n    665,\n    15\n  )\n)\n\n# 500 positives vs. 500 negatives\n# Overview\npos_neg_2_plot <- plot(\n  euler(\n    list(\n      \"500 \" = 501:1000,\n      \"25 \" = 501:525,\n      \"500\" = 1:500,\n      \"25\" = 1:25\n    ), \n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"dark green\", # Positive\n      \"purple\",      # False Negative\n      \"light blue\",  # Negative\n      \"red\"          # False positive\n    ),\n    alpha = 0.5\n  )\n)\n\n#FDR\nfdr_2_plot <- plot(\n  euler(\n    list(\n      \"True Positives\" = 526:1000,\n      \"False Positives\" = 1:25\n    ), \n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"green\", # Positive\n      \"red\"          # False Positive\n    ),\n    alpha = 0.5\n  ), \n  labels = c(\n    475,\n    25\n  )\n) \n\n\n# 200 positives vs. 800 negatives\npos_neg_3_plot <-plot(\n  euler(\n    list(\n      \"300\" = 701:1000,\n      \"15\" = 701:715,\n      \"700\" = 1:700,\n      \"35\" = 1:35\n    ), \n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"dark green\", # Positive\n      \"purple\",      # False Negative\n      \"light blue\",  # Negative\n      \"red\"          # False positive\n    ),\n    alpha = 0.5\n  )\n)\n\nfdr_3_plot <- plot(\n  euler(\n    list(\n      \"Positives\" = 716:1000,\n      \"False Positives\" = 1:35\n    ), \n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"green\", # Positive\n      \"red\"          # False Positive\n    ),\n    alpha = 0.5\n  ), \n  labels = c(\n    285,\n    35\n  )\n)\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nggarrange(\n  pos_neg_1_plot,\n  fdr_1_plot,\n  pos_neg_2_plot,\n  fdr_2_plot,\n  pos_neg_3_plot,\n  fdr_3_plot,\n  ncol = 2,\n  nrow = 3,\n  labels = c(\"Sample\",\"FDR\")\n)\n\n\n\n\nFigure 2: Venn diagrams showing how the ratio of Positives to Negatives in the population will impact the FDR even if you keep consistent thresholds for false positives (alpha = .05) and maintain a power value of .95\n\n\n\n\nIn the above figure green represents positives (bright green represents true positives), blue represents negatives, purple reflects false negatives and red reflects false positives. Consistent with the table above, the ratio between false positives and true positives shifts depending on how many positives and negatives there are in the population, thus changing the FDR."
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdr.html#family-wise-error-rate-and-positive-vs.-negative-effects",
    "href": "multipleTesting/fwer_vs_fdr.html#family-wise-error-rate-and-positive-vs.-negative-effects",
    "title": "FWER, FDR, Positive and Negative effects(R)",
    "section": "Family-Wise Error Rate and Positive vs. Negative effects",
    "text": "Family-Wise Error Rate and Positive vs. Negative effects\nThe family-wise error rate (FWER) is the likelihood that at least one of your negative effects has been incorrectly accepted and thus is false-positive. Unlike FDR, there is no comparison between effects that are positive or negative in the population, and so the ratio between positive and negative effects in the population should have no impact on the FWER. The simulations below will help assess if this is true."
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdr.html#simulations-of-fdr-with-different-positive-rates-in-the-population",
    "href": "multipleTesting/fwer_vs_fdr.html#simulations-of-fdr-with-different-positive-rates-in-the-population",
    "title": "FWER, FDR, Positive and Negative effects(R)",
    "section": "Simulations of FDR with different positive rates in the population",
    "text": "Simulations of FDR with different positive rates in the population\nWe will now run some simulations to generate expected FDR outputs using one-sample t-tests. The parameters will be as follows:\n\neffect size is 1 (Cohen’s d)\n\\(\\alpha\\) is .05\nPower (\\(1-\\beta\\)) is .95\n\nA Benjamini-Hochberg procedure will be applied to try to attain the FDR required for each simulation:\n\n\n\n\n\n\nExpand to see calculation for this simulation\n\n\n\n\n\n\noptions(scipen = 999)\nlibrary(tidyverse)\n\n# calculate how many participants are needed\nthis_effect_size = 1\nsim_pwr   = pwr::pwr.t.test(\n  d = this_effect_size, \n  sig.level = .05, \n  power = .95\n)\nthis_pp   = round(sim_pwr$n)\nq_values = c(.01,.025,.05,.075,.1)\nn_tests  = 100\n\n# preparing data frame for all the simulations\nfdr_sim_df <- data.frame(\n  q = c(\n    rep(q_values, each = n_tests*5)\n  ),\n  positive_prop = rep(c(1,3,5,7,9), n_tests * length(q_values))/10,\n  pop_pos = NA,\n  pop_neg = NA,\n  samp_pos = NA,\n  samp_neg = NA,\n  fdr = NA\n)\n\ntests_per_sim = 100\n  \nfor(i in 1:length(fdr_sim_df$q)){\n  # prepare a data frame to compare positive and negatives within a sample compared to the population\n  this_subset = data.frame(\n    population = rep(\"\",tests_per_sim),\n    sample     = \"\",\n    true_false = FALSE,\n    p.value    = NA\n  )\n  \n  this_pos_prop = fdr_sim_df$positive_prop[i]\n  for(k in 1:tests_per_sim){\n    # if the population is positive\n    if(k/tests_per_sim <= this_pos_prop){\n      this_subset$population[k] = \"positive\"\n      this_t_test <- t.test(\n        rnorm(\n          this_pp, \n          mean = this_effect_size, \n          sd = 1\n        ),\n        mu = 0\n      )\n      this_subset$p.value[k] <- this_t_test$p.value\n    }\n    \n    #if the population is negative\n    if(k/tests_per_sim > this_pos_prop){\n      this_subset$population[k] = \"negative\"\n      this_subset$p.value[k] = runif(1,min = 0,max = 1)\n    }\n  }\n  # sort the subset into appropriate ranks\n  this_subset %>% \n    arrange(p.value) -> sorted_subset\n  sorted_subset$rank = rank(sorted_subset$p.value)\n\n  sorted_subset$alpha = (sorted_subset$rank/tests_per_sim) * fdr_sim_df$q[i]\n  sorted_subset$sig   = sorted_subset$p.value < sorted_subset$alpha\n  max_sig = max(sorted_subset$rank[sorted_subset$sig])\n  sorted_subset$sample = \"negative\"\n  sorted_subset$hb_sample = \"negative\"\n  \n  fdr_sim_df$bh_max_alpha[i] = sorted_subset$alpha[max_sig]\n  sorted_subset$sample[1:max_sig] = \"positive\"\n  fdr_sim_df$true_pos[i]  = sum(sorted_subset$population == \"positive\" & sorted_subset$sample == \"positive\")\n  fdr_sim_df$true_neg[i]  = sum(sorted_subset$population == \"negative\" & sorted_subset$sample == \"negative\")\n  fdr_sim_df$false_pos[i] = sum(sorted_subset$population == \"negative\" & sorted_subset$sample == \"positive\")\n  fdr_sim_df$false_neg[i] = sum(sorted_subset$population == \"positive\" & sorted_subset$sample == \"negative\")\n  \n}\nfdr_sim_df$fdr = fdr_sim_df$false_pos/(fdr_sim_df$false_pos + fdr_sim_df$true_pos)\n\n\n\n\n\nggplot(fdr_sim_df, aes(x = q, y = fdr,color=as.factor(positive_prop))) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  xlab(\"Researcher estimated FDR\") +\n  ylab(\"Actual FDR\") + \n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1, color=\"Expected FDR\")) +\n  geom_jitter(width = .0025, height = 0.001) +\n  coord_cartesian(\n    xlim = (c(0,.1)),\n    ylim = (c(0,.4))\n  ) +\n  labs(color=\"Proportion Positive\")\n\n\n\n\nFigure 3: A visualisation of simulations of what false discovery rates achieved depending on the proportion of Positive effects in the population.\n\n\n\n\nThe above simulations show an interaction between the false discovery rate a researcher sets (known as \\(q\\)) and the actual false discovery rate depending on the proportion of positive and negative findings in the population. Whilst it might seem problematic that the underlying number of positives and negatives in the population impacts the actual false discovery rate, the good news is that even if only 10% of your tests should be positive (Proportion Positive = .1) based on the population, the Benjamini-Hochberg will on average keep the FDR below the researcher specified FDR.\n\n\n\n\n\n\nOptional: expand to see likelihood of a negative becoming a false-positive\n\n\n\n\n\nLet’s now visualise what the expected proportion of false-positives is compared to all negatives (both false-positives and true negatives):\n\nggplot(fdr_sim_df, aes(x = q, y = false_pos/(false_pos + true_neg),color=as.factor(positive_prop))) + \n  geom_point() + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  xlab(\"Researcher estimated FDR\") +\n  ylab(\"False Positives/(False Positives + True Negatives)\") + \n  #geom_segment(aes(x = 0, y = 0.05, xend = 1, yend = .05, color=\"Target FWER \\nof .05\")) +\n  geom_jitter(width = .0025, height = 0.001) +\n  coord_cartesian(\n    xlim = (c(0,.1)),\n    ylim = (c(0,.5))\n  ) +\n  labs(color=\"Proportion Positive\\nin the Population\")\n\n\n\n\nFigure 4: A visualisation of simulations that show the effect of the proportion of positive effects in the population on what proportion of negative findings that will become false-positives.\n\n\n\n\nThe above figure is not very intuitive, as it suggests that a higer proportion of negatives will be incorrectly identified as false-positives when there are more positives (in the population). However, the higher the ratio of positives to negatives in the population, the higher we should expect the highest alpha threshold to be when you use a procedural correction process like Benjamini-Hochberg (or Holm-Bonferroni) procedures. This is because the maximum alpha threshold increases the more tests that pass, and there should be more tests that pass if there are more positives that are being tested. The problem with this, is that this inflated maximum alpha allows more false-positives to sneak in. Let’s illustrate this with the following examples.\nLet’s imagine that you do 5 tests, 4 on positives, 1 on a negative.\n\nlibrary(kableExtra)\n\n\nexample_1_table <- data.frame(\n  population = c(\n    \"Positive\",\n    \"Positive\",\n    \"Positive\",\n    \"Negative\",\n    \"Positive\"\n  ),\n  rank = c(\n    1:5\n  ),\n  p.value = c(\n    c(.001,.015,.028,.041,.049)\n  )\n)\n\nexample_1_table$alpha = (example_1_table$rank/5) * .05\n\nexample_1_table %>% \n  kable(booktabs = T) %>%\n  kable_styling() %>%\n  row_spec(\n    which(example_1_table$p.value < .05), \n    bold = T,\n    color = \"white\",\n    background = \"blue\"\n  )\n\n\n\n \n  \n    population \n    rank \n    p.value \n    alpha \n  \n \n\n  \n    Positive \n    1 \n    0.001 \n    0.01 \n  \n  \n    Positive \n    2 \n    0.015 \n    0.02 \n  \n  \n    Positive \n    3 \n    0.028 \n    0.03 \n  \n  \n    Negative \n    4 \n    0.041 \n    0.04 \n  \n  \n    Positive \n    5 \n    0.049 \n    0.05 \n  \n\n\n\n\n\nThe Benjamini-Hochberg procedure means that we would accept all of the above findings, because the final row’s p-value is below it’s alpha, meaning that we accept all findings above it. This is a shame as the fourth row of them is a false positive (the population is negative even though the sample is now positive). This alpha threshold of .05 won’t occur in the next example where there are 2 Negatives despite the data being very similar:\n\nexample_2_table <- data.frame(\n  population = c(\n    \"Positive\",\n    \"Positive\",\n    \"Positive\",\n    \"Negative\",\n    \"Negative\"\n  ),\n  rank = c(\n    1:5\n  ),\n  p.value = c(\n    c(.001,.015,.028,.041,.5)\n  )\n)\n\nexample_2_table$alpha = (example_2_table$rank/5) * .05\n\nexample_2_table %>% \n  kable(booktabs = T) %>%\n  kable_styling() %>%\n  row_spec(\n    which(example_2_table$p.value < .03), \n    bold = T,\n    color = \"white\",\n    background = \"blue\"\n  )\n\n\n\n \n  \n    population \n    rank \n    p.value \n    alpha \n  \n \n\n  \n    Positive \n    1 \n    0.001 \n    0.01 \n  \n  \n    Positive \n    2 \n    0.015 \n    0.02 \n  \n  \n    Positive \n    3 \n    0.028 \n    0.03 \n  \n  \n    Negative \n    4 \n    0.041 \n    0.04 \n  \n  \n    Negative \n    5 \n    0.500 \n    0.05 \n  \n\n\n\n\n\nIn the above table we would only accepted the first three tests as there is no test after that in which the p-value is below the alpha value. This difference between the two above tables will not always happen, but as negative tests should give higher p-values (as their results are random) than positive tests (as the sample should hopefully represent the positive effect in the population), the maximum accepted p-value will generally be lower when there are fewer positive effects being tested.\nAs you can see above, this inflation of false positives is present in the Benjamini-Hochberg procedure, but also can emerge with other procedures like Holm-Bonferroni, for example:\nLet’s imagine that you do 5 tests, 4 on positives, 1 on a negative.\n\nexample_3_table <- data.frame(\n  population = c(\n    \"Positive\",\n    \"Positive\",\n    \"Positive\",\n    \"Positive\",\n    \"Negative\"\n  ),\n  rank = c(\n    1:5\n  ),\n  p.value = c(\n    c(.001,.01,.015,.024,.049)\n  )\n)\n\nexample_3_table$alpha = .05/(5 + 1 - example_3_table$rank)\n\nexample_3_table %>% \n  kable(booktabs = T) %>%\n  kable_styling() %>%\n  row_spec(\n    which(example_3_table$p.value < .05), \n    bold = T,\n    color = \"white\",\n    background = \"blue\"\n  )\n\n\n\n \n  \n    population \n    rank \n    p.value \n    alpha \n  \n \n\n  \n    Positive \n    1 \n    0.001 \n    0.0100000 \n  \n  \n    Positive \n    2 \n    0.010 \n    0.0125000 \n  \n  \n    Positive \n    3 \n    0.015 \n    0.0166667 \n  \n  \n    Positive \n    4 \n    0.024 \n    0.0250000 \n  \n  \n    Negative \n    5 \n    0.049 \n    0.0500000 \n  \n\n\n\n\n\nAll of the above tests narrowly passed the threshold, which is a problem as there is a test on a negative effect that we have accepted. However, we would be less likely to get this false positive if there had been another negative:\n\nexample_4_table <- data.frame(\n  population = c(\n    \"Positive\",\n    \"Positive\",\n    \"Positive\",\n    \"Negative\",\n    \"Negative\"\n  ),\n  rank = c(\n    1:5\n  ),\n  p.value = c(\n    c(.001,.01,.015,.049,.5)\n  )\n)\n\nexample_4_table$alpha = .05/(5 + 1 - example_4_table$rank)\n\nexample_4_table %>% \n  kable(booktabs = T) %>%\n  kable_styling() %>%\n  row_spec(\n    which(example_4_table$p.value < .01666), \n    bold = T,\n    color = \"white\",\n    background = \"blue\"\n  )\n\n\n\n \n  \n    population \n    rank \n    p.value \n    alpha \n  \n \n\n  \n    Positive \n    1 \n    0.001 \n    0.0100000 \n  \n  \n    Positive \n    2 \n    0.010 \n    0.0125000 \n  \n  \n    Positive \n    3 \n    0.015 \n    0.0166667 \n  \n  \n    Negative \n    4 \n    0.049 \n    0.0250000 \n  \n  \n    Negative \n    5 \n    0.500 \n    0.0500000 \n  \n\n\n\n\n\nThe effect that was previously accepted is no longer significant as the new negative effect replacing the old positive effect has a p-value of .5, and so is now the weakest rather than second weakest effect. As a result, the other negative with a .049 p-value is no longer compared to .05, but to .025, and is no longer significant.\nIs the positive association in false-positives with the proportion of positive effects a problem? If you are interested in minimising false-discovery rate then arguably not, as the increase of false-positives will be compensated by the increase in true positives (as you can see in the figures above)."
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdr.html#simulations-of-fwer-with-different-positive-rates-in-the-population",
    "href": "multipleTesting/fwer_vs_fdr.html#simulations-of-fwer-with-different-positive-rates-in-the-population",
    "title": "FWER, FDR, Positive and Negative effects(R)",
    "section": "Simulations of FWER with different positive rates in the population",
    "text": "Simulations of FWER with different positive rates in the population\nWe will now run some simulations to generate expected FWER outputs. The parameters will be the same as for FDR above:\n\neffect size is 1\n\\(\\alpha\\) is .05\nPower (\\(1-\\beta\\)) is .95\n\nA Holm-Šidák procedure will be applied to try to keep the FWER to .05:\n\n\n\n\n\n\nExpand to see calculation for this simulation\n\n\n\n\n\n\noptions(scipen = 999)\n\n# calculate how many participants are needed\nthis_effect_size = 1\nsim_pwr   = pwr::pwr.t.test(\n  d = this_effect_size, \n  sig.level = .05, \n  power = .95\n)\nthis_pp   = round(sim_pwr$n)\nn_tests  = 500\n\n# preparing data frame for all the simulations\nfwer_sim_df <- data.frame(\n  positive_prop = rep(c(1,3,5,7,9), n_tests)/10,\n  pop_pos = NA,\n  pop_neg = NA,\n  samp_pos = NA,\n  samp_neg = NA,\n  fwer = NA\n)\n\ntests_per_sim = 100\n  \nfor(i in 1:length(fwer_sim_df$positive_prop)){\n  # prepare a data frame to compare positive and negatives within a sample compared to the population\n  this_subset = data.frame(\n    population = rep(\"\",tests_per_sim),\n    sample     = \"\",\n    true_false = FALSE,\n    p.value    = NA\n  )\n  \n  this_pos_prop = fwer_sim_df$positive_prop[i]\n  for(k in 1:tests_per_sim){\n    # if the population is positive\n    if(k/tests_per_sim <= this_pos_prop){\n      this_subset$population[k] = \"positive\"\n      this_t_test <- t.test(\n        rnorm(\n          this_pp, \n          mean = this_effect_size, \n          sd = 1\n        ),\n        mu = 0\n      )\n      this_subset$p.value[k] <- this_t_test$p.value\n    }\n    \n    #if the population is negative\n    if(k/tests_per_sim > this_pos_prop){\n      this_subset$population[k] = \"negative\"\n      this_subset$p.value[k] = runif(1,min = 0,max = 1)\n    }\n  }\n  # sort the subset into appropriate ranks\n  this_subset %>% \n    arrange(p.value) -> sorted_subset\n  sorted_subset$rank = rank(sorted_subset$p.value)\n\n  ## FWER using HS (Holm-Šidák)\n  hb_tp = 0\n  hb_tn = 0\n  hb_fp = 0\n  hb_fn = 0\n  valid_alpha = TRUE\n  hb_max_alpha = NA\n  for(j in 1:tests_per_sim){\n    this_alpha = 1 - (1-.05)^(1/(tests_per_sim + 1 - j))\n    # positive\n    if(sorted_subset$p.value[j] < this_alpha & valid_alpha){\n      # true\n      if(sorted_subset$population[j] == \"positive\"){\n        hb_tp = hb_tp + 1\n      } else if(sorted_subset$population[j] == \"negative\"){\n        hb_fp = hb_fp + 1\n      }\n      hb_max_alpha = this_alpha\n    } else {\n      valid_alpha = FALSE\n      if(sorted_subset$population[j] == \"positive\"){\n        hb_fn = hb_fn + 1\n      } else if(sorted_subset$population[j] == \"negative\"){\n        hb_tn = hb_tn + 1\n      }\n    }\n  }\n  fwer_sim_df$hb_tp[i]        = hb_tp\n  fwer_sim_df$hb_fp[i]        = hb_fp\n  fwer_sim_df$hb_tn[i]        = hb_tn\n  fwer_sim_df$hb_fn[i]        = hb_fn\n  fwer_sim_df$hb_max_alpha[i] = hb_max_alpha\n\n}\n\n\n\n\n\n# identifying if any simulation had at least one false positive to identify FWER as 1 or 0\nfwer_sim_df$hb_fwer = ifelse(fwer_sim_df$hb_fp > 0, 1 , 0)\n\n# allocating simulations into sets of 100\nfwer_sim_df$sim_group = rep(1:25, each = 100)\n\nfwer_summary <- fwer_sim_df %>% \n  group_by(positive_prop, sim_group) %>% \n  summarise(\n    .groups = \"keep\",\n    fwer = mean(hb_fwer)\n  )\n\nfwer_summary_mean_se <- fwer_summary %>% \n  group_by(positive_prop) %>% \n  summarise(\n    fwer_mean = mean(fwer),\n    fwer_sd = sd(fwer)\n  )\n\n\nggplot(\n  data = fwer_summary_mean_se, \n  aes(\n    x=positive_prop,\n    y=fwer_mean, \n    fill=as.factor(positive_prop))\n  ) +\n  geom_col() +\n  geom_errorbar(aes(ymin = fwer_mean - fwer_sd, ymax = fwer_mean + fwer_sd)) +\n  xlab(\"Proportion of the population that is positive\") +\n  ylab(\"FWER per 100 simulations\")\n\n\n\n\nFigure 5: A visualisation of simulations that demonstrate that the proportion of positive effects in the population does not impact the family-wise error rate when using a Holm-Šidák procedure.\n\n\n\n\nThe above figure (error bars reflect SD) visualises how the FWER doesn’t change as a function of the proportion of the population effects are positive (as expected)."
  },
  {
    "objectID": "multipleTesting/fdr.html",
    "href": "multipleTesting/fdr.html",
    "title": "False Discovery Rate(R)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "multipleTesting/fdr.html#what-is-the-false-discovery-rate",
    "href": "multipleTesting/fdr.html#what-is-the-false-discovery-rate",
    "title": "False Discovery Rate(R)",
    "section": "What is the False Discovery Rate?",
    "text": "What is the False Discovery Rate?\nThe false discovery rate (FDR) is the expected proportion of false positives out of all positives (both true and false). In simpler terms, out of all the findings you present as positive, FDR reflects how many of them are false. It can be formalised as:\n\\[\nFDR = \\frac{FalsePositives}{FalsePositives + TruePositives}\n\\]\nWe will investigate one procedure that aims to control the FDR and then do simulations to confirm whether it is successful."
  },
  {
    "objectID": "multipleTesting/fdr.html#benjamini-hochberg-procedure",
    "href": "multipleTesting/fdr.html#benjamini-hochberg-procedure",
    "title": "False Discovery Rate(R)",
    "section": "Benjamini-Hochberg procedure",
    "text": "Benjamini-Hochberg procedure\nSimilar to Bonferroni-Holm corrections, you first rank all the p-values of all your tests, and then calculate the \\(\\alpha\\) threshold depending on the rank. This calculation will also include your estimation of the false discovery rate\n\\[\n\\alpha = (k/m)*Q\n\\]\n\n\\(\\alpha\\) is the p-value significance threshold for the specific test\n\\(k\\) is the rank of the current test’s p-value. 1 represents the smallest p-value\n\\(m\\) is the total number of tests\n\\(Q\\) is the false discovery rate (chosen by the researcher)\n\nLet’s see what \\(\\alpha_{bh}\\) values we get with this approach within a made-up experiment that had 10 tests\n\nRPython\n\n\n\nlibrary(kableExtra)\nbh_df <- data.frame(\n  p     = c(.001,.01,.025,.041,.045,.06,.08,.1,.12,.3),\n  rank  = 1:10,\n  tests = 10,\n  fdr   = .1\n)\nbh_df$alpha = (bh_df$rank/bh_df$tests) * bh_df$fdr\nbh_df$sig   = bh_df$alpha > bh_df$p\n\nbh_df %>% \n  kable(booktabs = T) %>%\n  kable_styling() %>%\n  row_spec(which(bh_df$alpha > bh_df$p), bold = T, color = \"white\", background = \"blue\")\n\n\n\n \n  \n    p \n    rank \n    tests \n    fdr \n    alpha \n    sig \n  \n \n\n  \n    0.001 \n    1 \n    10 \n    0.1 \n    0.01 \n    TRUE \n  \n  \n    0.010 \n    2 \n    10 \n    0.1 \n    0.02 \n    TRUE \n  \n  \n    0.025 \n    3 \n    10 \n    0.1 \n    0.03 \n    TRUE \n  \n  \n    0.041 \n    4 \n    10 \n    0.1 \n    0.04 \n    FALSE \n  \n  \n    0.045 \n    5 \n    10 \n    0.1 \n    0.05 \n    TRUE \n  \n  \n    0.060 \n    6 \n    10 \n    0.1 \n    0.06 \n    FALSE \n  \n  \n    0.080 \n    7 \n    10 \n    0.1 \n    0.07 \n    FALSE \n  \n  \n    0.100 \n    8 \n    10 \n    0.1 \n    0.08 \n    FALSE \n  \n  \n    0.120 \n    9 \n    10 \n    0.1 \n    0.09 \n    FALSE \n  \n  \n    0.300 \n    10 \n    10 \n    0.1 \n    0.10 \n    FALSE \n  \n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Create a DataFrame\nbh_df = pd.DataFrame({\n    'p': [.001, .01, .025, .041, .045, .06, .08, .1, .12, .3],\n    'rank': list(range(1, 11)),\n    'tests': 10,\n    'fdr': 0.1\n})\n\n# Calculate alpha and significance\nbh_df['alpha'] = (bh_df['rank'] / bh_df['tests']) * bh_df['fdr']\nbh_df['sig'] = bh_df['alpha'] > bh_df['p']\n\n# Create a custom styling function to highlight rows\ndef highlight_greaterthan(s, threshold, column):\n    is_max = pd.Series(data=False, index=s.index)\n    is_max[column] = s.loc[column] >= threshold\n    \n    # Create a style object with blue background and white color for the text\n    style = ['background-color: blue;color: white' if is_max.any() else '' for v in is_max]\n    \n    # Set white text for the entire row\n    return style\n\n\nbh_df.style.apply(highlight_greaterthan, threshold=1.0, column=['sig'], axis=1)\n\n\n\n\n\n\n\n\n\nWe can see that there’s a slightly odd pattern, in which the fourth smallest p-value is not significant but the fifth is. For the Benjamini-Hochberg correction you take the lowest ranked p-value that is still significant and then accept all tests that have a smaller p-value than it as significant. So the rank 4 test above would be accepted as significant because of this rule.\nLet us now check how this correction relates to positives and negatives in the population using some simulations. For these simulations we will split the data into “positive” and “negative” population tests. For “positive” effects in the population we will use one-sample t-tests to generate expected p-values. For negative effects in the population we will generate a p-value between 0 and 1 as each p-value is equally likely for negative effects. We will also assume the proportion of positive effects being tested is .5, i.e. that half of our tests should get a significant result to reflect the population and the other half should be negative.\n\n\n\n\n\n\nOptional: Click here to see the R code to generate the data for the figure below\n\n\n\n\n\n\nRPython\n\n\n\noptions(scipen = 999)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# calculate how many participants are needed\nthis_effect_size = 1\nsim_pwr   = pwr::pwr.t.test(\n  d = this_effect_size, \n  sig.level = .05, \n  power = .95, \n  type = \"one.sample\"\n)\nthis_pp   = round(sim_pwr$n)\nq_values = c(.01,.025,.05,.075,.1)\nn_tests  = 100\n\n# preparing data frame for all the simulations\nfdr_sim_df <- data.frame(\n  q = c(\n    rep(q_values, each = n_tests)\n  ),\n  positive_prop = .5,\n  pop_pos = NA,\n  pop_neg = NA,\n  samp_pos = NA,\n  samp_neg = NA,\n  fdr = NA\n)\n\ntests_per_sim = 100\n  \nfor(i in 1:length(fdr_sim_df$q)){\n  # prepare a data frame to compare positive and negatives within a sample compared to the population\n  this_subset = data.frame(\n    population = rep(\"\",tests_per_sim),\n    sample     = \"\",\n    true_false = FALSE,\n    p.value    = NA\n  )\n  \n  this_pos_prop = fdr_sim_df$positive_prop[i]\n  for(k in 1:tests_per_sim){\n    # if the population is positive\n    if(k/tests_per_sim <= this_pos_prop){\n      this_subset$population[k] = \"positive\"\n      this_t_test <- t.test(\n        rnorm(\n          this_pp, \n          mean = this_effect_size, \n          sd = 1\n        ),\n        mu = 0\n      )\n      this_subset$p.value[k] <- this_t_test$p.value\n    }\n    \n    #if the population is negative\n    if(k/tests_per_sim > this_pos_prop){\n      this_subset$population[k] = \"negative\"\n      this_subset$p.value[k] = runif(1,min = 0,max = 1)\n    }\n  }\n  # sort the subset into appropriate ranks\n  this_subset %>% \n    arrange(p.value) -> sorted_subset\n  sorted_subset$rank = rank(sorted_subset$p.value)\n\n  sorted_subset$alpha = (sorted_subset$rank/tests_per_sim) * fdr_sim_df$q[i]\n  sorted_subset$sig   = sorted_subset$p.value < sorted_subset$alpha\n  max_sig = max(sorted_subset$rank[sorted_subset$sig])\n  sorted_subset$sample = \"negative\"\n  sorted_subset$hb_sample = \"negative\"\n  \n  fdr_sim_df$bh_max_alpha[i] = sorted_subset$alpha[max_sig]\n  \n  if(max_sig >= 0){\n    sorted_subset$sample[1:max_sig] = \"positive\"\n    # false positives / (false positives + true positives)\n    fdr_sim_df$fdr[i] = sum(sorted_subset$population == \"negative\" & sorted_subset$sample == \"positive\")/\n      (\n        sum(sorted_subset$population == \"positive\" & sorted_subset$sample == \"positive\") +\n          sum(sorted_subset$population == \"negative\" & sorted_subset$sample == \"positive\")\n      )\n  } else {\n    fdr_sim_df$fdr[i] = NA\n    fdr_sim_df$fwer[i] = NA\n  }\n  \n  ## Quality check\n  fdr_sim_df$pop_pos[i] = sum(sorted_subset$population == \"positive\")\n  fdr_sim_df$pop_neg[i] = sum(sorted_subset$population == \"negative\")\n  fdr_sim_df$samp_pos[i] = sum(sorted_subset$sample == \"positive\")\n  fdr_sim_df$samp_neg[i] = sum(sorted_subset$sample == \"negative\")\n  \n  fdr_sim_df$false_pos[i] = sum(sorted_subset$sample == \"positive\" &\n                                  sorted_subset$population == \"negative\")\n  fdr_sim_df$false_neg[i] = sum(sorted_subset$sample == \"negative\" &\n                                  sorted_subset$population == \"positive\")\n  fdr_sim_df$true_pos[i] = sum(sorted_subset$sample == \"positive\" & \n                                 sorted_subset$population == \"positive\")\n  fdr_sim_df$true_neg[i] = sum(sorted_subset$sample == \"negative\" & \n                                 sorted_subset$population == \"negative\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nRPython\n\n\n\n# Maximise the code above to generate the required data\nggplot(fdr_sim_df, aes(x = q, y = fdr,color=as.factor(positive_prop))) + \n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  xlab(\"Researcher estimated FDR\") +\n  ylab(\"Actual FDR\") + \n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1, color=\"Expected FDR\")) +\n  geom_jitter(width = .0025, height = 0.001) +\n  coord_cartesian(\n    xlim = (c(0,.1)),\n    ylim = (c(0,.15))\n  ) +\n  labs(color=\"Proportion Positive\")\n\n\n\n\nFigure 1: A simulation of FDR when using Benjamini-Hochberg procedure. The “Researcher estimated FDR” is the q-value that the researcher chooses.\n\n\n\n\n\n\n\n\n\n\n\n\nThe above simulations show an association between the false discovery rate a researcher sets (known as \\(q\\)) and the actual false discovery rate. Whilst this page focuses on a scenario in which half of the effects being tested are positive, see here for the impact of having different positive rates on the FDR rate. The good news is that the Benjamani-Holchberg procedure generally keeps the FDR below the \\(q\\) that the researcher sets."
  },
  {
    "objectID": "multipleTesting/fwerQuestions.html",
    "href": "multipleTesting/fwerQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof fwer1_response = Inputs.radio(['True','False']);\ncorrect_fwer1 = 'False';\nfwer_1color = {\n  if(fwer1_response == correct_fwer1){\n    return 'blue';\n  } else {\n    return 'red';\n  }   \n}\nfwer1_result = {\n  if(fwer1_response == correct_fwer1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof fwer2_response = Inputs.radio(['Bonferroni','Šidák']);\ncorrect_fwer2 = 'Šidák';\nfwer2_result = {\n  if(fwer2_response == correct_fwer2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "multipleTesting/positiveNegative.html",
    "href": "multipleTesting/positiveNegative.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Click here for a reminder about what positive and negative effects are in the population\n\n\n\n\n\nIn the context of multiple testing we will refer to effects in the population that do exist as positives, and effects that don’t exist in the population as negatives. This means that a test on a negative effect should ideally you give you a non-significant (negative) result in your sample so that your sample reflects the population (and gives you a true negative). Similarly, if an effect in the population exists, ideally your test on your sample will give you a significant result to reflect a true positive."
  },
  {
    "objectID": "multipleTesting/fdrQuestions.html",
    "href": "multipleTesting/fdrQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich of the following reflects false discovery rate:\n\nviewof fdr_1_response = Inputs.radio(['False Positives/Negatives in the population','False Positves/Positives in the population','False positives/(False Positives + True Positives)']);\ncorrect_fdr_1 = 'False positives/(False Positives + True Positives)';\nfdr_1_result = {\n  if(fdr_1_response == correct_fdr_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter ranking all the p-values which of the following is true about the Benjamini-Hochberg procedure:\n\nviewof fdr_2_response = Inputs.radio(['You only accept p-values from the smallest to the largest until the first one that is above the corrected alpha value',\"You accept all p-values that are smaller than the highest p-value below it's respective alpha threshold.\"]);\ncorrect_fdr_2 = \"You accept all p-values that are smaller than the highest p-value below it's respective alpha threshold.\";\nfdr_2_result = {\n  if(fdr_2_response == correct_fdr_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "multipleTesting/fwer_vs_fdrQuestions.html",
    "href": "multipleTesting/fwer_vs_fdrQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhat association is there between the proportion of positive effects and FWER?\n\nviewof fwer_vs_fdr1_response = Inputs.radio(['Positive','Neutral','Negative']);\ncorrect_fwer_vs_fdr1 = 'Neutral';\nfwer_vs_fdr1_result = {\n  if(fwer_vs_fdr1_response == correct_fwer_vs_fdr1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat association is there between the proportion of positive effects and FDR?\n\nviewof fwer_vs_fdr2_response = Inputs.radio(['Positive','Neutral','Negative']);\ncorrect_fwer_vs_fdr2 = 'Negative';\nfwer_vs_fdr2_result = {\n  if(fwer_vs_fdr2_response == correct_fwer_vs_fdr2){\n    return 'Correct! There should be a lower rate of false positives compared to false positives and true positives in a study that is investigating mostly positive effects compared to a study that is investigating mostly negative effects.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "multipleTesting/backup_of_fwer_vs_fdr.html",
    "href": "multipleTesting/backup_of_fwer_vs_fdr.html",
    "title": "backup of fdr vs fwer code",
    "section": "",
    "text": "This is distinct from family-wise error-rate (FWER) because FWER’s corrections are optimised to reduce the risk of any negative result being incorrectly accepted as positive, and thus doesn’t need to consider the ratio of positives to negatives in the population.\nthe rate of false positives compared to all tests that should find a negative result (because the effect is negative in the population).\n\\[\nFWER = \\frac{FalsePositives}{FalsePositives + \\color{red}{TrueNegatives}}\n\\]\nHowever, in practice we don’t know which results are positive or negative in the population and thus how many of them are true or false. This is less of a problem for FWER because you are just comparing two types of results that should be negative to each other:\n\\[\nFWER  = \\frac{FalsePositives}{FalsePositives + \\color{red}{TrueNegatives}} = \\frac{m_{Incorrect Negatives As Positive}}{m_{Negatives}}\n\\]\n\n\\(m_{Incorrect Negatives As Positive}\\) is the number of findings in your study that should be negative based on the population, but are not negative in your sample (i.e. false positives)\n\\(m_{Negatives}\\) is the number of findings in your study that should be negative based on the population\n\nAs shown in FWER corrections, there are ways to restrict the likelihood of any of your tests being a false positive to 5% or less (although it involves mathematically assuming all your results are negatives in the population, which could be seen as conservative). However, on this page we are thinking of FWER as the proportion of\nHowever, FDR is more complicated because you would need to know how many true and false findings there are in the population. Let us imagine some scenarios to see how the false discovery rate would change depending on the ratio of positive and negative findings in the population:\n\nfdr_df <- data.frame(\n  pop_pos  = c(700,500,200),\n  pop_neg  = c(300,500,800),\n  power    = c(.95,.95,.95),\n  alpha    = c(.05,.05,.05)\n)\nfdr_df$true_pos  = fdr_df$pop_pos * fdr_df$power\nfdr_df$true_neg  = fdr_df$pop_neg * (1- fdr_df$alpha)\nfdr_df$false_pos = fdr_df$pop_neg * fdr_df$alpha\nfdr_df$false_neg = fdr_df$pop_pos * (1-fdr_df$power)\nfdr_df$FDR       = fdr_df$false_pos/(fdr_df$false_pos + fdr_df$true_pos)\nfdr_df$FWER      = fdr_df$false_pos/(fdr_df$false_pos + fdr_df$true_neg)\nknitr::kable(fdr_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npop_pos\npop_neg\npower\nalpha\ntrue_pos\ntrue_neg\nfalse_pos\nfalse_neg\nFDR\nFWER\n\n\n\n\n700\n300\n0.95\n0.05\n665\n285\n15\n35\n0.0220588\n0.05\n\n\n500\n500\n0.95\n0.05\n475\n475\n25\n25\n0.0500000\n0.05\n\n\n200\n800\n0.95\n0.05\n190\n760\n40\n10\n0.1739130\n0.05\n\n\n\n\n\nAs the above table shows, the FDR is not influenced by the ratio of positive vs. negative effects in the population, whereas FDR is strongly influenced by these. These could be visualised as follows:\n\n\n\n\n\n\nExpand to see calculation for the Venn Diagrams below\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nggarrange(\n  pos_neg_1_plot,\n  fdr_1_plot,\n  fwer_1_plot,\n  pos_neg_2_plot,\n  fdr_2_plot,\n  fwer_2_plot,\n  pos_neg_3_plot,\n  fdr_3_plot,\n  fwer_3_plot,\n  ncol = 3,\n  nrow = 3,\n  labels = c(\"Sample\",\"FDR\",\"FWER\")\n)\n\n\n\n\nIn the above figure green represents positives (bright green represents true positives), blue represents negatives, purple reflects false negatives and red reflects false positives. Consistent with the table above, the ratio between false positives and true positives shifts depending on how many positives and negatives there are in the population, thus changing the FDR. However, the ratio between negativesand false positives is consistent regardless of how many positives and negatives there are in the population, which is why the FWER is not influenced by this ratio between positives and negatives.\nSo is there a way to address this problem of the underlying ratio between positives and negatives?"
  },
  {
    "objectID": "multipleTesting/fwer.html",
    "href": "multipleTesting/fwer.html",
    "title": "Family-Wise Error (R)",
    "section": "",
    "text": "As mentioned in statistics basics, researchers generally accept a 5% risk of a study reporting a significant finding when the effect found in their sample is not representative of the population (i.e. an \\(\\alpha\\) value of .05). However, what happens if you conduct multiple tests within a study? Applying the same \\(\\alpha\\) value of .05 to each test within your study becomes a problem of multiple testing increasing the risk of a false positive (unless you happen to only be investigating real effects, which you cannot be sure you are doing). To illustrate the issue, let’s create some random data in which you want to test if there are optimal combinations of height and foot size that predict maths ability. One thing you might be thinking is that this is a meaningless analysis, that there is no optimal height and foot-size in relation to maths ability - and that seems like a sensible opinion. However, without correcting for multiple testing, it’s almost guaranteed that the following analysis on randomised data will produce false-positives, i.e. identify certain combinations of height and foot-size that are associated with significantly higher or lower mathematics scores.\nThe analysis below will create 12 heights and 12 shoe sizes, generate 20 participants in each combination with a random score out of 100 in a maths test. Then there will be comparisons between each group and all other participants to identify if their especially good (or bad) at maths (which, again, is a ridiculous thing to believe).\nThe above figure shows that with an analysis on randomised data there were multiple combinations of foot-size and height that were associated with significantly better or worse performance in the maths task. As this is randomised data, it should illustrate the risk of multiple testing without correction. You may be wondering how likely was it that we would get a false-positive. Assuming that the comparisons you are making are meaningless, the risk of a false positive can be summarised as:\n\\[\nfalsePositive_{risk} = (1-\\alpha)^{comparisons}\n\\]\nNow let’s discuss some corrections for multiple testing and see whether they successfully reduce the number of false positives in our hypothetical scenario."
  },
  {
    "objectID": "multipleTesting/fwer.html#bonferroni-correction",
    "href": "multipleTesting/fwer.html#bonferroni-correction",
    "title": "Family-Wise Error (R)",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nA simple correction to avoid the risk of false positives is to divide the \\(a\\) threshold by the number of tests:\n\\[\n\\alpha = \\frac{.5}{m}\n\\]\n\n\\(m\\) is the number of comparisons\n\nIn some ways, this does the job of correcting for false-positives well, as your p-value threshold is directly proportionate to the number of tests you complete. Let’s see if a Bonferroni correction removes all the false-positives:\n\n# reset the \"sig\" column\nmaths_summarised$sig = \"no\"\n\ncomparisons = length(heights) * length(shoe_size)\nfor(height in heights){\n  for(shoe_size in shoe_sizes){\n    exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]\n    con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]\n    this_t.test <- t.test(exp_group_scores, con_group_scores)\n    \n    # here is where the new alpha threshold is applied\n    if(this_t.test$p.value < .05/comparisons){\n      maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"yes\"\n    }\n  }\n}\n\n\nplot_ly(\n  data = maths_summarised, \n  x=~height, \n  y=~shoe,\n  z=~score, \n  color=~sig, \n  error_z = list(array=~se), \n  type = \"scatter3d\",\n  mode = \"markers\",\n  size = 1,\n  colors = yes_no_colors\n) \n\n\n\n\nFigure 2: Same visualisation as above, but this time the alpha threshold has a bonferroni correction.\n\n\n\nAll the points are red above, confirming that there were no false-positives. However, this was the new alpha value:\n\\[\n\\alpha = \\frac{.05}{N_{tests}} = \\frac{.05}{144} = 0.0003472222\n\\]\nWhich means that you would need either to be investigating a very large effect or testing very many participants. Bonferroni correction is thus very conservative, and increases the risk of a false-negative (incorrectly concluding that there is no effect). There is also an impact on family-wise error rate which needs to be explained before you consider other corrections."
  },
  {
    "objectID": "multipleTesting/fwer.html#family-wise-error-rate-fwer",
    "href": "multipleTesting/fwer.html#family-wise-error-rate-fwer",
    "title": "Family-Wise Error (R)",
    "section": "Family-wise error rate (FWER)",
    "text": "Family-wise error rate (FWER)\nIf our aim is to reduce the risk of a study reporting at least one false-positive to 5%, this is the family-wise error rate (FWER) and is distinct to the risk of a single statistical test giving you a false positive which is the \\(\\alpha\\) threshold. Without correcting the for multiple testing the family-wise error rate would increase with more tests (as shown above). However, some corrections for multiple testing are so conservative that the family-wise error rate will drop below 5%.\nA problem with Bonferroni corrections is that they do not maintain a family-wise error rate of .05. The more tests you conduct, the lower the family-wise error rate is. To illustrate this, we first need to calculate the likelihood that there are not any false-positives across all tests. For each individual test the likelihood is:\n\\[\n1-\\alpha_{bonferroni}\n\\]\nIf you ran 2 tests, a Bonferroni correction means that for each tests there is only a .025 (.05 divided by 2) chance that the test would give a false-positive. So for this example of 2 tests you would get:\n\\[\n1 - .025 = .975\n\\]\nA 97.5% chance of avoiding a false positive for one test alone. For each test you run, you multiply this likelihood by itself to calculate the likelihood that all your tests are not false-positives. So if there are 2 tests:\n\\[\n(1 - .025) * (1 - .025) = .975 * .975 = .950625\n\\]\nOr, you could just take the likelihood for any individual test’s false-positive and apply the power of the number of tests, e.g.:\n\\[\n(1-.025)^2 = .975^2 = .950625\n\\]\nOr more generally\n\\[\n(1 - \\alpha_{bonferroni})^m\n\\]\n\n\\(\\alpha_{bonferroni}\\) is the Bonferroni corrected alpha value\n\\(m\\) is still the number of tests\n\nNow that we have the likelihood of getting no false-positives, we can calculate the family-wise error rate (FWER) by subtracting this likelihood from 1:\n\\[\nFWER = 1 - (1 - \\alpha_{bonferroni})^m\n\\]\nIf there are 2 tests, the FWER is:\n\\[\nFWER = 1 - (1 - .05)^2 = 1 - .975^2 = 1 - .950625 = .049375\n\\]\nWhilst this is very similar to our original \\(\\alpha\\) threshold of .05, it is more conservative because the threshold for a false-positive is stricter (i.e. lower) than .05. Let’s check these out for Šidák and Holm-Bonferroni corrections."
  },
  {
    "objectID": "multipleTesting/fwer.html#šidák-correction",
    "href": "multipleTesting/fwer.html#šidák-correction",
    "title": "Family-Wise Error (R)",
    "section": "Šidák correction",
    "text": "Šidák correction\nA less conservative correction is Šidák which aims to maintain a family-wise error rate across all your tests. If your philosophy is that you only want a 5% chance of a false positive across your whole study, then Šidák’s formula successfully calculates the required \\(\\alpha\\) value:\n\\[\n\\alpha_{Šidák} = 1 - (1-.05)^{(1/m)}\n\\]\n\n\\(\\alpha_{Šidák}\\) is the Šidák corrected\n\\(.05\\) is included as the FWER that this formula aims to achieve. If you wanted a higher or lower FWER you would need to adjust this number in your formula\n\\(m\\) is (still) the number of tests you conduct in your study\n\n\n\n\n\n\n\nOptional - how do we get Šidák’s formula?\n\n\n\nFor those of you interested in how we get to Šidák’s formula, we use algebra to re-organise the FWER formula above to focus on calculating Alpha rather than FWER:\n\\[\nFWER = 1 - (1 - \\alpha)^m\n\\]\nLet’s minus 1 from both sides:\n\\[\nFWER - 1 = -(1 - \\alpha)^m\n\\]\nThen multiply both sides by -1:\n\\[\n1 - FWER = (1 - \\alpha)^m\n\\]\nThen apply the power of (1/m) to both sides:\n\\[\n(1 - FWER)^{1/m} = 1 - \\alpha\n\\]\nFollowed by adding alpha to both sides:\n\\[\n\\alpha + (1 - FWER)^{1/m} = 1\n\\]\nThen subtracting \\((1 - FWER)^{1/m}\\) from both sides:\n\\[\n\\alpha = 1 - (1 - FWER)^{1/m}\n\\]\nBoom, you’re done.\n\n\nNow let’s apply Šidák’s corrected \\(\\alpha\\) to the above maths test data.\n\n# reset the \"sig\" column\nmaths_summarised$sig = \"no\"\nsidak_alpha =  1 - (1 - .05)^(1/144)\n\ncomparisons = length(heights) * length(shoe_size)\nfor(height in heights){\n  for(shoe_size in shoe_sizes){\n    exp_group_scores = maths_scores$maths[maths_scores$height == height & maths_scores$shoe == shoe_size]\n    con_group_scores = maths_scores$maths[maths_scores$height != height & maths_scores$shoe != shoe_size]\n    this_t.test <- t.test(exp_group_scores, con_group_scores)\n    \n    # here is where the new alpha threshold is applied\n    if(this_t.test$p.value < sidak_alpha){\n      maths_summarised$sig[maths_summarised$height == height & maths_summarised$shoe == shoe_size] = \"yes\"\n    }\n  }\n}\n\n\nplot_ly(\n  data = maths_summarised, \n  x=~height, \n  y=~shoe,\n  z=~score, \n  color=~sig, \n  error_z = list(array=~se), \n  type = \"scatter3d\",\n  mode = \"markers\",\n  size = 1,\n  colors = yes_no_colors\n)\n\n\n\n\nFigure 3: Same visualisation as above, but this time the alpha threshold has a Šidák correction.\n\n\n\nGood news - no false positives 😺. However, the \\(\\alpha\\) threshold is simarly conservative as a Bonferroni correction:\n\nBonferroni: 0.0003472\nŠidák: 0.0003561\n\nAnd so both would require unusually strong effects or extremely large sample sizes to detect true-positive results."
  },
  {
    "objectID": "multipleTesting/fwer.html#holm-bonferroni-method",
    "href": "multipleTesting/fwer.html#holm-bonferroni-method",
    "title": "Family-Wise Error (R)",
    "section": "Holm-Bonferroni method",
    "text": "Holm-Bonferroni method\nThe Holm-Bonferroni method orders the p-values from the relevant tests in order of significance, and applies a Bonferroni correction threshold to the most significant results, and gradually reduces the threshold as you proceed through the ranks. To illustrate this, let’s imagine a researcher ran 4 tests, and got the following 4 p-values (already ranked)\n\n\\(p_1\\) = .005\n\\(p_2\\) = .0125\n\\(p_3\\) = .03\n\\(p_4\\) = .049\n\nIf we were to apply a Bonferroni correction, all p-values would need to be less than .05/4 = .0125 to be significant. Considering the fact that all results would be significant with an uncorrected \\(\\alpha\\) threshold, it seems that rejecting 3 out of 4 p-values could reflect an overly conservative \\(\\alpha\\). Holm-Bonferroni only compares the most significant result against a Bonferroni corrected threshold, and then reduces the denominator by 1 for each comparison made (\\(k\\) below):\n\\[\n\\frac{.05}{m + 1 - k}\n\\]\n\n\\(m\\) is the number of tests/comparisons\n\\(k\\) is the rank of the current comparison’s p-value (1 for the smallest p-value)\n\nLet’s apply this formula to each of our 4 p-values:\n\n\\(p_1\\) = .005 compared to the threshold of \\(.05/(4 + 1 - 1) = .0125\\) significant\n\\(p_2\\) = .0125 compared to the threshold of \\(.05/(4 + 1 - 2) = .0167\\) significant\n\\(p_3\\) = .03 compared to the threshold of \\(.05/(4 + 1 - 3) = .025\\) not significant\n\nOnce you reach a non-significant result, you stop your comparisons as you’ve identified the p-values that are at too high a risk of a false-positive. To highlight the weirdness of what would happen if you continued:\n\n\\(p_4\\) = .049 compared to the threshold of \\(.05/(4 + 1 - 4) = .05\\) significant!?!?!?!?\n\nAs \\(p_3\\) is more significant than \\(p_4\\) it would be strange to conclude that the finding associated with \\(p_4\\) is significant but \\(p_3\\) isn’t.\nAn advantage of this approach is that you won’t only capture the most significant result(s) in the way that you would with a Bonferroni correction, but the scaling adjustment is (arguably) an acceptable risk to false-positives compared to the risk of false-negatives for all p-values that a Bonferroni correction involves.\nBut what about the family-wise error rate (FWER)? Well, remember that FWER refers to the risk of at least one false positive, so the FWER formula is identical to a Bonferroni correction as the likelihood of the first false positive across all tests is the Bonferroni corrected alpha value:\n\\[\nFWER = 1 - (1 - \\alpha_{bonferroni})^m\n\\]\n\n\n\n\n\n\nOptional - likelihood of a second false-positive?\n\n\n\n\n\nWhilst FWER is identical for Bonferroni and Holm-Bonferroni corrections, Holm-Bonferroni has a higher risk of getting multiple false-positives. Let’s illustrate this with binomial mathematics, starting with the likelihood of a Bonferroni test’s likelihood of 2 false positives if there are 2 tests:\n\nbonferroni_comb <- data.frame(\n  description = c(\n    \"Both tests are false-positives\",\n    \"Only the first test is a false-positive\",\n    \"Only the second test is a false-positive\",\n    \"Neither test is a false-positive\"\n  ),\n  first.test = c(.025,.025,.975,.975),\n  second.test = c(.025,.975,.025,.975)\n)\nbonferroni_comb$likelihood = bonferroni_comb$first.test * bonferroni_comb$second.test\n\nknitr::kable(bonferroni_comb)\n\n\n\n\n\n\n\n\n\n\ndescription\nfirst.test\nsecond.test\nlikelihood\n\n\n\n\nBoth tests are false-positives\n0.025\n0.025\n0.000625\n\n\nOnly the first test is a false-positive\n0.025\n0.975\n0.024375\n\n\nOnly the second test is a false-positive\n0.975\n0.025\n0.024375\n\n\nNeither test is a false-positive\n0.975\n0.975\n0.950625\n\n\n\n\nsum(bonferroni_comb$likelihood)\n\n[1] 1\n\n\nIf you add all the likelihoods together you get 1, confirming that we have addressed all outcomes’ likelihoods. The likelihood of 2 false-positives is .0025. We can visualise the above as follows:\n\nggplot() + \n  geom_vline(xintercept = 1) + \n  geom_hline(yintercept = 1) +\n  xlim(0,1) + \n  ylim(0,1) +\n  xlab(\"first result p-value\") +\n  ylab(\"second result p-value\") +\n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .05,\n      ymin = 0,\n      ymax = .05\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .05,\n      ymin = .05,\n      ymax = 1\n    ),\n    fill = \"green\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.05,\n      xmax = 1,\n      ymin = 0,\n      ymax = .05\n    ),\n    fill = \"blue\",\n    alpha = .5\n  )   + \n  geom_rect(\n    mapping = aes(\n      xmin = .05,\n      xmax = 1,\n      ymin = 0.05,\n      ymax = 1\n    ),\n    fill = \"orange\",\n    alpha = .5\n  ) \n\n\n\n\nThe area for each color on the above figure represents the likelihood of its respective outcome:\n\nRed: “Both tests are false-positives”, p = 0.000625, i.e. takes 0.000625 of the total area of the above figure\nGreen: “Only the first test is a false-positive”, p = 0.024375, i.e. takes 0.024375 of the total area of the above figure\nBlue: “Only the second test is a false-positive”, p = 0.024375, i.e. takes 0.024375 of the total area of the above figure\nOrange: “Neither test is a false-positive”, p = 0.950625, i.e. takes 0.950625 of the total area of the above figure\n\nLet’s now compare the above table to a Bonferroni-Holm methodology. It’s a little more complicated by the fact that there are 2 different p-value thresholds, which makes it more difficult to calculate outcomes that are mutually exclusive. We will try to be precise to avoid making references to overlapping outcomes:\n\nbonferroni_holm_comb <- data.frame(\n  description = c(\n    \"Both are false positives, but only the first test is <.025\",\n    \"Both are false positives, but only the second test is <.025\",\n    \"Both are false positives and both < .025\",\n    \"Only the first test is a false-positive\",\n    \"Only the second test is a false-positive\",\n    \"Neither test is a false-positive\"\n  ),\n  first.test = c(\n    .025, # likelihood of 0 to .025 as it is the ONLY test that it < .025\n    .025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025\n    .025, # likelihood of 0 to .025 as both tests are < .025\n    .025, # likelihood of 0 to .025 as it is the ONLY false-positive\n    .95,  # likelihood of 0 to .95 as the other test is significant\n    .975  # likelihood of 0 to .975 as the other test is not significant\n  ),\n  second.test = c(\n    .025, # likelihood of .025 to .05 as ONLY the OTHER test is < .025\n    .025, # likelihood of 0 to .025 as it is the ONLY test that it < .025\n    .025, # likelihood of 0 to .025 as both tests are <.025\n    .95,  # likelihood of 0 to .025 as it is the ONLY false-positive\n    .025, # likelihood of 0 to .95 as the other test is significant\n    .975  # likelihood of 0 to .975 as the other test is not significant\n  )\n)\nbonferroni_holm_comb$likelihood = bonferroni_holm_comb$first.test * bonferroni_holm_comb$second.test\n\n# knitr::kable(bonferroni_holm_comb)\nsum(bonferroni_holm_comb$likelihood)\n\n[1] 1\n\n\nLooks like we succeeded 😸. Now let’s visualise the above:\n\nggplot() + \n  geom_vline(xintercept = 1) + \n  geom_hline(yintercept = 1) +\n  xlim(0,1) + \n  ylim(0,1) +\n  xlab(\"First result p-value\") +\n  ylab(\"Second result p-value\") +\n  geom_rect(\n    mapping = aes(\n      xmin = .0,\n      xmax = .025,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"blue\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0,\n      xmax = .025,\n      ymin = .025,\n      ymax = .05\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = .025,\n      xmax = .05,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"green\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.05,\n      xmax = 1,\n      ymin = 0,\n      ymax = .025\n    ),\n    fill = \"purple\",\n    alpha = .5\n  )   + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.0,\n      xmax = .025,\n      ymin = 0.05,\n      ymax = 1\n    ),\n    fill = \"brown\",\n    alpha = .5\n  ) + \n  geom_rect(\n    mapping = aes(\n      xmin = 0.025,\n      xmax = 1,\n      ymin = 0.025,\n      ymax = 1\n    ),\n    fill = \"black\",\n    alpha = .5\n  ) \n\n\n\n\n\nRed: “Both are false positives, but only the first test is <.025”, p = 0.000625, i.e. takes 0.000625 of the total area of the above figure\nGreen: “Both are false positives, but only the second test is <.025”, p = 0.000625, i.e. takes 0.024375 of the total area of the above figure\nBlue: “Both are false positives and both < .025”, p = 0.024375, i.e. takes 0.000625 of the total area of the above figure\nBrown: “Only the first test is a false-positive”, p = 0.02375, i.e. takes 0.950625 of the total area of the above figure\nPurple: “Only the second test is a false-positive”, p = 0.02375, i.e. takes 0.950625 of the total area of the above figure\nGrey: “Neither test is a false-positive”, p = 0.02375, i.e. takes 0.950625 of the total area of the above figure"
  },
  {
    "objectID": "multipleTesting/fwer.html#holm-šidák-method",
    "href": "multipleTesting/fwer.html#holm-šidák-method",
    "title": "Family-Wise Error (R)",
    "section": "Holm-Šidák method",
    "text": "Holm-Šidák method\nCombining the principles above, you can apply Holm’s philosophy\n\\[\n\\frac{.05}{m + 1 - k}\n\\]\nto the Šidák correction:\n\\[\n\\alpha_{Šidák} = 1 - (1-.05)^{(1/m)}\n\\]\nto get\n\\[\n\\alpha_{h-s} = 1 - ( 1 - .05)^{1/(m + 1-k)}\n\\]\n\n\\(\\alpha_{h-s}\\) refers to the Holm-Šidák alpha value\n\\(m\\) refers to the number of tests\n\\(k\\) refers to the rank of the current test you are calculating the threshold for\n\nAn advantage of using this Holm-Šidák method is that it will maintain a FWER of .05 in the same way that the Šidák method does. This is because the FWER reflects the likelihood of at least one false-positive (as described above), and the Šidák method maintains a FWER of .05.\n\nQuestion 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof fwer1_response = Inputs.radio(['True','False']);\ncorrect_fwer1 = 'False';\nfwer_1color = {\n  if(fwer1_response == correct_fwer1){\n    return 'blue';\n  } else {\n    return 'red';\n  }   \n}\nfwer1_result = {\n  if(fwer1_response == correct_fwer1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof fwer2_response = Inputs.radio(['Bonferroni','Šidák']);\ncorrect_fwer2 = 'Šidák';\nfwer2_result = {\n  if(fwer2_response == correct_fwer2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n  \n\n\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n!correct_vector == \"correct\" \n\n[1] FALSE  TRUE FALSE\n\n# which is the same as\n!(correct_vector == \"correct\")\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "rBasics/fundamentals.html",
    "href": "rBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "R allows you to complete calculations, so lets start with that. Type into your markdown or script\nOften, you can use R as a calculator, for example, if you want to know what two people’s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nIt’s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g. how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. “my object” would be an invalid object name.\nObject names are case-sensitive, so if your object is called “MyObject” you cannot refer to it as “myobject”.\n“.” and “_” are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou can’t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing “t.te” and pressing the tab will give you “t.test”\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you don’t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe “c” above is short for “combine” as you’re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the “typeof” function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (“heights”) is a “double” vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers can’t always be integers. Strings are identified as “character” objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the “mpg” dataframe. To look at it, just type in it’s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below won’t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see what’s in the “manufacturer” column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about “intelligent copy and paste” to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming you’ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code we’ve copied so that you can apply it to your own scripts. There’s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\nggplot(             # R will keep looking at your code until all the open brackets have been closed)\n  mpg,              #\n  aes(              #\n    displ,          #\n    hwy,            #\n    colour = class  #\n  )\n) +                 # R will look to the next line if you end a line with +\ngeom_point()        #\n\n\n\n\nHere’s how I would comment it out:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    displ,          # x-axis\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\n\n\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    cty,            # x-axis - updated\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\n\n\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "rBasics/filetypes.html",
    "href": "rBasics/filetypes.html",
    "title": "Types of Scripts",
    "section": "",
    "text": "Welcome to using R. This subsection will explain the basics about R. First, lets discuss the different ways to write R code, as there are (at least) 5:\n\ntype it directly into the console (generally not recommended)\nsave it as a script (better) note that script can refer to any of the below, but in this case is being used to describe a script that doesn’t generate a notebook\nsave it as an R Markdown (better still) - this allows you to make beautiful documents\nsave it as an R Notebook (arguably better than R Markdown) - this allows you to make beautiful documents, and is quicker\n\n\nThe Console\nAt the bottom left of RStudio you should have a console that looks something like what’s highlighted in red below:\n\n\n\nconsole\n\n\nYou can type straight into the console, to get a result. You can scroll through your previous commands by pressing the up arrow in the console. Each time code is run in the console it updates the environment in the top right of R-Studio:\n\n\n\nenvironment\n\n\n\n\nScripts\nThe word “script” can be interpreted specifically, to refer to a type of R file that includes a lot of code, or generally to refer to any file that includes both R code and code that allows you make a nice looking report. In this subsection, we will be focusing on “script” as a particular type of file. To create a script, click on File -> New File -> R Script\n\n\n\nnewScript\n\n\nYou will then be shown a blank script, in which you can write a series of functions, and then run them. To run the lines of code, select a line, and then press CTRL-ENTER, or highlight a chunk of code and then press CTRL-ENTER. In either case, the code will be sent to the console and run there.\nAn advantage of a script over just using the console is that you can analyse your data in both structured and complex ways which is difficult if you are typing code directly into the console.\n\n\nR Markdown\nAs highlighted above, R Markdown is a type of “script” in the general sense of the word, but allows you to create beautiful .html notebooks (.html files are what internet pages are based on). You are in fact reading an example of what can be produced by R Markdown (and R Notebooks). To make an R Markdown file, click on File -> New File -> R Markdown. You will be asked for a title, author and what output you would like. I would suggest “first markdown”, your name and “html” as the respective answers. You should then see something like:\n\n\n\nmarkdown\n\n\nThe following points apply to both R Markdown and R Notebooks\nIf you look above, you may notice that there are 2 types of code: Markdown (to write a nice looking report) and R (in grey chunks). I think these are well explained here: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf so I’ll just explain that R Markdowns run all the code in the chunks each time they generate the output file (e.g. html file). This is important to know, because R Notebooks do not run all the code in all the chunks when you generate them (see below for more on this).\n\n\nR Notebooks\nR Notebooks can be created by clicking on File -> New File -> R Notebook. They look quite similar to R Markdowns, but automatically generate the .html output each time you save the notebook. The output file will be a .nb.html file in the same folder as your notebook.\nVery importantly - the .nb.html file will be built based on what happened the last time you run each R chunk. If you never ran the R Chunk, then the nb.html file will not use the output from that chunk. This makes R Notebooks quicker than R Markdowns, because you don’t have to generate the output from scratch each time, as it will just use whatever was generated the last time the chunk was run. However, this means that there’s a risk your .html output will not be what you expect if you failed to run all of your chunks before saving your file. To address this risk (when you’ve finished editing your file), you can select the “run all” option.\n\n\n\nrunAllChunks"
  },
  {
    "objectID": "installing.html",
    "href": "installing.html",
    "title": "Installing R(Studio)",
    "section": "",
    "text": "Installing R\nDownload and install the following depending on your operating system:\n\nWindows: https://cran.r-project.org/bin/windows/base/\nMac: and select the R-x.x.x.pkg notarized and signed option\n\n\n\nInstalling RStudio\n\nhttps://www.rstudio.com/products/rstudio/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "href": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "title": "R-squared vs. Adjusted R-squared",
    "section": "",
    "text": "When completing a regression, there’s always a risk of “overfitting” the data, i.e. creating a model that includes predictors that have no meaningful association with the outcome variable. One reason that overfitting the data is a problem is that it is almost impossible for a predictor to have no association with an outcome variable. For that to happen you would need any data points that suggested a positive association between the outcome and the predictor to be equally balanced out by data points that suggest a negative association:\n\nRPython\n\n\n\nlibrary(ggplot2)\nno_association_df <- data.frame(\n  predictor = c(1,1,1,2,2,2,3,3,3),\n  outcome   = c(1,2,3,1,2,3,1,2,3)\n)\n\nggplot(no_association_df, aes(x = predictor, y = outcome)) + geom_point() + geom_smooth(method=lm, formula = 'y ~ x')\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sample participants in pairs\nno_association_df = {\n  'predictor': [1,1,1,2,2,2,3,3,3],\n  'outcome': [1,2,3,1,2,3,1,2,3]\n}\n\nno_association_df = pd.DataFrame(no_association_df)\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\ncoef = np.polyfit(no_association_df[\"predictor\"],no_association_df[\"outcome\"],1)\npoly1d_fn = np.poly1d(coef) \n# poly1d_fn is now a function which takes in x and returns an estimate for y\n\nplt.plot(no_association_df[\"predictor\"],no_association_df[\"outcome\"], 'ko', no_association_df[\"predictor\"], poly1d_fn(no_association_df[\"predictor\"]), '-b') #'--k'=black dashed line, 'yo' = yellow circle marker\n\n# add title on the x-axis\nplt.xlabel(\"predictor\")\n\n# add title on the y-axis\nplt.ylabel(\"outcome\")\n\nplt.show()\n\n\n\n\n\n\n\nUnrealistic Data Distribution\n\n\nFig. X. An example of how unrealistically balanced your data needs to be to find no association. As this (almost) never happens in reality, samples are biased towards finding associations between predictor and outcome variables even when there aren’t any in the population. For example, let’s generate some random data, and see what R-Values we find. Remember, random data really shouldn’t have any association between predictor and outcome variables.\n\nRPython\n\n\n\nrandom_df = data.frame(\n  random_iv_1 = runif(100),\n  random_iv_2 = runif(100),\n  random_iv_3 = runif(100),\n  random_dv = runif(100)\n)\nrmarkdown::paged_table(random_df)\n\n\n\n  \n\n\n\n\n\n\nimport random\nfrom tabulate import tabulate\n\nrandom_df = {\n    'random_iv_1': np.random.random_sample(size = 100),\n    'random_iv_2': np.random.random_sample(size = 100),\n    'random_iv_3': np.random.random_sample(size = 100),\n    'random_dv': np.random.random_sample(size = 100)\n}\n \n# convert it to a data frame\nrandom_df = pd.DataFrame(random_df)\n\n# print the table\nprint(tabulate(random_df[:10], headers=random_df.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\n\nRPython\n\n\n\nrandom_lm <- lm(random_dv ~ random_iv_1, random_df)\nrandom_summary <- summary(random_lm)\nrandom_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1, data = random_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4946 -0.2820 -0.0021  0.2668  0.5181 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.50543    0.06388   7.912  3.9e-12 ***\nrandom_iv_1 -0.04103    0.11429  -0.359     0.72    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3184 on 98 degrees of freedom\nMultiple R-squared:  0.001313,  Adjusted R-squared:  -0.008878 \nF-statistic: 0.1288 on 1 and 98 DF,  p-value: 0.7204\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_lm = smf.ols(formula='random_dv ~ random_iv_1', data=random_df).fit()\nrandom_lm.summary()\n\n\n\n\n\n\n\nTable\n\n\nNote that the above output is generated each time this page is rendered (generated), and so by chance may happen to look like the random predictor is significant. If so, there’s a 95% chance that this predictor will not be significant next time the page is rendered.\nLooking at the output above, we can see that 0.1313% of the variance of random_dv was explained by random_iv_1 before correction. Considering that these were randomly generated numbers, that’s 0.1313% too much. However, the Adjusted R-squared is only -0.0089. Note that Adjusted R-squared can be a negative number, and a negative number suggests that based on the sample, the predictor(s) has(/have) no association with the outcome variable in the population.\nA formula for the adjusted r-squared is:\n\\[\n\\bar{R^2} = 1-\\frac{SS_{res}/df_{res}}{SS_{tot}/df_{tot}}\n\\] \\(\\bar{R^2}\\) is the Adjusted R-Squared \\(SS_{total}\\) is the Sum of Squares of the total (i.e. how much total variance there is around the mean to explain) \\(SS_{res}\\) is the Sum of Squares of the residuals (i.e. how much isn’t explained by the model) \\(df_{total}\\) is the Degrees of Freedom of the total. This is the number of data points - 1, so is N - 1 \\(df_{res}\\) is the Degrees of Freedom of the residuals. The degrees of freedom for the residuals takes into account the number of data points and the number of predictors, and so is N - 1 - 1\nLet’s use the above formula to manually calculate the Adjusted R Squared\n\nRPython\n\n\n\nss_res <- sum(random_lm$residuals^2)\nss_total <- sum(\n  (\n    random_df$random_dv - mean(random_df$random_dv)\n  )^2\n)\n\n\nrandom_r_square = ss_total - ss_res\ndf_total <- length(random_lm$residuals) - 1\ndf_res <- length(random_lm$residuals) -\n  1 - # remove 1 from the number of data points\n  1 # remove another 1 to reflect there being 1 predictor\nadjusted_random_r_square = 1 - (ss_res/df_res)/(ss_total/df_total)\n\nadjusted_random_r_square\n\n[1] -0.008877712\n\n\n\n\n\nss_res = sum(random_lm.resid**2)\nss_total = sum((random_df[\"random_dv\"] - random_df[\"random_dv\"].mean())**2)\n\nrandom_r_square = ss_total-ss_res\ndf_total = len(random_lm.resid)-1\ndf_res = len(random_lm.resid)-1 - 1\n\nadjusted_random_r_square = 1-(ss_res/df_res)/(ss_total/df_total)\nadjusted_random_r_square\n\n\n\n\n-0.009973456268681513\nThe number above should match the Adjusted R-Squared from the multiple regression above. Let’s explore what happens when we have multiple predictors:\n\nRPython\n\n\n\nrandom_lm_multiple <- lm(random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, random_df)\nrandom_multiple_summary <- summary(random_lm_multiple)\nrandom_multiple_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, \n    data = random_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5252 -0.2615 -0.0324  0.2530  0.6246 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.7745     0.1092   7.093 2.24e-10 ***\nrandom_iv_1  -0.0978     0.1126  -0.868   0.3874    \nrandom_iv_2  -0.2159     0.1042  -2.073   0.0408 *  \nrandom_iv_3  -0.2486     0.1051  -2.366   0.0200 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3076 on 96 degrees of freedom\nMultiple R-squared:  0.08703,   Adjusted R-squared:  0.0585 \nF-statistic: 3.051 on 3 and 96 DF,  p-value: 0.03227\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_multiple_summary = smf.ols(formula='random_dv ~ random_iv_1 + random_iv_2 + random_iv_3', data=random_df).fit()\nrandom_multiple_summary.summary()\n\n\n\n\n\n\n\nTable\n\n\nTwo things to look for from the above: - The model with 3 predictors has higher (Multiple) R-Squared than the model with only 1 predictor. This reflects problems with over-fitting the model: the more predictors you include in your sample, the more variance in the outcome that will be explained by the predictors, even if those associations between the predictors are arbitrary (i.e. don’t reflect anything about the general population). - Adjusted R-squared values are less susceptible to this bias of overfitting the data (but is not completely invulnerable to it). All statistical tests are vulnerable to false positives and including Adjusted R-squared values.\nRemember, the adjusted r-square is necessary for us to make claims about the general population. If we just wanted to make a claim about our sample, we would just use the r-squared, as we don’t need to correct our estimate."
  },
  {
    "objectID": "GeneralLinearModels/mixedAnova.html",
    "href": "GeneralLinearModels/mixedAnova.html",
    "title": "Mixed ANOVA (incomplete)",
    "section": "",
    "text": "Let’s create data to allow us to compare between 2 years, and between Europe and Americas\n\nRPython\n\n\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 <- subset(\n  gapminder,   # the data set\n  year == 2002 & continent == \"Africa\" |\n  year == 2007 & continent == \"Africa\" |\n  year == 2002 & continent == \"Europe\" |\n  year == 2007 & continent == \"Europe\"\n)\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7759  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 = gapminder.loc[(gapminder['year'] == 2002) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2002) & (gapminder['continent'] == \"Europe\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Europe\") ]\n\nfrom statsmodels.formula.api import ols\n\nfit = ols('lifeExp ~ C(year) + C(continent)', data=gapminder_2_by_2).fit() \n\nfit.summary()\n\nlm_4_continents_aov_table = sm.stats.anova_lm(fit, typ=2)\nlm_4_continents_aov_table\n\n\n\n\nmanual calculation of f-value for 2 x 2\n\nRPython\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\noverallMeanLifeExp = mean(gapminder_2_by_2$lifeExp)\ntotalVar = sum((gapminder_2_by_2$lifeExp - mean(gapminder_2_by_2$lifeExp))^2)\ngapminder_2_by_2 %>%\n  group_by(continent, year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> year_continent_means\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nyear_continent_means\n\n# A tibble: 4 × 5\n# Groups:   continent [2]\n  continent  year mean_lifeExp countries betweenSS\n  <fct>     <int>        <dbl>     <int>     <dbl>\n1 Africa     2002         53.3        52     4396.\n2 Africa     2007         54.8        52     3094.\n3 Europe     2002         76.7        30     6033.\n4 Europe     2007         77.6        30     6866.\n\nsum(year_continent_means$betweenSS)\n\n[1] 20389.47\n\ntotalVar\n\n[1] 30311.89\n\nsum(year_continent_means$betweenSS)/totalVar\n\n[1] 0.6726556\n\n(sum(year_continent_means$betweenSS))/totalVar\n\n[1] 0.6726556\n\ndf_total <- length(gapminder_2_by_2$country) - 1\ndf_res <- length(gapminder_2_by_2$country) - \n  1 - #data points\n  3   # predictors\n\nss_res = totalVar - sum(year_continent_means$betweenSS)\n\n1 - (ss_res/df_res)/(totalVar/df_total)\n\n[1] 0.6665179\n\n##\n# break down of types of variance\n##\ncontinent_df <- gapminder_2_by_2 %>%\n  group_by(continent) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(continent_df$betweenSS)\n\n[1] 20318.97\n\nyear_df <- gapminder_2_by_2 %>%\n  group_by(year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(year_df$betweenSS)\n\n[1] 67.79278\n\n##\n# interaction\n##\nsum(year_continent_means$betweenSS) - sum(continent_df$betweenSS) - sum(year_df$betweenSS)\n\n[1] 2.70036\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n20319 +\n68 +\n3\n\n[1] 20390\n\n\n\n\n\nfrom siuba import group_by, summarize, _\n\noverallMeanLifeExp = gapminder_2_by_2['lifeExp'].mean()\n\ntotalVar = sum((gapminder_2_by_2['lifeExp'] - gapminder_2_by_2['lifeExp'].mean())**2)\n\n\nyear_continent_means=(gapminder_2_by_2\n  >> group_by(_.continent, _.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_continent_means['betweenSS'] = year_continent_means['countries'] * ((overallMeanLifeExp - year_continent_means['mean_lifeExp'])**2)\n\nprint(tabulate(year_continent_means, headers=year_continent_means.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nsum(year_continent_means['betweenSS'])\n\ntotalVar\n\nsum(year_continent_means['betweenSS'])/totalVar\n\ndf_total = len(gapminder_2_by_2['country']) - 1\ndf_res = len(gapminder_2_by_2['country']) - 1 - 3\nss_res = totalVar - sum(year_continent_means['betweenSS'])\n1 - (ss_res/df_res)/(totalVar/df_total)\n\ncontinent_df=(gapminder_2_by_2\n  >> group_by(_.continent)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\ncontinent_df['betweenSS'] = continent_df['countries'] * ((overallMeanLifeExp - continent_df['mean_lifeExp'])**2)\nsum(continent_df['betweenSS'])\n\nyear_df=(gapminder_2_by_2\n  >> group_by(_.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_df['betweenSS'] = year_df['countries'] * ((overallMeanLifeExp - year_df['mean_lifeExp'])**2)\nsum(year_df['betweenSS'])\n\nsum(year_continent_means['betweenSS']) - sum(continent_df['betweenSS']) - sum(year_df['betweenSS'])\n\nprint(tabulate(gapminder[:10], headers=gapminder.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n20319 +68 +3"
  },
  {
    "objectID": "GeneralLinearModels/mixedAnova.html#way-anova",
    "href": "GeneralLinearModels/mixedAnova.html#way-anova",
    "title": "Mixed ANOVA (incomplete)",
    "section": "3 way ANOVA",
    "text": "3 way ANOVA\n\nRPython\n\n\n\ngapminder_2_by_2$pop_split = \"high\"\ngapminder_2_by_2$pop_split[gapminder_2_by_2$pop < median(gapminder_2_by_2$pop)] = \"low\"\n\ngapminder_2_by_2 %>%\n  group_by(continent, year, pop_split) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> three_way_summary\n\n`summarise()` has grouped output by 'continent', 'year'. You can override using\nthe `.groups` argument.\n\nsum(three_way_summary$betweenSS)\n\n[1] 20403.63\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7759  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n                                                  Df Sum Sq Mean Sq F value\nfactor(year)                                       1     68      68   1.067\nfactor(continent)                                  1  20319   20319 319.911\nfactor(pop_split)                                  1     13      13   0.212\nfactor(year):factor(continent)                     1      3       3   0.046\nfactor(year):factor(pop_split)                     1      0       0   0.000\nfactor(continent):factor(pop_split)                1      0       0   0.007\nfactor(year):factor(continent):factor(pop_split)   1      0       0   0.000\nResiduals                                        156   9908      64        \n                                                 Pr(>F)    \nfactor(year)                                      0.303    \nfactor(continent)                                <2e-16 ***\nfactor(pop_split)                                 0.646    \nfactor(year):factor(continent)                    0.830    \nfactor(year):factor(pop_split)                    0.989    \nfactor(continent):factor(pop_split)               0.931    \nfactor(year):factor(continent):factor(pop_split)  0.988    \nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent) * factor(pop_split), \n    data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5189  -4.8868  -0.3127   3.1208  22.0864 \n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   52.96628\nfactor(year)2007                                               1.53805\nfactor(continent)Europe                                       23.52019\nfactor(pop_split)low                                           0.69131\nfactor(year)2007:factor(continent)Europe                      -0.59779\nfactor(year)2007:factor(pop_split)low                         -0.06377\nfactor(continent)Europe:factor(pop_split)low                  -0.26305\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low  0.07923\n                                                              Std. Error\n(Intercept)                                                      1.59392\nfactor(year)2007                                                 2.21201\nfactor(continent)Europe                                          2.60286\nfactor(pop_split)low                                             2.21201\nfactor(year)2007:factor(continent)Europe                         3.65535\nfactor(year)2007:factor(pop_split)low                            3.12825\nfactor(continent)Europe:factor(pop_split)low                     3.65535\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    5.16944\n                                                              t value Pr(>|t|)\n(Intercept)                                                    33.230  < 2e-16\nfactor(year)2007                                                0.695    0.488\nfactor(continent)Europe                                         9.036 5.91e-16\nfactor(pop_split)low                                            0.313    0.755\nfactor(year)2007:factor(continent)Europe                       -0.164    0.870\nfactor(year)2007:factor(pop_split)low                          -0.020    0.984\nfactor(continent)Europe:factor(pop_split)low                   -0.072    0.943\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low   0.015    0.988\n                                                                 \n(Intercept)                                                   ***\nfactor(year)2007                                                 \nfactor(continent)Europe                                       ***\nfactor(pop_split)low                                             \nfactor(year)2007:factor(continent)Europe                         \nfactor(year)2007:factor(pop_split)low                            \nfactor(continent)Europe:factor(pop_split)low                     \nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.97 on 156 degrees of freedom\nMultiple R-squared:  0.6731,    Adjusted R-squared:  0.6585 \nF-statistic: 45.89 on 7 and 156 DF,  p-value: < 2.2e-16\n\n20319 +\n68 +\n3 + \n  13\n\n[1] 20403\n\n20403.63\n\n[1] 20403.63\n\n\n\n\n\ngapminder_2_by_2['pop_split']=0\n\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']<np.median(gapminder_2_by_2['pop'])]=\"low\"\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']>=np.median(gapminder_2_by_2['pop'])]=\"high\"\n\nthree_way_summary = (gapminder_2_by_2\n  >> group_by(_.continent, _.year, _.pop_split)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nthree_way_summary['betweenSS'] = three_way_summary['countries'] * ((overallMeanLifeExp - three_way_summary['mean_lifeExp'])**2)\nsum(three_way_summary['betweenSS'])\n\n\n\nfit2 = ols('lifeExp ~ C(year) + C(continent) + C(pop_split)', data=gapminder_2_by_2).fit() \n\nfit2.summary()"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html",
    "href": "GeneralLinearModels/generalLinearModels.html",
    "title": "General Linear Models and Sum of Squares (R, Python)",
    "section": "",
    "text": "this_page = \"glm\""
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#what-are-general-linear-models",
    "href": "GeneralLinearModels/generalLinearModels.html#what-are-general-linear-models",
    "title": "General Linear Models and Sum of Squares (R, Python)",
    "section": "What are general linear models?",
    "text": "What are general linear models?\nGeneral linear models allow you to analyse data in which the dependent variable is continuous. For example, if you are analysing the height of a group of individuals, you might use one of the following analyses:\n\nt-test, comparisons between two conditions e.g. are males taller than females?\nregression, one or more predictors of a single outcome e.g. does foot size, weight etc. predict height? (Note that correlations are equivalent to a regression with a single predictor)\nANOVA, comparisons between 3 or more conditions or between multiple categorical factors, e.g. are there differences in height between sexes and nationalities?\n\nLinear refers to the dependent variable being continuous.\nGeneral refers to the fact that the independent variables can both be continuous (e.g. regression) or categorical (e.g. t-test or ANOVA).\nIn general linear models all analyses involve creating a model, and capturing what is and isn’t explained by the model (i.e. the error of the model). All analyses in general linear models can be formulated as:\n\\[\nData = Model + Error\n\\]\nData: The dependent variable in your analysis\nModel: A model which predicts a phenomenon. This could be multiple independent variables.\nError: What data isn’t explained by the model."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "href": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "title": "General Linear Models and Sum of Squares (R, Python)",
    "section": "Dummy vs. effect coding for categorical variables in a model",
    "text": "Dummy vs. effect coding for categorical variables in a model\nGeneral Linear Models need numerical values for the predictors. As categorical variables (e.g. Sex) don’t have a numeric value by default, we have to substitute the categories with numbers:\n\nEffect coding can be used when you have a binary categorical variable, and you allocate one level 1 and the other -1. For example, you could allocate all females the score 1, and all non-female participants -1. A disadvantage of this approach is that it works best when you have binary categorical variable, but doesn’t work as well when you have 3 or more levels. For example, coding female, male and non-binary sex doesn’t work well with effect coding.\nDummy coding involves allocating a 1 if someone is in a cateogory, and 0 if they are outside of the category. For example, you could allocate 1 to all your female participants, and 0 to all participants who aren’t female to a variable “sex_female”. An advantage of this approach is that you have flexibility to have more than 2 levels, such as having “sex_female”, “sex_male” and “sex_nonbinary” as variables that are all either 1 or 0."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "href": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "title": "General Linear Models and Sum of Squares (R, Python)",
    "section": "Mean as the simplest model of data",
    "text": "Mean as the simplest model of data\nIf you want to estimate what someone’s life expectancy would be in 2007, you could look at the mean life expectancy using the gapminder data. In terms of how this corresponds to the above model:\n\\[\nData = Model + Error\n\\]\n\\[\nestimatedLifeExpectancy = mean(lifeExpectancy) + Error\n\\]\n\nRPython\n\n\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].mean()\n\n67.00742253521126\n\n\n\n\\[\nestimatedLifeExpectancy = 67.01 + Error\n\\]\nWhich could be visualised as:\n\nRPython\n\n\n\nlibrary(ggplot2)\nggplot(\n  gapminder_2007, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_jitter() +\n  geom_hline(yintercept = mean(gapminder_2007$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Plot of mean lifeExp and residuals\n\n\n\n\n\n\n\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], gapminder_2007[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\nplt.show()\n\n\n\n\nFig. 1. Plot of mean lifeExp and residuals\n\n\n\n\n\nIn English, the above model and figure allow you to predict that anyone’s life expectancy will be 67 years. However, as you can also see, there’s a huge amount of error, i.e. variance in life expectancy that is not explained by the model. These errors can be squared and summed to give the sum of squares, a statistic of how much error there is around the model:\n\\[\nSS = \\sum(Y_i-\\bar{Y})^2\n\\]\nWhich can be visualised as follows:\n\nRPython\n\n\n\nggplot(\n  gapminder_2007, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 2: Plot of squared residuals\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], (gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, color='black', s=10)\n\n# vertical lines\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\n\n# show plot\nplt.show()\n\n\n\n\nFig. 2. Plot of squared residuals\n\n\n\n\n\nYou can inspect fig. 1. and fig. 2. to see how much error is associated with each data point compared to the model (mean). Fig. 2. is positive because it is the distance of the data-point from the mean squared. If you added together all the squares (pink lines) in fig. 2. that would give you the sum of squares.\nAs you may have guessed, it is possible to have more precise models that have less error, and thus a smaller sum of squares. The sum of squares around the mean is also the total sum of squares, and the total variance. When we calculate the proportion of the variance that a model explains, we are comparing it to this variance around the mean.\nLet’s explore those possibilities now in the following pages:\n\nt-tests for comparisons between two groups of participants or two conditions within participants\none-way ANOVAs for comparisons between 2 or more groups of participants\nrepeated-measures ANOVAs for comparisons between 2 or more groups of conditions within participants\nmixed-effect ANOVAs for analyses with at least 1 between-subject factor and 1 within-subject factor\nANCOVAs for analyses in which you control for a covariate."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html",
    "href": "GeneralLinearModels/TTests.html",
    "title": "T-Tests (R, Python incomplete)",
    "section": "",
    "text": "In this website you can choose to expand or shrink the page to match the level of understanding you want."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "href": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "title": "T-Tests (R, Python incomplete)",
    "section": "One-sample t-tests",
    "text": "One-sample t-tests\nOne-sample t-tests test whether your sample’s values are significantly higher or lower than a pre-specified value (\\(\\mu\\)). You can do this by seeing how big the difference is between the mean in your sample (\\(\\bar{x}\\)) and the \\(\\mu\\), when taking into account the general variance in your data and the number of participants you have.\n\n\n\n\n\n\nFormula for a one-sample t-test\n\n\n\n\n\nOne sample t-tests try to explain whether variance of data is better explained around one specific value (sample mean \\(\\bar{x}\\)) compared to another (previously assumed value \\(\\mu\\)), when compared to the general variance in the data ( \\(\\sigma/\\sqrt{N}\\) explained more below).\n\\[\nt_{oneSample}=\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{N} }\n\\]\nFor example, imagine that you wanted to test whether life expectancy is higher than 55 across the world:\n\nYour \\(\\mu\\) would be 55. This can be thought of as the assumed population mean that we want to use our sample to test.\nYour \\(\\bar{x}\\) would be the sample mean.\n\n\n\n\n\n\n\n\n\n\nAn example of a one-sample t-test\n\n\n\n\n\nLet’s visualise these values using gapminder data from 2007:\n\nRPython\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007\n)\nggplot(gapminder_2007, aes(x=year,y=lifeExp)) + \n  geom_jitter() + \n  xlab(\"\") + \n  theme(axis.text.x = element_blank()) +\n  theme(axis.ticks.x = element_blank()) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = 55,\n      yend = 55,\n      color = \"Mu\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = mean(lifeExp),\n      yend = mean(lifeExp),\n      color = \"Sample Mean\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n# Create the plot\nplt.figure(figsize=(8, 4))\n\n# create the scatterplot with some jitter\nsns.stripplot(x=\"year\", y='lifeExp', data=gapminder_2007, dodge=True, jitter=0.5)\n\n# add an horizontal line for Mu\nplt.axhline(y=55, color='r', linestyle='-', label='Mu')\n\n# add an horizontal line for the mean of 'lifeExp'\nplt.axhline(y=gapminder_2007['lifeExp'].mean(), color='g', linestyle='-', label='Sample Mean')\n\n# remove the label on the x-axis\nplt.xlabel(\"\")\n\n# remove the tick on the x-axis\nplt.xticks([])\n\n# add the legend \nplt.legend()\n\n# show the plot\nplt.show()\n\n\n\n\n\nScatterplot with Mu and Sample Mean of ‘Life expectancy’\n\n\n\n\n\nSo the t-value will reflect how significantly higher or lower the sample mean (\\(\\bar{x}\\)) is than the assumed population mean (\\(\\mu\\)). Let’s apply the data from our data to the general formula:\n\\[\nt_{oneSample}=\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{N}} = \\frac{67.00742-55}{12.07302/\\sqrt{142}} = 11.85163\n\\]\nLet’s confirm that we get the same t-value using a function:\n\nRPython\n\n\n\n\nCode\n# t-statistic\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\n\n\n\n\nCode\nfrom scipy import stats\n# Perform a t-test\nt_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 11.851628442323024\np-value: 6.463174215427706e-23\n\n\n\nWe can see that the t-value we calculated is the same as calculated by functions (once you take into account rounding numbers). You may have noticed that there is also a p-value associated with t-tests that is very small, suggesting that it is very unlikely that our sample would have such high life expectancy if the \\(\\mu\\) was correct, i.e. if the life expectancy across countries was actually 55 years old. So, the next question is how do we calculate p-values like this using t-values?"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "title": "T-Tests (R, Python incomplete)",
    "section": "Paired samples t-tests",
    "text": "Paired samples t-tests\nPaired samples t-tests can be approached like 1-sample t-tests, but you first of all need to collapse the data to have a single variable to compare to a \\(\\mu\\) of zero. Let’s do this for gapminder data, comparing life expectancy between 2002 and 2007:\n\nRPython\n\n\n\n\nCode\ngapminder_2002_2007_life_exp <- gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]\nt.test(gapminder_2002_2007_life_exp, mu = 0)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2002_2007_life_exp\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean of x \n   1.3125 \n\n\n\n\n\n\nCode\ngapminder_2002_2007_life_exp = life_exp_2007.reset_index(drop=True) - life_exp_2002.reset_index(drop=True)\nt_statistic, p_value = stats.ttest_1samp(gapminder_2002_2007_life_exp, popmean=0)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 14.664513524875451\np-value: 3.738316746290281e-30\n\n\n\nThe above suggests that life expectancy was significantly different. Let’s see if we get the exact same value when we use a paired t-test in R:\n\nRPython\n\n\n\n\nCode\nt.test(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002], paired=T)\n\n\n\n    Paired t-test\n\ndata:  gapminder$lifeExp[gapminder$year == 2007] and gapminder$lifeExp[gapminder$year == 2002]\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean difference \n         1.3125 \n\n\n\n\n\n\nCode\n# Filter data for the year 2007\nlife_exp_2007 = gapminder[gapminder['year'] == 2007]['lifeExp']\n\n# Filter data for the year 2002\nlife_exp_2002 = gapminder[gapminder['year'] == 2002]['lifeExp']\n\n# Perform a paired t-test\nt_statistic, p_value = stats.ttest_rel(life_exp_2007, life_exp_2002)\n\nprint(\"T-statistic:\", t_statistic)\nprint(\"P-value:\", p_value)\n\n\nt-statistic: 14.664513524875451\np-value: 3.738316746290281e-30\n\n\n\nLooks identical. Let’s compare formulas to see why this is ( \\(\\mu\\) = 0 , and so it isn’t written) :\n\\[\nt_{paired} = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sigma_{pooled}/\\sqrt{N}} = \\frac{\\bar{x_3}}{\\sigma_{pooled}/\\sqrt{N}}\n\\]\nWhere\n\n\\(\\bar{x_1}\\) is the mean of condition 1\n\\(\\bar{x_2}\\) is the mean of condition 2\n\\(\\bar{x_3}\\) is the mean of the result you get when you subtract condition 2 from condition 1 for each participant, i.e. \\(mean(x_1-x_2)\\).\n\\[\n\\sigma_{pooled}  = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}} OR \\frac{\\sum(x_1 - x_2)^2}{N-1}\n\\] One way effectively gets the average of the standard deviations of condition and 1. The second way gets the standard deviation of the differences between conditions 1 and 2. Both give you the same outcome.\n\\(N\\) is the number of participants\n\nYou can rewrite the above formula to compare \\(\\bar{x_3}\\) to \\(\\mu\\), as we know \\(\\mu\\) is zero, which would make this formula (effectively) identical to the one above for one-sample t-tests:\n\\[\n\\frac{\\bar{x_3} - \\mu}{\\sigma_{pooled}/\\sqrt{N}}\n\\]\nThe following explains how to calculate F if analysing your data as a model. This isn’t necessary if you just want to complete a t-test, but does help clarify how t-tests overlap with ANOVAs and regressions.\n\n\n\n\n\n\nF and value\n\n\n\nTo calculate the F value we will use the following formula:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}\n\\]\nSo the question is, what is our model, and what is explained and what is not explained by it. Our model is that life expectancy is explained by year. We predict that any difference between 2002 and 2007 in life expectancy in any country’s is the mean life expectancy difference across all countries. The sum of squares thus is the mean difference between 2007 and 2002’s life expectancy multiplied by the number of countries:\n\\[\nSS_{explained} = \\sum_{country}{{((\\bar{lifeExp}_{2007} - \\bar{lifeExp}_{2002}})^2)} = ((\\bar{lifeExp}_{2007} - \\bar{lifeExp}_{2002})^2) * N_{countries}\n\\]\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nCode\ngapminder %>% \n  filter(year == 2007 | year == 2002) -> gapminder_2007_2002\nexplained_ss = ((mean(gapminder_2007_2002$lifeExp[gapminder_2007_2002$year == 2002]) - mean(gapminder_2007_2002$lifeExp[gapminder_2007_2002$year == 2007]))^2) * length(unique(gapminder_2007_2002$country))\nexplained_ss\n\n\n[1] 244.6172\n\n\n\\[\nSS_{unexplained} = SS_{total} - SS_{explained}\n\\]\nSo we need to calculate the total Sum of Squares, which is all variance between 2002 and 2007 for each country (squared):\n\\[\nSS_{total} = \\sum{((lifeExp_{2007} - lifeExp_{2002})^2)}\n\\]\n\n\nCode\n# restructuring data to simplify processing\ngapminder_2007_2002 %>% \n  select(country, year, lifeExp) %>% \n  pivot_wider(names_from=year, values_from = lifeExp)  -> gapminder_wide\n\ntotal_ss = sum((gapminder_wide[[\"2002\"]] - gapminder_wide[[\"2007\"]])^2)\ntotal_ss\n\n\n[1] 405.0048\n\n\nTo calculate the unexplained variance, let’s subtract the explained from total sum of squares\n\n\nCode\nunexplained_ss = total_ss - explained_ss\nunexplained_ss\n\n\n[1] 160.3876\n\n\nNow we can complete the above formula taking into account the explained degrees of freedom is 1 (2 levels -1 = 1), and the unexplained degrees of freedom is 141 (142 countries - 1 = 141):\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{244.6172/1}{160.3876/141} = 215.048\n\\]\nLet’s check if this is the same value we get using a function:\n\n\nCode\nez::ezANOVA(gapminder_2007_2002, within = year, dv= lifeExp, wid = country)$ANOVA$F\n\n\nWarning: \"year\" will be treated as numeric.\n\n\nWarning: There is at least one numeric within variable, therefore aov() will be\nused for computation and no assumption checks will be obtained.\n\n\n[1] 215.048"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "title": "T-Tests (R, Python incomplete)",
    "section": "Independent Samples t-tests",
    "text": "Independent Samples t-tests\n\nANOVA approach\nT-tests are restricted to comparisons between 2 conditions/groups, so we will restrict the Gapminder data to allow a comparison between 2 continents. To see if life expectancy was different if you are born in Europe compared to the Americas, let’s first check what the sum of squares is when you just use the mean as the model of life expectancy across these contents (so we’re not separating by continent yet):\n\nRPython\n\n\n\n\nCode\ngapminder_americas_europe <- subset(\n  gapminder_2007,   # the data set\n  continent == \"Europe\" | continent == \"Americas\"\n)\n\nggplot(\n  gapminder_americas_europe, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_point() +\n  geom_hline(yintercept = mean(gapminder_americas_europe$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 1: The errors around the mean of life expectancy across Europe and American countries\n\n\n\n\n\n\n\n\nCode\ngapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == \"Europe\") | (gapminder_2007['continent'] == \"Americas\")]\n\ngapminder_americas_europe[\"lifeExp_rank\"] = gapminder_americas_europe[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], gapminder_americas_europe[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_americas_europe[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=gapminder_americas_europe[\"lifeExp\"], ymax=gapminder_americas_europe[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"lifeExp\")\n\nplt.show()\n\n\n\n\n\nFig. 3. The errors around the mean of life expectancy across Europe and American countries.\n\n\n\n\n\nOnce we square the errors in the pink lines above, we’ll get the squares:\n\nRPython\n\n\n\n\nCode\nggplot(\n  gapminder_americas_europe, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\nsum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\n\n\n[1] 953.4478\n\n\n\n\n\nFigure 2: The squared errors around the mean of life expectancy across Europe and American countries\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], (gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, color='black', s=10)\n# only one line may be specified; full height\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\nplt.show()\n\nsum((gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2)\n\n\n\n\n\nThe squared errors around the mean of life expectancy across Europe and American countries\n\n\n953.4477649818183\n\n\n\nAnd when you add all of these together:\n\\[\nSumOfSquares = \\sum(Y_i-\\bar{Y})^2 = 953.4478\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#calculating-p-values-from-t-distributions",
    "href": "GeneralLinearModels/TTests.html#calculating-p-values-from-t-distributions",
    "title": "T-Tests (R, Python incomplete)",
    "section": "Calculating p-values from t-distributions",
    "text": "Calculating p-values from t-distributions\nSimilar to the normal distribution, we can calculate how likely it is that you would get a t-value or higher (or lower) by drawing a t-distribution that represents the likelihood of getting any t-value. For example, if you wanted to calculate the likelihood of getting a t-value of 2 or higher you could generate a figure as follows (note the area under the curve):\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(-4, 4)), aes(x)) +\n  stat_function(fun = dt,   args =list(df =5), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = dt, args =list(df =5), color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is a t-distribution bell-curved?\n\n\n\n\n\nA t-distribution reflects the likelihood of getting a particular t-value if the null hypothesis is correct. So the most likely t-value you will get is 0. It’s equally likely that you will get a positive and negative t-value, so the distribution will be symmetrical. The bell shape reflects how data clusters around a mean (same as for a normal distribution).\n\n\n\n\n\n\n\n\n\nDegrees of freedom, t-distributions and p-values\n\n\n\n\n\nA t-distribution is like a normal distribution (it’s a bell curve), but also takes into account the number of participants (or data points) using degrees of freedom (N-1). Here are the t-distributions you would get for 101, 21 and 6 participants:\n\nRPython\n\n\n\n\nCode\ncurve(dt(x, df=100), from=-4, to=4, col='green')\ncurve(dt(x, df=20), from=-4, to=4, col='red', add=TRUE)\ncurve(dt(x, df=5), from=-4, to=4, col='blue', add = TRUE) \n\n#add legend\nlegend(-4, .3, legend=c(\"df=5\", \"df=20\", \"df=100\"),\n       col=c(\"blue\", \"red\", \"green\"), lty=1, cex=1.2)\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t\n\n# Define the x values\nx = np.linspace(-4, 4, 1000)\n\n# Create density curves for different degrees of freedom\nplt.plot(x, t.pdf(x, df=100), color='green', label='df=100')\nplt.plot(x, t.pdf(x, df=20), color='red', label='df=20')\nplt.plot(x, t.pdf(x, df=5), color='blue', label='df=5')\n\n# Add a legend\nplt.legend(loc='upper left')\n\n# Set axis labels and plot title\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('T-Distribution Density Curves')\n\n# Show the plot\nplt.show()\n\n\n\n\n\nT-Distribution Density Curves’\n\n\n\n\n\nWe can see that the slopes get steeper the higher the \\(df\\) is (i.e. the more participants you have). This means that the results become more significant even with the same t-value. Not convinced? Let’s use a t-value of 2 to illustrate this. In the following figure you can see that the area under the curve for a t-value of 2 or more gets visually smaller the more participants/the higher the \\(df\\) value is:\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(-4, 4)), aes(x)) +\n  stat_function(fun = dt,   args =list(df =5), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = dt, args =list(df =5), color = \"blue\") +\n  stat_function(fun = dt, args =list(df =20), color = \"red\") +\n  stat_function(fun = dt, args =list(df =100), color = \"green\") \n\n\n\n\n\n\n\n\n\nCode\nfrom plotnine import ggplot, aes, stat_function, xlim, geom_area, geom_line\nfrom scipy.stats import t\n\nimport pandas as pd\n\n# Define the data frame with x values\ndf = pd.DataFrame({'x': [-4, 4]})\n\n# Create the  plot\n(\n    ggplot(df, aes(x='x'))\n    + stat_function(fun=lambda x: t.pdf(x, df=5), geom=\"area\", xlim=(2, 4), fill=\"blue\")\n    + stat_function(fun=lambda x: t.pdf(x, df=5), color=\"blue\")\n    + stat_function(fun=lambda x: t.pdf(x, df=20), color=\"red\")\n    + stat_function(fun=lambda x: t.pdf(x, df=100), color=\"green\")\n)\n\n\n\n\n\nFigure with the area under the curve for a t-value of 2’\n\n\n\n\n\nRemember, area under the curve IS the p-value, so the area under the curve for a t-value of 2 or above for each degrees of freedom is:\n\nRPython\n\n\n\n\nCode\n# area under the curve for 2 and above\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=5, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.05096974\n\n\nCode\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=20, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.02963277\n\n\nCode\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=100, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.02410609\n\n\n\n\n\n\nCode\nfrom scipy.stats import t\n\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 5\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.050969739414929105\n\n\nCode\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 20\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.02963276772328527\n\n\nCode\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 100\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.024106089365566796\n\n\n\nNote that if you wanted a 2-tailed test (i.e. you didn’t have an expected direction of your finding) you would double the area under the curve/p-value.\nSimilar principles apply for an F-distribution, described next."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#calculating-p-values-from-f-distributions",
    "href": "GeneralLinearModels/TTests.html#calculating-p-values-from-f-distributions",
    "title": "T-Tests (R, Python incomplete)",
    "section": "Calculating p-values from F-distributions",
    "text": "Calculating p-values from F-distributions\nAs F-values are based on the sum of squares, which are always positive, they cannot be negative. Also, F distributions can reflect more complex designs than t-distributions. Put together, the distributions are not symmetrical. However, the principle of calculating an F-value and comparing it to an F-distribution to get a p-value is the same as for t-values above. For example, if you wanted to calculate the likelihood of getting an F-value of 2 or higher you would calculate the area under the curve for an F-distribution (note the highlighted area):\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(0, 4)), aes(x)) +\n  stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = df, args =list(df1 =5, df2=1), color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegrees of freedom, f-distributions and p-values\n\n\n\n\n\nThe shape of F-distributions is influenced by having both explained and unexplained degrees of freedom. This means that they can have a variety of shapes to reflect these differences in degrees of freedom. First, let’s look at some distributions for designs with 2 conditions but differing numbers of participants:\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(0, 4)), aes(x)) +\n  # stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = df, args =list(df1 =5, df2=1), color = \"blue\") +\n  stat_function(fun = df, args =list(df1 =20, df2=1), color = \"red\") +\n  stat_function(fun = df, args =list(df1 =100, df2=1), color = \"green\") +\n  annotate(\"label\", x = 2, y = .4, label = \"df1 = 5, df2=1\", colour=\"blue\") +\n  annotate(\"label\", x = 2, y = .35, label = \"df1 = 20, df2=1\", colour=\"red\") +\n  annotate(\"label\", x = 2, y = .3, label = \"df1 = 100, df2=1\", colour=\"green\") +\n  xlab(\"F-Value\") +\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom plotnine import ggplot, aes, stat_function, annotate, labs\nfrom scipy.stats import f\n\n# Create a DataFrame with x values\ndf = pd.DataFrame({'x': [0, 4]})\n\n# Create the plot\n(\n    ggplot(df, aes(x='x'))\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=5, dfd=1),\n        color='blue'\n    )\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=20, dfd=1),\n        color='red'\n    )\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=100, dfd=1),\n        color='green'\n    )\n    + annotate(\n        \"label\", x=2, y=0.4, label=\"df1=5, df2=1\", color=\"blue\"\n    )\n    + annotate(\n        \"label\", x=2, y=0.35, label=\"df1=20, df2=1\", color=\"red\"\n    )\n    + annotate(\n        \"label\", x=2, y=0.3, label=\"df1=100, df2=1\", color=\"green\"\n    )\n    + labs(x=\"F-Value\", y=\"Density\")\n)\n\n\n\n\n\nF-Distribution Density Curves’\n\n\n\n\n\nThe shape is roughly the same, but shifts slightly as you get more people. What happens if we look at distributions where there are about 100 people but 2 or more conditions (df2 = 2 or more).\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(0, 4)), aes(x)) +\n  # stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = df, args =list(df1 =100, df2=1), color = \"blue\") +\n  stat_function(fun = df, args =list(df1 =100, df2=2), color = \"red\") +\n  stat_function(fun = df, args =list(df1 =100, df2=3), color = \"green\") +\n  annotate(\"label\", x = 2, y = .4, label  = \"df1 = 100, df2=1\", colour=\"blue\") +\n  annotate(\"label\", x = 2, y = .35, label = \"df1 = 100, df2=2\", colour=\"red\") +\n  annotate(\"label\", x = 2, y = .3, label  = \"df1 = 100, df2=3\", colour=\"green\") +\n  xlab(\"F-Value\") +\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\n\nCode\nfrom plotnine import ggplot, aes, stat_function, annotate, labs\n\n# Create a DataFrame with x values\ndf = pd.DataFrame({'x': [0, 4]})\n\n# Create the plot\n(\n    ggplot(df, aes(x='x'))\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=100, dfd=1),\n        color='blue'\n    )\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=100, dfd=2),\n        color='red'\n    )\n    + stat_function(\n        fun=lambda x: f.pdf(x, dfn=100, dfd=3),\n        color='green'\n    )\n    + annotate(\n        \"label\", x=2, y=0.4, label=\"df1=100, df2=1\", color=\"blue\"\n    )\n    + annotate(\n        \"label\", x=2, y=0.35, label=\"df1=100, df2=2\", color=\"red\"\n    )\n    + annotate(\n        \"label\", x=2, y=0.3, label=\"df1=100, df2=3\", color=\"green\"\n    )\n    + labs(x=\"F-Value\", y=\"Density\")\n)\n\n\n\n\n\nF-Distribution Density Curves’\n\n\n\n\n\nIn all cases, you get your p-value from calculating the area under the curve for that F-value or above.\n\n\n\n\n\n\n\n\n\n\n\n\nF-value approach (AKA General Linear Model (GLM) approach)\n\n\n\n\n\n\n\n\n\n\n\ninternal callout\n\n\n\n\n\nbeep bop\n\n\n\nTo calculate the F-Value, we want to create a model that captures explained variance and unexplained variance. This model will aim to explain any variance around the population mean (mu or \\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\n\\(y\\) is the data point value you are trying to predict.\n\\(\\bar{y}\\) is mean of all y data points. Note that for this formula you will always have the same predicted outcome (the mean).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the Mu (\\(\\mu\\)) and will also suggested that there is significant reason to reject the \\(\\mu\\) as the real mean of your population.\n\n\n\n\n\n\nFirst, we need to capture how much variance there is around the \\(\\mu\\), which we’ll do using sum of squares:\n\\[\nSS_{total} = \\sum(y_i-\\mu)^2\n\\]\n\n\\(y_i\\) is an individual data point\n\n\n\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - 55)^2)\n\n\n[1] 41025.16\n\n\n\n\n\n\nCode\n# import numpy \nimport numpy as np\n\n# calculate the squared dfference\nnp.sum((gapminder_2007['lifeExp'] - 55) ** 2)\n\n\n41025.157014\n\n\n\nSo we have to explain 41025.16 in variance around the \\(\\mu\\).\nOur model’s explanation of variance around the \\(\\mu\\) is the sample mean (\\(\\bar{y}\\)). For each data point the variance explained (from the \\(mu\\)) is the difference between \\(\\mu\\) and \\(\\bar{y}\\), and then that leaves the unexplained variance (from the \\(mu\\)) as the difference between the data point and the sample mean (\\(\\bar{y}\\)). If we visualise this for a single data point it should look something like:\n\nFor the above data point the variance (around the \\(\\mu\\)) explained is \\(\\bar{y} - \\mu = 67.007 - 55 = 22.007\\). However, we can see that the model isn’t perfect for this data point, and so there is a residual left over, i.e. a difference between the predicted value of 67.007 and the actual value of 58.04, meaning the unexplained variance is \\(67.007 - 58.04 = 8.967\\).\nYour explained variance by this model is thus a repetition of comparisons of the \\(\\mu\\) and \\(\\bar{y}\\) as many times as there are data points (and squaring this difference to make the values positive):\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n\n[1] 20473.3\n\n\n\n\n\n\nCode\nlen(gapminder_2007['lifeExp']) *( 55 - gapminder_2007['lifeExp'].mean())**2\n\n\n20473.303823352093\n\n\n\nAs described above, unexplained variance is the residuals around the sample mean, as this is variance that is not explained by the model. We can summarise what we get when we calculate the sum of squares of these residuals around the sample mean as follows:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n\n[1] 20551.85\n\n\n\n\n\n\nCode\nnp.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n\n20551.853190647882\n\n\n\nTo capture the effectiveness of the model we can calculate the F-value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nRPython\n\n\n\n\nCode\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n\n[1] 140.4611\n\n\n\n\n\n\nCode\n# Calculate the sum of squared differences between each value in 'lifeExp' and the mean\nss_between = len(gapminder_2007['lifeExp']) * (55 - gapminder_2007['lifeExp'].mean()) ** 2\n\n# Calculate the sum of squared differences within groups\nss_within = np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n# Calculate the degrees of freedom for between groups and within groups\ndf_between = 1\ndf_within = len(gapminder_2007['lifeExp']) - 1\n\n# Calculate the F-statistic\nf_value = (ss_between / df_between) / (ss_within / df_within)\n\nprint(f_value)\n\n\n140.46109673488004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANVOA vs. T-test formula\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} =\n\\]\n\\[\n\\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\n\\(T\\) is the t-value\n\\(F\\) is the f-value\n\\(SS_{exp}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexp}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{exp}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels).\n\nTo confirm, the formula for a one-sample t-test is just:\n\\[\nT = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\]\nF-values are squares of t-values, so let’s see if this is true here also:\n\nRPython\n\n\n\n\nCode\nsqrt(f_value)\n\n\n[1] 11.85163\n\n\nCode\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Calculate the square root of the F-value\nnp.sqrt(f_value)\n\n\n11.851628442323022\n\n\nCode\n# Perform a t-test\nt_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 11.851628442323024\np-value: 6.463174215427706e-23"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#glm-approach-optional",
    "href": "GeneralLinearModels/TTests.html#glm-approach-optional",
    "title": "T-Tests (R, Python incomplete)",
    "section": "GLM approach (optional)",
    "text": "GLM approach (optional)\nIf we were to more strictly conceptualise this as a linear model, then we need to create a model that predicts the life expectancy based on the continent. As continent is a categorical variable, we need to allocate numbers for each continent. This can either be dummy coding (1 for 0 for each continent) or effect coding (1 or -1 for each continent). Let’s go through both approaches.\n\nDummy coding\nIn this model, we will dummy code America as 1 and Europe as 0. This means that our model will tell us how much life expectancy increases/decreases when you live in America compared to Europe. A general linear model with 1 predictor is generally structured as follows:\n\\[\nY = a + B*X  + e\n\\]\nOr in relation to our variables\n\\[\nGDP = a + B_{continent} * X_{continent} + e =\n\\]\n\n\\(X\\) refers to the value of our dummy coded variable, which is either 0 (Europe) or 1 (Americas).\n\\(a\\) is the intercept, i.e. the value of \\(Y\\) when \\(X = 0\\). Thus, as \\(X=0\\) for Europe, \\(a\\) is the mean life expectancy in Europe. This will be clarified below.\n\\(B\\) is the gradient (AKA Coefficient AKA Beta) of the predictor. In our case, this will be the difference in means between Europe and the Americas, which will be further clarified below.\n\\(e\\) is the error, or residual (i.e. the difference between the predicted value and the actual value).\n\nSo it’s been claimed that \\(a\\) will be the mean life expectancy in Europe. We can see why this is the case by seeing what happens when \\(X=0\\), which follows our dummy coding of Europe as 0.\n\\[\nLifeExp = a + B_{continent} * 0 + e = a + e\n\\]\nSo predicting GDP for Europe is just \\(a\\) and whatever is not predicted by \\(a\\) is the error (\\(e\\)). The best prediction of GDP in Europe is calculating the mean of life expectancy in Europe. Thus \\(a\\) is the mean life expectancy in Europe.\nNow, to get the gradient (\\(B\\)), we need to calculate how much life expectancy goes up or down as \\(X\\) increases by 1.\n\n\nCode\ngapminder_americas_europe\n\n\n   contEffect                country continent year lifeExp       pop gdpPercap\n1           1                Albania    Europe 2007  76.423   3600523  5937.030\n2          -1              Argentina  Americas 2007  75.320  40301927 12779.380\n3           1                Austria    Europe 2007  79.829   8199783 36126.493\n4           1                Belgium    Europe 2007  79.441  10392226 33692.605\n5          -1                Bolivia  Americas 2007  65.554   9119152  3822.137\n6           1 Bosnia and Herzegovina    Europe 2007  74.852   4552198  7446.299\n7          -1                 Brazil  Americas 2007  72.390 190010647  9065.801\n8           1               Bulgaria    Europe 2007  73.005   7322858 10680.793\n9          -1                 Canada  Americas 2007  80.653  33390141 36319.235\n10         -1                  Chile  Americas 2007  78.553  16284741 13171.639\n11         -1               Colombia  Americas 2007  72.889  44227550  7006.580\n12         -1             Costa Rica  Americas 2007  78.782   4133884  9645.061\n13          1                Croatia    Europe 2007  75.748   4493312 14619.223\n14         -1                   Cuba  Americas 2007  78.273  11416987  8948.103\n15          1         Czech Republic    Europe 2007  76.486  10228744 22833.309\n16          1                Denmark    Europe 2007  78.332   5468120 35278.419\n17         -1     Dominican Republic  Americas 2007  72.235   9319622  6025.375\n18         -1                Ecuador  Americas 2007  74.994  13755680  6873.262\n19         -1            El Salvador  Americas 2007  71.878   6939688  5728.354\n20          1                Finland    Europe 2007  79.313   5238460 33207.084\n21          1                 France    Europe 2007  80.657  61083916 30470.017\n22          1                Germany    Europe 2007  79.406  82400996 32170.374\n23          1                 Greece    Europe 2007  79.483  10706290 27538.412\n24         -1              Guatemala  Americas 2007  70.259  12572928  5186.050\n25         -1                  Haiti  Americas 2007  60.916   8502814  1201.637\n26         -1               Honduras  Americas 2007  70.198   7483763  3548.331\n27          1                Hungary    Europe 2007  73.338   9956108 18008.944\n28          1                Iceland    Europe 2007  81.757    301931 36180.789\n29          1                Ireland    Europe 2007  78.885   4109086 40675.996\n30          1                  Italy    Europe 2007  80.546  58147733 28569.720\n31         -1                Jamaica  Americas 2007  72.567   2780132  7320.880\n32         -1                 Mexico  Americas 2007  76.195 108700891 11977.575\n33          1             Montenegro    Europe 2007  74.543    684736  9253.896\n34          1            Netherlands    Europe 2007  79.762  16570613 36797.933\n35         -1              Nicaragua  Americas 2007  72.899   5675356  2749.321\n36          1                 Norway    Europe 2007  80.196   4627926 49357.190\n37         -1                 Panama  Americas 2007  75.537   3242173  9809.186\n38         -1               Paraguay  Americas 2007  71.752   6667147  4172.838\n39         -1                   Peru  Americas 2007  71.421  28674757  7408.906\n40          1                 Poland    Europe 2007  75.563  38518241 15389.925\n41          1               Portugal    Europe 2007  78.098  10642836 20509.648\n42         -1            Puerto Rico  Americas 2007  78.746   3942491 19328.709\n43          1                Romania    Europe 2007  72.476  22276056 10808.476\n44          1                 Serbia    Europe 2007  74.002  10150265  9786.535\n45          1        Slovak Republic    Europe 2007  74.663   5447502 18678.314\n46          1               Slovenia    Europe 2007  77.926   2009245 25768.258\n47          1                  Spain    Europe 2007  80.941  40448191 28821.064\n48          1                 Sweden    Europe 2007  80.884   9031088 33859.748\n49          1            Switzerland    Europe 2007  81.701   7554661 37506.419\n50         -1    Trinidad and Tobago  Americas 2007  69.819   1056608 18008.509\n51          1                 Turkey    Europe 2007  71.777  71158647  8458.276\n52          1         United Kingdom    Europe 2007  79.425  60776238 33203.261\n53         -1          United States  Americas 2007  78.242 301139947 42951.653\n54         -1                Uruguay  Americas 2007  76.384   3447496 10611.463\n55         -1              Venezuela  Americas 2007  73.747  26084662 11415.806\n      t_fit\n1  77.64860\n2  73.60812\n3  77.64860\n4  77.64860\n5  73.60812\n6  77.64860\n7  73.60812\n8  77.64860\n9  73.60812\n10 73.60812\n11 73.60812\n12 73.60812\n13 77.64860\n14 73.60812\n15 77.64860\n16 77.64860\n17 73.60812\n18 73.60812\n19 73.60812\n20 77.64860\n21 77.64860\n22 77.64860\n23 77.64860\n24 73.60812\n25 73.60812\n26 73.60812\n27 77.64860\n28 77.64860\n29 77.64860\n30 77.64860\n31 73.60812\n32 73.60812\n33 77.64860\n34 77.64860\n35 73.60812\n36 77.64860\n37 73.60812\n38 73.60812\n39 73.60812\n40 77.64860\n41 77.64860\n42 73.60812\n43 77.64860\n44 77.64860\n45 77.64860\n46 77.64860\n47 77.64860\n48 77.64860\n49 77.64860\n50 73.60812\n51 77.64860\n52 77.64860\n53 73.60812\n54 73.60812\n55 73.60812\n\n\n\n\nEffect coding"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#t-tests",
    "href": "GeneralLinearModels/TTests.html#t-tests",
    "title": "T-Tests (R, Python incomplete)",
    "section": "T-tests",
    "text": "T-tests\nTo capture how large the effect is, there are slightly different calculations when doing a within subject design (1 or 2 conditions that all participants complete) vs. a between subject design. In all cases these are Cohen \\(d\\) calculations.\n\nOne Sample T-tests\nThe size of an effect is how big the difference is between the mean and the \\(\\mu\\) that you are comparing it to, compared to the standard deviation:\n\\[\nd = \\frac{\\bar{x} - \\mu}{SD(x)}\n\\]\nThe general benchmarks for how big or small a Cohen’s \\(d\\) value are as follows:\n\n.01 and below is very small (Sawilowsky 2009)\n.2 and below is small (Cohen 2013)\n.5 is medium (Cohen 2013)\n.8 is large (Cohen 2013)\n1.2 is very large (Sawilowsky 2009)\n2 is huge (Sawilowsky 2009)\n\n\n\nPaired sample T-tests\nThe size of an effect is how big the difference is between the conditions, compared to the standard deviation:\n\\[\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{SD(x_1-x_2)}\n\\]\nThis actually can be thought of the same as one-sample t-tests after you take the step to calculate the difference between each value in \\(x_1\\) and \\(x_2\\) to make \\(x_3\\) to compare to a \\(\\mu\\) of 0:\n\\[\nx_3 = x_1 - x_2\n\\] \\[\n\\mu = 0\n\\] \\[\nd = \\frac{\\bar{x_1}-\\bar{x_2}}{SD(\\bar{x_1}-\\bar{x_2})} = \\frac{\\bar{x_3}}{SD(x_3)} = \\frac{\\bar{x_3}-\\mu}{SD(x_3)}\n\\]\n\n\nIndependent Sample T-tests\nThe formula for this is somewhat similar to that for a paired samples t-test, but because you are comparing between groups of participants you can’t collapse the values between conditions because there are no pairings. So, you need to calculate the pooled standard deviation.\n\\[\nSD_{pooled} = \\sqrt{(SD(x_1)^2 + SD(x_1)^2)/2}\n\\]\n\\[\nd = \\frac{\\bar{x_1}-\\bar{x_2}}{SD_{pooled}}\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#f-tests",
    "href": "GeneralLinearModels/TTests.html#f-tests",
    "title": "T-Tests (R, Python incomplete)",
    "section": "F-tests",
    "text": "F-tests\nThe effect sizes for F-tests are based on how much variance is explained by the model, which sometimes is exactly the same as \\(r^2\\). Let’s review some of the calculations above to confirm this:\n\\[\nr^2 = \\frac{SS_{explained}}{SS_{total}} = \\frac{222.6202}{953.4478} = 0.2335\n\\]\nIf we run a linear model on the data we should also confirm the \\(r^2\\) is .2335:\n\nRPython\n\n\n\n\nCode\nsummary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\n\n\nCall:\nlm(formula = lifeExp ~ contEffect, data = gapminder_americas_europe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6921  -2.1364   0.4494   2.5671   7.0449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  75.6284     0.5028 150.416  < 2e-16 ***\ncontEffect    2.0202     0.5028   4.018 0.000186 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.713 on 53 degrees of freedom\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.219 \nF-statistic: 16.14 on 1 and 53 DF,  p-value: 0.0001864\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Create the model\nmodel = smf.ols('lifeExp ~ contEffect', data=gapminder_americas_europe)\nresults = model.fit()\n\n# Print the summary\nprint(results.summary())\n\n\n\n\n\nTable\n\n\n\n\n\nWe’ll use a function to confirm that this is the same for eta squared also:\n\nRPython\n\n\n\n\nCode\nlsr::etaSquared(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\n\n              eta.sq eta.sq.part\ncontEffect 0.2334896   0.2334896\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\n\n# Create the model\nmodel = smf.ols('lifeExp ~ contEffect', data=gapminder_americas_europe)\nresults = model.fit()\n\n# Calculate eta-squared\nanova_table = anova_lm(results)\neta_squared = anova_table['sum_sq'][0] / (anova_table['sum_sq'][0] + anova_table['sum_sq'][1])\n\nprint(\"Eta-squared (η²):\", eta_squared)\n\n\nEta-squared (η²): 0.2334896271386854\n\n\n\nBoom, you got effect sizes for F-tests like ANOVAs.\nIf you have understood this page it will set you up for the pages on each type of ANOVA as it uses the same concepts again and again."
  },
  {
    "objectID": "GeneralLinearModels/oneWayAnova.html",
    "href": "GeneralLinearModels/oneWayAnova.html",
    "title": "One Way ANOVA (incomplete)",
    "section": "",
    "text": "The mathematics behind a one-way ANOVA is actually just an extension of the GLM approach to independent samples t-tests. To illustrate this, we will investigate if there are significant differences in life expectancy between Africa, the Americas and Europe using gapminder data. Let’s first focus on the relevant contents in this dataset and then capture the sum of squares around the mean (i.e. the total sum of squares):\nBefore we start, lets visualise the three types of variance/sum of squares we will want to capture for our ANOVAs: Total, Between Subject and Error."
  },
  {
    "objectID": "GeneralLinearModels/oneWayAnova.html#one-way-anova",
    "href": "GeneralLinearModels/oneWayAnova.html#one-way-anova",
    "title": "One Way ANOVA (incomplete)",
    "section": "One Way ANOVA",
    "text": "One Way ANOVA\nAs highlighted above, ANOVAs aim to calculate what proportion of the total variance around the mean can be explained by your factors. So let’s start by calculating the total sum of squares:\n\\[\nSS_{total} = \\sum{(x_i - \\mu)^2}\n\\]\n\ntotal_ss = sum((gapminder_3_continents$lifeExp - mean(gapminder_3_continents$lifeExp))^2)\ntotal_ss\n\n[1] 17477.99\n\n\nSo 1.7477987^{4} is the all the variance around the mean to be explained. The general linear model we will run is to see how much variance is explained by the mean of each continent. The formula for this is:\n\\[\nSS_{between} = \\sum_{group}{(\\sum_{item}(\\bar{x}_{group} - \\mu)^2})\n\\]\nOr, applied to our current data:\n\\[\nSS_{between} = \\sum_{continent}{(\\sum_{country}(\\bar{x}_{continent} - \\mu)^2)}\n\\]\n\n\\(\\mu\\) represents the mean across all countries\n\\(\\bar{x}_{group}\\) represents the mean of the group\nSimilarly, \\(\\bar{x}_{continent}\\) is the mean across all countries in a continent\n\nIn English, for each country the model predicts that the value will be the mean life expectancy in its continent. So for Algeria, Angola and Botswana, the model predicts that the life expectancy will be 54.8060385. To capture all the variance that this model captures, we need to compare the predicted values to the group mean. We square these differences to make all the numbers positive so that when we sum then we don’t get (close to) zero after all the values above the mean balance out those below the mean. In terms of how this looks manually in R:\n\n## Manual calculation\nss_between = (\n  # SS for mean of Europe compared to the grand mean across all countries\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Europe\") +\n  # SS for mean of Americas compared to the grand mean across all countries\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Americas\") +\n  # SS for mean of Africa compared to the grand mean across all countries\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Africa\") \n)\nss_between\n\n[1] 12016.81\n\n\nSo, let’s just confirm that our manual calculation is the same as what a package will give us:\n\nsummary(aov(lifeExp ~ factor(continent), data = gapminder_3_continents))[[1]][[\"Sum Sq\"]][1]\n\n[1] 12016.81\n\n\nLooks good. Let’s confirm what the effect size for this is next."
  },
  {
    "objectID": "GeneralLinearModels/oneWayAnova.html#r2-and-r2_adj",
    "href": "GeneralLinearModels/oneWayAnova.html#r2-and-r2_adj",
    "title": "One Way ANOVA (incomplete)",
    "section": "\\(R^2\\) and \\(R^2_{adj}\\)",
    "text": "\\(R^2\\) and \\(R^2_{adj}\\)\nThe variance explained by the model is the \\(R^2\\) value, which is:\n\\[\nR^2 = \\frac{SS_{between}}{SS_{total}}\n\\]\nIf we manually calculate it we get:\n\nss_between/total_ss\n\n[1] 0.6875397\n\n\nLet’s double check this with a function based calculation:\n\nsummary(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\n\n\nCall:\nlm(formula = lifeExp ~ factor(continent), data = gapminder_3_continents)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.193  -3.934  -0.339   3.121  21.636 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 54.806      1.005   54.54   <2e-16 ***\nfactor(continent)Americas   18.802      1.764   10.66   <2e-16 ***\nfactor(continent)Europe     22.843      1.661   13.75   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.246 on 104 degrees of freedom\nMultiple R-squared:  0.6875,    Adjusted R-squared:  0.6815 \nF-statistic: 114.4 on 2 and 104 DF,  p-value: < 2.2e-16\n\n\nLovely. You may have noticed that there is also an adjusted R-Squared reported. There is a concern that an unadjusted R-Squared doesn’t take into account inflation in effect size calculations that happens in samples compared to populations (see degrees of freedom for an explanation of this). To apply degrees of freedom to address this bias, we actually need to reconceptualise the previous formula to focus on the unexplained variance (i.e. error):\n\\[\nSS_{between} = SS_{total}-SS_{error}\n\\]\nThis means that we can reconstruct the above formula:\n\\[\nR^2 = \\frac{SS_{between}}{SS_{total}} = \\frac{SS_{total}-SS_{error}}{SS_{total}} = 1 - \\frac{SS_{error}}{SS_{total}}\n\\]\nThis is helpful because we need to adjust our calculation to increase the weighting of error to counterbalance bias. We do this by applying degrees of freedom to both \\(SS_{error}\\) and \\(SS_{total}\\) , but the adjustment will result in new value for error being proportionately higher than for total:\n\\[\nR^2_{adj} = 1 - \\frac{SS_{error}/df_{error}}{SS_{total}/df_{total}}\n\\]\n\\[\ndf_{error} = N - 1 - (levels-1)\n\\]\n\\[\ndf_{total} = N-1\n\\]\nIn fact, the above includes the Mean Square Error (MSE) and the Mean Square Total (MST)\n\\[\nMSE = \\frac{SS_{error}}{df_{error}} = \\frac{SS_{error}}{N-1-(levels-1)}\n\\]\n\\[\nMST = \\frac{SS_{total}}{df_{total}} = \\frac{SS_{total}}{N-1}\n\\]\nSo another way you can think of this is:\n\\[\nR^2_{adj} = 1 - \\frac{SS_{error}/df_{error}}{SS_{total}/df_{total}} = \\frac{MSE}{MST}\n\\]\nLet’s see what we get when we calculate this manually\n\nerror_ss = total_ss-ss_between\nmse =  error_ss/(\n  dim(gapminder_3_continents)[1] - # N\n    (3-1)                        - # levels - 1\n    1\n)\nmst = total_ss/(\n  dim(gapminder_3_continents)[1] - # N\n    1\n )\n\n1-(mse/mst)\n\n[1] 0.6815309\n\n\nLet’s check if this value the same as you get when using a function:\n\nthis_summary = summary(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\nthis_summary$adj.r.squared\n\n[1] 0.6815309"
  },
  {
    "objectID": "GeneralLinearModels/oneWayAnova.html#eta-squared-eta2",
    "href": "GeneralLinearModels/oneWayAnova.html#eta-squared-eta2",
    "title": "One Way ANOVA (incomplete)",
    "section": "Eta-Squared (\\(\\eta^2\\))",
    "text": "Eta-Squared (\\(\\eta^2\\))\nEta-squared ( \\(\\eta^2\\) ) is actually the same as \\(R^2\\), but is the terminology associated with ANOVAs rather than linear models. Let’s confirm with a function:\n\nlsr::etaSquared(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\n\n                     eta.sq eta.sq.part\nfactor(continent) 0.6875397   0.6875397"
  },
  {
    "objectID": "GeneralLinearModels/oneWayAnova.html#p-value-calculation",
    "href": "GeneralLinearModels/oneWayAnova.html#p-value-calculation",
    "title": "One Way ANOVA (incomplete)",
    "section": "P-value calculation",
    "text": "P-value calculation\nTo calculate the p-value we need to calculate the F-value and then compare it to an f-distribution. The F-value is basically the sum of squares of what can be explained divided by the sum of squares of what cannot be explained, when controlling for degrees of freedom:\n\\[\nF = \\frac{SS_{between}/df_{between}}{SS_{error}/df_{error}}\n\\]\n\\[\ndf_{between} = (levels - 1)\n\\]\n\\[\ndf_{error} = (N - (levels-1) - 1)\n\\]\nIn our case, this is:\n\n(ss_between/2)/(error_ss/(107 - (3-1) - 1))\n\n[1] 114.4212\n\n\nLet’s confirm using a function\n\nthis_summary = summary(aov(lifeExp ~ factor(continent), data = gapminder_3_continents))\nthis_summary[[1]][[\"F value\"]][1]\n\n[1] 114.4212\n\n\nHow significant is this? Let’s draw our F-distribution\n\nggplot(data.frame(x = c(0, 120)), aes(x)) +\n  # stat_function(fun = df, args =list(df1 =5, df2=1), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = df, args =list(df1 =104, df2=2), color = \"blue\") +\n  geom_vline(xintercept = 114.4) +\n  # stat_function(fun = df, args =list(df1 =100, df2=2), color = \"red\") +\n  # stat_function(fun = df, args =list(df1 =100, df2=3), color = \"green\") +\n  # annotate(\"label\", x = 2, y = .4, label  = \"df1 = 100, df2=1\", colour=\"blue\") +\n  # annotate(\"label\", x = 2, y = .35, label = \"df1 = 100, df2=2\", colour=\"red\") +\n  # annotate(\"label\", x = 2, y = .3, label  = \"df1 = 100, df2=3\", colour=\"green\") +\n  xlab(\"F-Value\") +\n  ylab(\"Density\")\n\n\n\n\nAs we can see above, there is almost nothing under the curve (it’s imperceptible) for F of 114.4 and above. That means that the p-value will be tiny to reflect this very minimal area under the curve. Let’s check:\n\nthis_summary[[1]][[\"Pr(>F)\"]][1]\n\n[1] 5.362027e-27\n\n\nThat looks a fair representation of the proportion of area under the curve is greater than 114.4."
  },
  {
    "objectID": "categorical/contingencyQuestions.html",
    "href": "categorical/contingencyQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWill Cramer’s V give the same value as Phi?\n\nviewof contingency_1_response = Inputs.radio(['Yes','No']);\ncorrect_contingency_1 = 'Yes';\ncontingency_1_result = {\n  if(contingency_1_response == correct_contingency_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nChi Square tests tell you…\n\nviewof contingency_2_response = Inputs.radio(['Effect size','Significance']);\ncorrect_contingency_2 = 'Significance';\ncontingency_2_result = {\n  if(contingency_2_response == correct_contingency_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "categorical/contingency.html",
    "href": "categorical/contingency.html",
    "title": "Contingency (R)",
    "section": "",
    "text": "Imagine you want to estimate if males or females are more likely to go into certain professions. We could start by looking at a psychology department and analysing the number of female and male lecturers (i.e. frequency). Let’s imagine a department from the 1953, in which there were 5 female and 50 male lecturers, which now in 2023 has 50 female and 50 male lecturers. Whilst we can see that the ratio is more even now than it was in the 1953, but can we quantify how strong the effect of time is, and how significant the result is (and thus whether we are inclined to assume that the change in this department is representative of a general change in all departments).\nTo summarise the data, we can create a contingency table:\n(a), (b), (c) and (d) are references for specific cells and will be used for calculations below.\nThe above table visualises how contingent the ratio of female to male staff numbers are on the year.\nThere are a few statistics we can use to capture the size and/or significance of contingency effects:\nNote that Phi \\(\\phi\\) and Cramer’s \\(V\\) are similar to Pearson/Spearman \\(r\\) correlation coefficients, in that they convey the strength of an association. However, whilst \\(r\\) tells you both the direction of an association and the strength of it, the direction of \\(\\phi\\) is difficult to interpret (see below) and \\(V\\) is not directional.\nLet’s now use the above example to go through the calculations for \\(\\phi\\), \\(\\chi^2\\) and \\(V\\)"
  },
  {
    "objectID": "categorical/contingency.html#phi-phi",
    "href": "categorical/contingency.html#phi-phi",
    "title": "Contingency (R)",
    "section": "Phi \\(\\phi\\)",
    "text": "Phi \\(\\phi\\)\nTo capture the strength of the association between 2 categorical variables that have 2 levels each we can use the following formula:\n\\[\n\\phi = \\frac{a * d - b * c}{\\sqrt{(e * f * g * h)}}\n\\]\nFor our example above, we get:\n\\[\n\\frac{females_{1953} * males_{2023} - females_{2023} * males_{1953}}{\\sqrt{females * males * staff_{1953} * staff_{2023}}}\n\\]\nThe top half of our formula allows us to capture how much of an interaction there is between sex and year by multiplying opposites and then comparing the difference. If there is not much difference between these opposites, it suggests there isn’t an interaction. If you think there is no association between your categorical variables, then you would expect that there would be a similar ratio of females to males between 1953 and 2023, perhaps something like:\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n10 (b)\n15 (e)\n\n\nMale\n50 (c)\n100 (d)\n150 (f)\n\n\nTotal\n55 (g)\n110 (h)\n165\n\n\n\nWhen multiplying females in 1953 with males in 2023 we get \\(5 * 100 = 500\\)\nWhen multiplying females in 2023 with males in 1953 we get \\(10*50 = 500\\)\nSo as these opposites aren’t very different, there’s no interaction (\\(500 - 500 = 0\\)).\nLet’s try to crystalise how this comparison of opposites looks when the ratios are reversed, so that 0% of staff are female in 1953, and 0% of staff are male in 2023:\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n0 (a)\n100 (b)\n100 (e)\n\n\nMale\n50 (c)\n0 (d)\n50 (f)\n\n\nTotal\n50 (g)\n100 (h)\n150\n\n\n\nWhen multiplying females in 1953 with males in 2023 we get \\(0 * 0 = 0\\)\nWhen multiplying females in 2023 with males in 1953 we get \\(100*50 = 5000\\)\nThese opposites seem very different as \\(5000 - 0 = 5000\\), but we need to capture the scale of what a value would be if there is a perfect association:\n\\[\n\\sqrt{females * males * staff_{1953} * staff_{2023}} = \\sqrt{100 * 50 * 50* 100} = \\sqrt{25000000} = 5000\n\\]\nAs \\(5000/5000 = 1\\) we can say that there’s a perfect association between time and sex in staff numbers.\n\n\n\n\n\n\nOptional clarification about perfect associations\n\n\n\nNote that it’s only possible to get a \\(\\phi\\) of 1 or -1 if you have 2 diagonal cells with zero in them. This is because a perfect association between your variables would require that data points are always associated with one combination or its opposite (i.e. being male in 1953 or female in 2023), otherwise that data point is not explained by the association.\n\n\nNow that we’ve gone through the extreme examples, let’s have a look at how strong the association is between sex and time in the original example data\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n50 (b)\n55 (e)\n\n\nMale\n50 (c)\n50 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155\n\n\n\n\nlibrary(psych)\n# table above in a matrix\ncontingency_data <- matrix(\n  data = c(5,50,50,50),\n  nrow = 2,\n  ncol = 2\n)\n\n# manual calculation using the formula above\n(\n  # a * d\n  contingency_data[1,1] * contingency_data[2,2] - \n  # b * c\n  contingency_data[1,2] * contingency_data[2,1]\n)/sqrt(\n  # e * f * g * h\n  sum(contingency_data[1,]) *\n  sum(contingency_data[2,]) *\n  sum(contingency_data[,1]) *\n  sum(contingency_data[,2])\n)\n\n[1] -0.4090909\n\n# using psych package\nphi(contingency_data, digits = 7)\n\n[1] -0.4090909\n\n\nSo we now have captured the size of the effect, let’s look at whether we can capture how significant the effect is by using Chi-squared \\(\\chi^2\\) analyses:"
  },
  {
    "objectID": "categorical/contingency.html#chi-squared-chi2",
    "href": "categorical/contingency.html#chi-squared-chi2",
    "title": "Contingency (R)",
    "section": "Chi-squared \\(\\chi^2\\)",
    "text": "Chi-squared \\(\\chi^2\\)\nChi-squared tests allow us to compare our observed frequencies to what frequencies you would expect if the null hypothesis is true, i.e. if there is no association between your categorical variables. We already have our observed data:\n\n\n\nObserved\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n50 (b)\n55 (e)\n\n\nMale\n50 (c)\n50 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155 (i)\n\n\n\nBut now we need to calculate the expected data if the null hypothesis is true. We would expect that the number of females in 1953 is\n\\[\nfemales1953_{expected} = \\frac{females * Staff_{1953}}{Total}\n\\]\n\n\n\n\n\n\nOptional - why does the above formula make sense?\n\n\n\nIn case you’re not convinced, we can restructure the above formula to make it a bit clearer\nIf you know how many females there are total, then you want to work out what proportion you should expect in 1953 compared to 2023. If the null hypothesis is correct, that the ratio of females to males should not change between years, then the number of females in 1953 vs. 2023 should reflect the ratio of 1953 to 2023 generally:\n\\[\nstaffProportion = \\frac{Staff_{1953}}{Staff_{1953} + Staff_{2023}}\n\\]\nThen you apply this proportion to females to estimate how many you expect in 1953:\n\\[\nfemales1953_{expected} = females * staffProportion\n\\]\nBringing everything above together (\\(fem\\) = \\(females\\)):\n\\[\nfem1953_{exp} = fem * staffProp = fem * \\frac{Staff_{1953}}{Staff_{1953} + Staff_{2023}} = females * \\frac{Staff_{1953}}{Total} = \\frac{fem * Staff_{1953}}{Total}\n\\]\n\n\nUsing the above formula, let’s estimate all four cells\n\nFemales in 1953 = \\(\\frac{females * Staff_{1953}}{Total} = \\frac{55 * 55}{155} = 19.51613\\)\nMales in 1953 = \\(\\frac{males * Staff_{1953}}{Total} = \\frac{100 * 55}{155} = 35.48387\\)\nFemales in 2023 = \\(\\frac{females * Staff_{2023}}{Total} = \\frac{55 * 100}{155} = 35.48387\\)\nMales in 2023 = \\(\\frac{males * Staff_{2023}}{Total} = \\frac{100 * 100}{155} = 64.51613\\)\n\n\n\n\n\n\n\n\n\n\nExpected\n1953\n2023\nTotal\n\n\n\n\nFemale\n19.51613 (a)\n35.48387 (b)\n55 (e)\n\n\nMale\n35.48387 (c)\n64.51613 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155 (i)\n\n\n\nNow that we’ve calculated the expected values for each cell, we can compare our observed values against the expected values and square the difference:\n\n\n\n\n\n\n\n\n\nObserved vs. Expected\n1953\n2023\nTotal\n\n\n\n\nFemale\n\\((5 - 19.51613)^2\\) = 210.7180302\n\\((50 - 35.48387)^2 =\\) 210.7180302\n55\n\n\nMale\n\\((50 -35.48387)^2\\) = 210.7180302\n\\((50 - 64.51613)^2 =\\) 210.7180302\n100\n\n\nTotal\n55\n100\n155\n\n\n\nThe \\(\\chi^2\\) value is the sum of these squared divided by the expected frequency:\n\\[\n\\chi^2 = \\sum\\frac{(O_i - E_i)^2}{E_i}\n\\]\nwhich in our case is\n\\[\n\\frac{210.7180302}{19.51613}  + \\frac{210.7180302}{35.48387} + \\frac{210.7180302}{35.48387} + \\frac{210.7180302}{64.51613} = 25.94009\n\\]\nNow that we have a score, we need to calculate a p-value from it using an appropriate chi-square distribution. The distribution we draw will depend on degrees of freedom (this link explains the concept, but not how it changes the distribution shape). Whilst greater degrees of freedom normally results in a narrower distribution in which test statistics become more significant (see t-distributions for an example), \\(\\chi^2\\) distribution shapes change depending on the degrees of freedom. For example, if there’s only 1 degree of freedom, the \\(\\chi^2\\) distribution is one sided:\n\ncurve(dchisq(x, df = 1), from = 0, to = 5,xlab = expression(chi^2))\n\n\n\n\nHowever, if there are 3 (or more) degrees of freedom it becomes a curve:\n\ncurve(dchisq(x, df = 3), from = 0, to = 40,xlab = expression(chi^2))\n\n\n\n\nUltimately, you are still trying to work out the likelihood of achieving your chi-square score or higher by calculating the area under the curve. The degrees of freedom of freedom are calculated as follows:\n\\[\ndf = (rows - 1) * (columns - 1)\n\\]\nSo in our case the degrees of freedom are:\n\\[\ndf = (rows - 1) * (columns - 1) = (2 - 1) * (2 - 1) = 1\n\\]\nWe calculated the \\(\\chi^2\\) as 25.94, so to estimate how significant the result is, we use area under the curve to capture how likely it is to get a \\(\\chi^2\\) value of 25.94 and above:\n\nthis_chi_2 = 25.94\n\n#create density curve\ncurve(dchisq(x, df = 1), from = 0, to = 30,\nmain = 'Chi-Square Distribution (df = 1)',\nylab = 'Density',\nlwd = 2, xlab = expression(chi^2))\n\n#create vector of x values\nx_vector <- seq(this_chi_2, 30,.01)\n\n#create vector of chi-square density values\np_vector <- dchisq(x_vector, df = 1)\n\n#fill in portion of the density plot from 0 to 40\npolygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),\n        col = adjustcolor('red', alpha=0.3), border = NA)\n\n\n\n\nThe area under the curve is imperceptible because it is so small. Let’s zoom in to the 20-30 range on the x-axis:\n\nthis_chi_2 = 25.94\n\n#create density curve\ncurve(dchisq(x, df = 1), from = 20, to = 30,\nmain = 'Chi-Square Distribution (df = 1)',\nylab = 'Density',\nlwd = 2, xlab = expression(chi^2))\n\n#create vector of x values\nx_vector <- seq(this_chi_2, 30,.01)\n\n#create vector of chi-square density values\np_vector <- dchisq(x_vector, df = 1)\n\n#fill in portion of the density plot from 0 to 40\npolygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),\n        col = adjustcolor('red', alpha=0.3), border = NA)\n\n\n\n\nNow we can see that the possibility of getting a \\(\\chi^2\\) of 25.94 or above is very unlikely, and so our result is significant. How significant? Let’s take a shortcut and use a function to confirm we calculated the \\(\\chi^2\\) correctly and what area under the curve is reflected above (and thus what the p-value is):\n\noptions(scipen = 999)\nsex_year_table = rbind(c(5, 50), c(50 ,50))\ndimnames(sex_year_table) <- list(\n  sex = c(\"Female\",\"Male\"),\n  year = c(1953, 2023)\n)\n# turning off correction\nchisq.test(sex_year_table,correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  sex_year_table\nX-squared = 25.94, df = 1, p-value = 0.0000003522\n\n\nThe p-value is 0.0000004, indicating that \\(\\chi^2\\) values of 25.94 and above take 0.0000004 of the area under the curve. This provides very significant evidence that there’s an association between time and the ratio of females:males.\n\n\n\n\n\n\nProbability density vs. cumulative distribution\n\n\n\nWhen looking up \\(\\chi^2\\) distributions, be aware that there there’s a difference between a probability density function like we used above, and a cumulative distribution function:\n\ncurve(pchisq(x, df = 1), from = 0, to = 5,xlab = expression(chi^2))\n\n\n\n\nA cumulative distribution visualises the likelihood of a \\(\\chi^2\\) value and below as the y-axis for any individual \\(\\chi^2\\) value. It has already added up the cumulative probabilities for the lower \\(\\chi^2\\) values.\nTo illustrate this concept, let’s evaluate the likelihood of getting a \\(\\chi^2\\) of 1 or less. Let’s use the probability density function to visualise area under the curve for this:\n\nthis_chi_2 = 1\n\n#create density curve\ncurve(dchisq(x, df = 1), from = 0, to = 5,\nmain = 'Chi-Square Distribution (df = 1)',\nylab = 'Density',\nlwd = 2, xlab = expression(chi^2))\n\n#create vector of x values\nx_vector <- seq(0, this_chi_2,.01)\n\n#create vector of chi-square density values\np_vector <- dchisq(x_vector, df = 1)\n\n#fill in portion of the density plot from 0 to 40\npolygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),\n        col = adjustcolor('red', alpha=0.3), border = NA)\n\n\n\n\nWe can see that this range of 0 to 1 covers a large proportion of the area under the curve. We can see exactly how much within the following cumulative probability function:\n\ncurve(pchisq(x, df = 1), from = 0, to = 5,xlab = expression(chi^2))\nabline(h = pchisq(1, df=1), v = 1, lty = 1, col = \"red\")\n\n\n\n\nSo now you can see that the area under the curve from 0 to 1 for the probability density function is 0.6826895 of the total area under the curve. The probability of getting a \\(\\chi^2\\) greater than 1 is thus 1 - 0.6826895 which is 0.3173105."
  },
  {
    "objectID": "categorical/contingency.html#cramers-v",
    "href": "categorical/contingency.html#cramers-v",
    "title": "Contingency (R)",
    "section": "Cramer’s V",
    "text": "Cramer’s V\nAs mentioned earler, Cramer’s \\(V\\) gives you the same results as \\(\\phi\\), but can also be used to summarise the strength of association for designs more complicated than 2 x 2. Cramer’s \\(V\\) can be calculated as follows:\n\\[\nV^2 = \\sqrt{\\frac{\\chi^2/n}{min(cols-1,rows-1)}}\n\\]\nLet’s confirm that it gives the same value as \\(\\phi\\):\n\n# phi calcultion from above\nphi(contingency_data, digits = 7)\n\n[1] -0.4090909\n\n# manual calculation of Cramer\nsqrt((25.94009/155)/1)\n\n[1] 0.409091\n\n# cramer function\nlibrary(confintr)\ncramersv(contingency_data)\n\n[1] 0.4090909\n\n\nYou may notice that \\(\\phi\\) was negative whereas Cramer’s V is positive. This is because \\(\\chi^2\\) values are always positive (as they are calculated based off squared values), and so Cramer’s \\(V\\) is positive because it’s based on \\(\\chi^2\\) values. It’s not intuitive to identify the direction of a finding based on it being a positive or negative \\(\\phi\\), so it can be helpful to also include other summaries of the findings direction as described below."
  },
  {
    "objectID": "categorical/contingency.html#other-metrics-for-2-x-2-contingency-data",
    "href": "categorical/contingency.html#other-metrics-for-2-x-2-contingency-data",
    "title": "Contingency (R)",
    "section": "Other metrics for 2 x 2 contingency data",
    "text": "Other metrics for 2 x 2 contingency data\nWe would like to capture the direction of our findings for the following data:\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n50 (b)\n55 (e)\n\n\nMale\n50 (c)\n50 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155\n\n\n\n\nOdds Ratio\nIf we want to summarise how much the female:male ratio has shifted between years, we can use an odds ratio\n\\[\n\\frac{a/c}{b/d} = \\frac{female_{1953}/male_{1953}}{female_{2023}/male_{2023}}\n\\]\nA value of 1 would reflect no association between sex and year. A number above 1 would reflect a relatively higher number of females to males in 1953 compared to 2023. A number below would reflect a relatively higher number of females to males in 2023 compared to 1953. We can see from the above table that the ratio of females to males is higher in 2023 than 1953, so let’s see if the odds-ratio reflects this:\n\\[\n\\frac{a/c}{b/d} = \\frac{female_{1953}/male_{1953}}{female_{2023}/male_{2023}} = \\frac{5/50}{50/50} = .1\n\\]\nGroovy, the odds ratio gives a relatively intuitive gauge of how big the differences in ratios is between years.\n\n\nRisk Ratio\nThe risk ratio is similar conceptually to odds ratio, but compares proportions. In our case, it would look at the proportion of females within each year group and then compare them:\n\\[\n\\frac{a/(a+c)}{b/(b+c)} = \\frac{female_{1953}/(male_{1953} + female_{1953})}{female_{2023}/(male_{2023} + female_{2023})} = \\frac{propFemale_{1953}}{propFemale_{2023}} = \\frac{5/55}{50/100} = \\frac{10}{55} = .182\n\\]\nAs with the odds ratio above, a risk ratio below 1 would suggest that the proportion of females is lower in 1953 than 2023, and a risk ratio above 1 would suggest that the proportion of females is higher in 1953 than 2023."
  },
  {
    "objectID": "categorical/contingency.html#what-to-do-if-you-have-5-or-less-in-one-of-your-cells",
    "href": "categorical/contingency.html#what-to-do-if-you-have-5-or-less-in-one-of-your-cells",
    "title": "Contingency (R)",
    "section": "What to do if you have 5 or less in one of your cells",
    "text": "What to do if you have 5 or less in one of your cells\nThere is an argument that if there is 5 or less in any cell in a contingency table you should not use Chi-Square without a correction as there’s a concern of an inflated risk of false-positives without this correction (we won’t be going into the maths behind this risk). We will go through the options of using a Yates Continuity Correction or a Fisher’s Exact test to deal with this situation.\n\nYates Continuity Correction\nIn Yates Continuity Correction you subtract .5 from each cell to reduce the final \\(\\chi^2\\) value:\n\\[\n\\chi^2_{Yates} = \\sum\\frac{(abs(O_i - E_i)-.5)^2}{E_i}\n\\]\nLet’s compare a manual calculation of a Yates corrected \\(\\chi^2\\) value to one using a function:\n\n# manual calculation\n(((abs(5 - 19.51613)-.5)^2)/19.51613) +\n(((abs(50 - 35.48387)-.5)^2)/35.48387) +\n(((abs(50 - 35.48387)-.5)^2)/35.48387) +\n(((abs(50 - 64.51613)-.5)^2)/64.51613)\n\n[1] 24.18388\n\n# using function\nsex_year_table = rbind(c(5, 50), c(50 ,50))\ndimnames(sex_year_table) <- list(\n  sex = c(\"Female\",\"Male\"),\n  year = c(1953, 2023)\n)\n# turning off correction\nchisq.test(sex_year_table,correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  sex_year_table\nX-squared = 24.184, df = 1, p-value = 0.0000008756\n\n\nYou may have noticed that we have turned correct (i.e. apply Yates Correction) to T for TRUE. Functions will sometimes automatically apply this correction, so it’s worth checking your output carefully to see if this has happened.\n\n\nFisher’s Exact Test\nWhilst \\(\\chi^2\\) analysis p-values are based on an approximation based on a probability density function distribution of \\(\\chi^2\\) values, Fisher’s Exact Test calculates exactly how likely it is to get an outcome for each permutation:\n\\[\n\\frac{(a+b)!*(c+d)!*(a+c)!*(b+d)!}{a!*b!*c!*d!*n!}\n\\]\nImportantly, you must repeat this calculation for each more extreme value, and add together all outcomes to get the p-value that reflects the likelihood of getting this contingency or a more extreme one. To get a more extreme outcome look at cell (a) and judge whether increasing or reducing its value by 1 is more extreme. If you remove 1 from (a) you need to adjust all the other columns to maintain the row and column totals, for example:\nOriginal table\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n50 (b)\n55 (e)\n\n\nMale\n50 (c)\n50 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155 (n)\n\n\n\nSlightly more extreme table\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n4 (a)\n51 (b)\n55 (e)\n\n\nMale\n51 (c)\n49 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155 (n)\n\n\n\nThen run the calculation again and add it to your previous p-value, and continue until you reach the maximum possible value in a (i.e. it’s no lower than 0 or no higher than (e) or (g)). Note that this will be a 1-tailed hypothesis and you will need to reflect whether it is going against your hypothesis or not. In our case, it is consistent with our hypothesis that females are under-represented in 1953, so calculating all cases with less representation of females in 1953 is testing the significance of this hypothesis.\n\n# due to massive numbers using Brobdingnag package to avoid numbers becoming infinite\nlibrary(Brobdingnag)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Brobdingnag'\n\n\nThe following objects are masked from 'package:Matrix':\n\n    diag, t\n\n\nThe following objects are masked from 'package:base':\n\n    diag, t\n\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(5)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(155))\n  )\n) +\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(4)) *\n    as.brob(factorial(51)) *\n    as.brob(factorial(51)) *\n    as.brob(factorial(49)) *\n    as.brob(factorial(155))\n  )\n) +\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(3)) *\n    as.brob(factorial(52)) *\n    as.brob(factorial(52)) *\n    as.brob(factorial(48)) *\n    as.brob(factorial(155))\n  )\n) +\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(2)) *\n    as.brob(factorial(53)) *\n    as.brob(factorial(53)) *\n    as.brob(factorial(47)) *\n    as.brob(factorial(155))\n  )\n) +\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(1)) *\n    as.brob(factorial(54)) *\n    as.brob(factorial(54)) *\n    as.brob(factorial(46)) *\n    as.brob(factorial(155))\n  )\n) +\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(0)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(45)) *\n    as.brob(factorial(155))\n  )\n)\n\n[1] 0.00000009582059\n\n# 0.00000009582\n\n# compare with package:\nfisher.test(sex_year_table, alternative = 'less')\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  sex_year_table\np-value = 0.00000009582\nalternative hypothesis: true odds ratio is less than 1\n95 percent confidence interval:\n 0.0000000 0.2471546\nsample estimates:\nodds ratio \n 0.1013897 \n\n\n\n\n\n\n\n\nRabbit Hole 🐰 🕳️\n\n\n\nMost psychology textbooks don’t address the following, so you can proceed to later pages without it.\n\nWhy/How Fisher’s Exact Test works\nThe above formula is a bit dense, so let’s focus on a simpler example to explain the mathematics behind it.\nPermutations on the probability page describes ways of calculating how many unique permutations there are in situations like this. Let’s apply an explanation of Fisher’s test from Chapter 13 of (Hoffman 2015) to explain how the maths works for our current example.\nTo start with, let’s work out how many unique permutations of females vs. males we could have within 1953. Using the general formula:\n\\[\n\\frac{(females_{1953} + males_{1953})!}{females_{1953}! * males_{1953}!} = \\frac{55!}{5! * 50!} = 3478761\n\\]\nNow let’s work out the same for 2023:\n\\[\n\\frac{(females_{2023} + males_{2023})!}{females_{2023}! * males_{2023}!} = \\frac{100!}{50! * 50!} = 100891344545564518731063754752\n\\]\nNow we can multiply the combinations from 1953 to those from 2023, as we can apply each combination from one year to any combination from the other year, to get 350976874642672723890193854162272256.\nNow that we’ve counted every possible valid permutation with our data (i.e. with the correct number of males and females in their respective years), we need to divide it by the total number of permutations that could exist regardless of year (as we have so far been focusing within years). With 55 females and 100 males we get the following:\n\\[\n\\frac{(females + males)!}{females! * males!} = \\frac{(55+100)!}{55! * 100!} = 4041787069361244292202808568245231453995008\n\\]\nSo the likelihood of this specific outcome is:\n\\[\n\\frac{350976874642672723890193854162272256}{4041787069361244292202808568245231453995008} = 0.00000008683705\n\\]\nNot super likely. Let’s compare this conceptual calculation with the above formula, and then use algebra to show that they are the same. Let’s check if we get the same value first:\n\nas.numeric(\n  (\n    as.brob(factorial(55)) *\n    as.brob(factorial(100)) *\n    as.brob(factorial(55)) *\n    as.brob(factorial(100))\n  ) / (\n    as.brob(factorial(5)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(50)) *\n    as.brob(factorial(155))\n  )\n)\n\n[1] 0.00000008683705\n\n\n… and now use algebra to show why, with reference to the table\n\n\n\n\n1953\n2023\nTotal\n\n\n\n\nFemale\n5 (a)\n50 (b)\n55 (e)\n\n\nMale\n50 (c)\n50 (d)\n100 (f)\n\n\nTotal\n55 (g)\n100 (h)\n155 (n)\n\n\n\n\\[\n\\frac{perms_{1953}*perms_{2023}}{permsAcrossYears} = \\frac{(\\frac{g!}{a!*c!})*(\\frac{h!}{b!*d!})}{\\frac{n!}{e!*f!}} = \\frac{(\\frac{g! * h!}{a! * b! * c! * d!})}{(\\frac{n!}{e! * f!})} = \\frac{(\\frac{e!*f!*g!*h!}{a!*b!*c!*d})}{n!}= \\frac{(a+b)!*(c+d)!*(a+c)!*(b+d)!}{a!*b!*c!*d!*n!}\n\\]"
  },
  {
    "objectID": "categorical/contingency.html#g-test",
    "href": "categorical/contingency.html#g-test",
    "title": "Contingency (R)",
    "section": "G-test",
    "text": "G-test\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nThere’s growing use of G-tests in place of \\(\\chi^2\\) tests, as they are more robust when the observed numbers are 2 times greater than the expected number. Otherwise they generally come to the same conclusion as \\(\\chi^2\\) tests. The formula for g-tests is:\n\\[\nG = 2 * \\sum O_i * ln(\\frac{O_i}{E_i})\n\\]\nLets compare the outputs for a g-test on our data to a \\(\\chi^2\\) test:\n\nlibrary(AMR)\n\n\nAttaching package: 'AMR'\n\n\nThe following objects are masked from 'package:confintr':\n\n    kurtosis, skewness\n\n\nThe following object is masked from 'package:psych':\n\n    pca\n\ng.test(contingency_data)\n\nWarning in g.test(contingency_data): `fisher.test()` is always more reliable\nfor 2x2 tables and although much slower, often only takes seconds.\n\n\n\n    G-test of independence\n\ndata:  contingency_data\nX-squared = 29.482, p-value = 0.00000005645\n\nchisq.test(contingency_data)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_data\nX-squared = 24.184, df = 1, p-value = 0.0000008756\n\n\n\nQuestion 1\nWill Cramer’s V give the same value as Phi?\n\nviewof contingency_1_response = Inputs.radio(['Yes','No']);\ncorrect_contingency_1 = 'Yes';\ncontingency_1_result = {\n  if(contingency_1_response == correct_contingency_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nChi Square tests tell you…\n\nviewof contingency_2_response = Inputs.radio(['Effect size','Significance']);\ncorrect_contingency_2 = 'Significance';\ncontingency_2_result = {\n  if(contingency_2_response == correct_contingency_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "categorical/contingency.html#references",
    "href": "categorical/contingency.html#references",
    "title": "Contingency (R)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics",
    "section": "",
    "text": "this_page = \"stats_basics\""
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-variable",
    "href": "statsBasics/statsBasics.html#what-is-a-variable",
    "title": "Statistics Basics",
    "section": "What is a variable?",
    "text": "What is a variable?\nIn statistics, a variable is any (measurable) attribute that describes any organism or object. It’s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-hypothesis",
    "href": "statsBasics/statsBasics.html#what-is-a-hypothesis",
    "title": "Statistics Basics",
    "section": "What is a hypothesis?",
    "text": "What is a hypothesis?\nA(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e. randomly) if your main hypothesis were not true. We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability.\n\n\n\n\n\n\n\nNote\n\n\n\nIn terms of whether your research conclusions are valid, there are 4 broad possible outcomes:\n\n\n\n\n\n\n\n\n\nTrue\nFalse\n\n\n\n\nPositive\nYou conclude there is an effect in your sample and this reflects the general population.\n(i.e. the effect you found was real)\ne.g. You conclude that big dogs have more fur than small dogs (presumably this is true as there is more dog to put fur on in big dogs, right?)\nYou conclude there is an effect in your sample, but this does not reflect the general population.\n(i.e. the effect you found was not real)\ne.g. You conclude that half-full glasses have more water than half-empty glasses\nThis is a type 1 error\n\n\nNegative\nYou conclude that there is no effect in your sample, and this reflects the general population.\n(i.e. you are correct to say that there is no effect)\ne.g. You conclude that half-full glasses have as much water as a half-empty glasses\nYou conclude that there is no effect in your sample, but this does not reflect the general population.\n(i.e. you are incorrect to say that there is no effect)\ne.g. You conclude that big dogs do not have more fur than small dogs\nThis is a type 2 error"
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\n\\(Alpha\\) (⍺) is the p-value threshold that identifies if a result is “significant” or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is “significant” (i.e. so unlikely that this would have happened by chance that we conclude this didn’t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e. we say there is an effect when there isn’t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "href": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "title": "Statistics Basics",
    "section": "What is power (and what is the beta value)",
    "text": "What is power (and what is the beta value)\nPower tells you how likely you are to get a significant result (assuming the effect you are investigating is real) based on:\n\nthe sample size of participants you have. The more participants you have, the more powerful your analysis will be (i.e. the more likely you are to find a significant result).\nthe effect size (e.g. Cohen’s d). The bigger the effect size, the more powerful your analysis will be. Effect sizes in power calculations are generally based on previous studies with similar paradigms.\nthe \\(alpha\\) (⍺) threshold. A smaller \\(alpha\\) threshold requires a higher powered analysis to get a significant result.\n\nTo explore this, you might find the following interactive tool helpful to see what happens when you change the above:\nhttps://observablehq.com/@patrickmineault/interactive-demo-in-pure-js\n\nQuestion 1\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\nAlpha values are…\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/algebra.html",
    "href": "statsBasics/algebra.html",
    "title": "Algebra",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "statsBasics/algebra.html#why-are-we-discussing-algebra",
    "href": "statsBasics/algebra.html#why-are-we-discussing-algebra",
    "title": "Algebra",
    "section": "Why are we discussing algebra?",
    "text": "Why are we discussing algebra?\nIn many of the pages on this website there is algebra used to explain how the formulas work. So we’ll go through what algebra is here."
  },
  {
    "objectID": "statsBasics/probability.html",
    "href": "statsBasics/probability.html",
    "title": "Probability (R)",
    "section": "",
    "text": "This concept is somewhat foundational to almost all the other content in this website, but is often overlooked in textbooks. An advantage about understanding it is that will give you a more complete understanding later when we go through concepts such as probability distributions such as binomial and normal distributions."
  },
  {
    "objectID": "statsBasics/probability.html#probability-of-single-outcomes",
    "href": "statsBasics/probability.html#probability-of-single-outcomes",
    "title": "Probability (R)",
    "section": "Probability of single outcomes",
    "text": "Probability of single outcomes\nIn informal language we might talk about there being a 50% chance of something happening, such as a 50% chance of getting “heads” when flipping a coin. At the risk of telling you what you already know, if there’s a 50% chance of getting “heads”, then we assume there is a 50% chance of getting “tails”, because the likelihood of all outcomes put together must equal 100% or else it suggests there’s another outcome you haven’t considered. For example, if a flipped coin lands on heads 45% of the time, and on tails 45% of the time, then (45% + 45% =) 90% of the time it will land on heads or tails, which means 10% of the time it will do one or more other things (e.g. land on neither side). Going forward we will assume we’re using coins that can only land on heads or tails.\nWithin statistics we tend to use decimalised numbers rather than percentages, so a 10% chance is written as .1, a 50% chance is written as .5 and 100% chance is written as 1. This is important as when you have considered all possible outcomes you should be able to add their likelihoods up to make 1."
  },
  {
    "objectID": "statsBasics/probability.html#probability-of-combinations-of-outcomes",
    "href": "statsBasics/probability.html#probability-of-combinations-of-outcomes",
    "title": "Probability (R)",
    "section": "Probability of combinations of outcomes",
    "text": "Probability of combinations of outcomes\nImagine that you want to predict the likelihood of flipping a coin on heads or tails a certain number of times in a row. The outcome of each coin flip is binary, i.e. there is only 1 possible outcome out of 2 options. If this isn’t a biased coin, we can calculate some basic expectations about what will happen after 2 flips of the coin:\n\nEach flip of the coin has a 0.5 chance of landing on Heads.\nIf we wanted to calculate the likelihood of 2 flips of heads, there’s a 0.5 chance we get the first heads flip, and then another 0.5 chance we’ll get the second head flip. To summarise this, we can multiply both 0.5 chances together to get 0.25.\nTo summarise the likelihood of all combinations we could make the following table:\n\n\n\n\n\n\n\n\n\nFirst flip and likelihood\nSecond flip and likelihood\nOverall likelihood\n\n\nHeads (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nHeads (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\n\nLikelihood of any of the above happening\n.25 + .25 + .25 + .25 = 1\n\n\n\nTo achieve what you have above, you need to consider every combination of possible outcomes and calculate their likelihood. A useful quality check is to make sure that when you add the overall likelihoods together you get 1, otherwise…\n\nIf you have less than 1 it suggests you have overlooked an outcome\nIf you have more than 1 it suggests you have either\n\nOverestimated the likelihood of a specific outcome\nTreated overlapping outcomes as if they are mutually exclusive\n\n\n\nMutually exclusive outcomes\nIn the above example each of the four combinations are mutually exclusive, as they are specific and distinct. It is impossible that your flip of your coins was both:\n\nheads and then tails\nheads and then heads\n\nEven though the first flip is heads in both outcomes, the second flip is distinct and so the two flips could not be both outcomes as they are exclusive.\nHowever, there are some scenarios in which it’s less clear whether two outcomes are mutually exclusive. The chance of rolling any side of a die is 1 in 6. The chance of rolling a certain number and above can be calculated by counting the number of valid sides there are and then dividing by 6. For example, the chance of rolling 3 and above is 4 in 6 because there are 4 valid sides (3,4,5 and 6).\nImagine that you are rolling dice twice in a row - what is the likelihood that one role will be at least 3 and the other will be at least 4? Let’s start with an incorrect answer:\n\nRPython\n\n\n\ndice_outcomes <- data.frame(\n  Description <- c(\n    \"First is 3 or more, second is 4 or more\", # valid outcome\n    \"First is 4 or more, second is 3 or more\", # valid outcome\n    \"First is 3 or more, second is 3 or LESS\", # invalid outcome\n    \"First is 4 or more, second is 2 or LESS\", # invalid outcome\n    \"First is 3 or LESS, second is 4 or more\", # invalid outcome\n    \"First is 2 or LESS, second is 3 or more\", # invalid outcome\n    \"First is 2 or LESS, second is 2 or LESS\"  # invalid outcome\n  ),\n  likelihood = c(\n    (4/6) * (3/6), # \"First is 3 or more, second is 4 or more\", # valid outcome\n    (3/6) * (4/6), # \"First is 4 or more, second is 3 or more\", # valid outcome\n    (3/6) * (3/6), # \"First is 3 or more, second is 3 or LESS\", # invalid outcome\n    (3/6) * (2/6), # \"First is 4 or more, second is 2 or LESS\", # invalid outcome\n    (3/6) * (3/6), # \"First is 3 or LESS, second is 4 or more\", # invalid outcome\n    (2/6) * (4/6), # \"First is 2 or LESS, second is 3 or more\", # invalid outcome\n    (2/6) * (2/6)  # \"First is 2 or LESS, second is 2 or LESS\"  # invalid outcome\n  )\n)\nknitr::kable(dice_outcomes)\n\n\n\n\n\n\n\n\nDescription….c..First.is.3.or.more..second.is.4.or.more….First.is.4.or.more..second.is.3.or.more…\nlikelihood\n\n\n\n\nFirst is 3 or more, second is 4 or more\n0.3333333\n\n\nFirst is 4 or more, second is 3 or more\n0.3333333\n\n\nFirst is 3 or more, second is 3 or LESS\n0.2500000\n\n\nFirst is 4 or more, second is 2 or LESS\n0.1666667\n\n\nFirst is 3 or LESS, second is 4 or more\n0.2500000\n\n\nFirst is 2 or LESS, second is 3 or more\n0.2222222\n\n\nFirst is 2 or LESS, second is 2 or LESS\n0.1111111\n\n\n\n\nsum(dice_outcomes$likelihood)\n\n[1] 1.666667\n\n\n\n\n\nimport pandas as pd\n\n# Create a dictionary with the data\ndata = {\n    \"Description\": [\n        \"First is 3 or more, second is 4 or more\",  # valid outcome\n        \"First is 4 or more, second is 3 or more\",  # valid outcome\n        \"First is 3 or more, second is 3 or LESS\",  # invalid outcome\n        \"First is 4 or more, second is 2 or LESS\",  # invalid outcome\n        \"First is 3 or LESS, second is 4 or more\",  # invalid outcome\n        \"First is 2 or LESS, second is 3 or more\",  # invalid outcome\n        \"First is 2 or LESS, second is 2 or LESS\"   # invalid outcome\n    ],\n    \"likelihood\": [\n        (4 / 6) * (3 / 6),  # valid outcome\n        (3 / 6) * (4 / 6),  # valid outcome\n        (3 / 6) * (3 / 6),  # invalid outcome\n        (3 / 6) * (2 / 6),  # invalid outcome\n        (3 / 6) * (3 / 6),  # invalid outcome\n        (2 / 6) * (4 / 6),  # invalid outcome\n        (2 / 6) * (2 / 6)   # invalid outcome\n    ]\n}\n\n# Create a DataFrame from the dictionary\ndice_outcomes = pd.DataFrame(data)\n\n# Print the DataFrame\nprint(dice_outcomes.to_markdown(index=False))\n\n# Calculate the sum of the \"likelihood\" column\ntotal_likelihood = dice_outcomes[\"likelihood\"].sum()\ntotal_likelihood\n\n\n\n\nTable with probabilities\n\n\n1.6666666666666665\n\n\n\nWhen we add all the outcome likelihoods together we get more than 1, which makes me (and maybe you) sad. This reflects us (or me, you didn’t ask me to make the above table) treating some outcomes as if they are exclusive when they are not. For example, the two valid outcomes have overlap as they both include the likelihoods that both dice are of 4 or more. This has inflated the likelihood of the valid outcomes when you add these outcomes together, as you have doubled up on the overlap. To visualise this:\n\nRPython\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_rect(\n    mapping = aes(\n      xmin = 2.5,\n      xmax = 6.5,\n      ymin = 3.5,\n      ymax = 6.5\n    ),\n    fill = \"blue\",\n    alpha = .5\n  ) +\n  geom_rect(\n    mapping = aes(\n      xmin = 3.5,\n      xmax = 6.5,\n      ymin = 2.5,\n      ymax = 6.5\n    ),\n    fill = \"red\",\n    alpha = .5\n  ) +\n  theme_bw() +\n  theme(\n    plot.background = element_blank(),\n    #panel.grid.minor = element_line(color = \"black\", linewidth =  2),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank()\n  ) + \n  coord_fixed() +\n  xlab(\"First roll\") +\n  ylab(\"Second roll\") + \n  geom_vline(\n    xintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),\n    color=\"white\"\n  ) + \n  geom_hline(\n    yintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),\n    color=\"white\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 6, by = 1)) +\n  scale_y_continuous(breaks = seq(0, 6, by = 1)) \n\n\n\n\nFigure 1: a summary of the overlap between 2 outcomes. The likelihood of any particular dice roll is 1 out of 36 (see how there are 36 combinations in the above grid), and so you could calculate the likelihood of one dice being at least 3 and the other being at least 4 by making a grid like above and then calculating what proportion of the area is covered by valid rolls. If half of your rolls are valid, then half of the area would be covered. In our case, 15 out of 36 possible combinations is valid, and so 15/36 of the area in the above grid is your likelihood. However, this figure also visualises how mathematical shortcuts described above of the likelihood of getting an outcome can be incorrect if you make false assumptions about whether outcomes are mutually exclusive. As we can see above, the likelihood of dice 1 being 3 or above and dice 2 being 4 or above (blue) is NOT exclusive from the likelihood of dice 1 being 3 and above and dice being 3 and above (red), which is why the purple squares reflect the overlap between these outcomes.\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Define the rectangles\nrectangles = [\n    plt.Rectangle((2.0, 3.0), 4, 3, color='blue', alpha=0.5),\n    plt.Rectangle((3.0, 2.0), 3, 4, color='red', alpha=0.5)\n]\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Add rectangles to the plot\nfor rect in rectangles:\n    ax.add_patch(rect)\n\n# Set axis limits\nax.set_xlim(0, 6)\nax.set_ylim(0, 6)\n\n# Customize the plot\nax.set_aspect('equal', 'box')\nax.set_xlabel('First roll')\nax.set_ylabel('Second roll')\nax.set_xticks(range(7))\nax.set_yticks(range(7))\nax.grid(which='major', linestyle='-', linewidth='2', color='white')\nax.grid(which='minor', linestyle='-', color='white')\n\n# Remove panel borders\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\n# Show the plot\nplt.show()\n\n\n\n\nFigure 1: a summary of the overlap between 2 outcomes. The likelihood of any particular dice roll is 1 out of 36 (see how there are 36 combinations in the above grid), and so you could calculate the likelihood of one dice being at least 3 and the other being at least 4 by making a grid like above and then calculating what proportion of the area is covered by valid rolls. If half of your rolls are valid, then half of the area would be covered. In our case, 15 out of 36 possible combinations is valid, and so 15/36 of the area in the above grid is your likelihood. However, this figure also visualises how mathematical shortcuts described above of the likelihood of getting an outcome can be incorrect if you make false assumptions about whether outcomes are mutually exclusive. As we can see above, the likelihood of dice 1 being 3 or above and dice 2 being 4 or above (blue) is NOT exclusive from the likelihood of dice 1 being 3 and above and dice being 3 and above (red), which is why the purple squares reflect the overlap between these outcomes.\n\n\n\n\n\nIn the above visualisation of all possible dice role combinations, the proportion of the area covered reflects the likelihood of each valid combination highlighted: “First is 3 or more, second is 4 or more” in blue and “First is 4 or more, second is 3 or more” in red. As you can see, there is a lot of overlap (purple/pink), reflecting that these were not mutually exclusive possibilities, and so adding them together will inflate the estimated likelihood of either of them happening.\nAn elegant solution is to subtract the likelihood associated with repetition from your original calculation:\n\nRPython\n\n\n\n(4/6) * (3/6) + # \"First is 3 or more, second is 4 or more\", # valid outcome\n(3/6) * (4/6) - # \"First is 4 or more, second is 3 or more\", # valid outcome\n(3/6) * 3/6     # overlap between the two valid outcomes above \n\n[1] 0.4166667\n\n\n\n\n\noutcome1 = (4 / 6) * (3 / 6)  # \"First is 3 or more, second is 4 or more\", valid outcome\noutcome2 = (3 / 6) * (4 / 6)  # \"First is 4 or more, second is 3 or more\", valid outcome\noverlap = (3 / 6) * (3 / 6)   # overlap between the two valid outcomes above\n\noutcome1+outcome2-overlap\n\n0.41666666666666663\n\n\n\nTo confirm this is correct, we can use R to count how many times out of 36 the dice will be valid:\n\nRPython\n\n\n\nvalid_dice <- matrix(FALSE,nrow = 6,ncol = 6)\nfor(i in 1:6){\n  for(j in 1:6){\n    if(i >= 3 & j >= 4){\n      valid_dice[i,j] = TRUE\n    }\n    if(i >= 4 & j >= 3){\n      valid_dice[i,j] = TRUE\n    }\n  }\n}\nsum(valid_dice)/ # number of valid roll combinations\n36               # total number of roll combinations\n\n[1] 0.4166667\n\n\n\n\n\nimport numpy as np\n\nvalid_dice = np.zeros((6, 6), dtype=bool)\n\nfor i in range(6):\n    for j in range(6):\n        if i >= 2 and j >= 3:\n            valid_dice[i, j] = True\n        if i >= 3 and j >= 2:\n            valid_dice[i, j] = True\n\nnp.sum(valid_dice)/36\n\n0.41666666666666663\n\n\n\nWe get the same output."
  },
  {
    "objectID": "statsBasics/probability.html#the-likelihood-of-something-not-happening-is-often-a-useful-shortcut",
    "href": "statsBasics/probability.html#the-likelihood-of-something-not-happening-is-often-a-useful-shortcut",
    "title": "Probability (R)",
    "section": "The likelihood of something NOT happening is often a useful shortcut",
    "text": "The likelihood of something NOT happening is often a useful shortcut\nAs shown above, calculating of the probabilities can be done in more and less reliable and elegant ways. Generally speaking, an elegant calculation is less likely to result in error. One common example of how to be more elegant is how we address the chance of avoiding an undesirable outcome, such as a false-positive. If we ran three tests on random data, and accepted an \\(\\alpha\\) threshold of .05 (i.e. a 5% chance that we would incorrectly identify an effect), then there are (at least) 2 ways you could calculate the likelihood of at least 1 false positive. The slower way:\n\\[\nfp_1 + fp_2 + fp_3  + fp_{1,2} + fp_{1,3} + fp_{2,3} + fp_{1,2,3}\n\\]\n\n\\(fp_1\\) refers to a false positive only for test 1\n\\(fp_2\\) refers to a false positive only for test 2\n\\(fp_3\\) refers to a false positive only for test 3\n\\(fp_{1,2}\\) refers to a false positive only for tests 1 and 2\n\\(fp_{1,3}\\) refers to a false positive only for tests 1 and 3\n\\(fp_{2,3}\\) refers to a false positive only for tests 2 and 3\n\\(fp_{1,2,3}\\) refers to a false positive for all three tests\n\n\nRPython\n\n\n\nat_least_1_fp = \n.05 * .95 * .95 + # false positive only for test 1\n.95 * .05 * .95 + # false positive only for test 2\n.95 * .95 * .05 + # false positive only for test 3\n.05 * .05 * .95 + # false positive only for test 1 and 2\n.05 * .95 * .05 + # false positive only for test 1 and 3\n.95 * .05 * .05 + # false positive only for test 2 and 3\n.05 * .05 * .05   # false positive only for test all tests\nat_least_1_fp\n\n[1] 0.142625\n\n\n\n\n\nfalse_positive_probabilities = [\n    0.05 * 0.95 * 0.95,  # false positive only for test 1\n    0.95 * 0.05 * 0.95,  # false positive only for test 2\n    0.95 * 0.95 * 0.05,  # false positive only for test 3\n    0.05 * 0.05 * 0.95,  # false positive for test 1 and 2\n    0.05 * 0.95 * 0.05,  # false positive for test 1 and 3\n    0.95 * 0.05 * 0.05,  # false positive for test 2 and 3\n    0.05 * 0.05 * 0.05   # false positive for all tests\n]\n\nat_least_1_fp = 1 - (1 - sum(false_positive_probabilities))\nat_least_1_fp\n\n0.142625\n\n\n\nTo check this is correct, when we add the above number to the likelihood of 0 false-positives we should get 1:\n\nRPython\n\n\n\n.95 * .95 * .95 + # zero false positives\n  at_least_1_fp\n\n[1] 1\n\n\n\n\n\n.95 * .95 * .95 + at_least_1_fp\n\n1\n\n\n\nBut, we could simply subtract the likelihood of zero false positives from 1:\n\nRPython\n\n\n\n1 - (.95 * .95 * .95)\n\n[1] 0.142625\n\n\n\n\n\n1 - (.95 * .95 * .95)\n\n0.142625\n\n\n\nand get the same result.\n\n\n\n\n\n\nInconsistent p-value thresholds\n\n\n\nNote that it’s easier to do simple calculations like above if the p-value thresholds are consistent (e.g. .05 in this case), whereas inconsistent p-value thresholds between outcomes can lead to false assumptions about mutual exclusivity as described above."
  },
  {
    "objectID": "statsBasics/probability.html#permutation-analyses",
    "href": "statsBasics/probability.html#permutation-analyses",
    "title": "Probability (R)",
    "section": "Permutation analyses",
    "text": "Permutation analyses\n\n\n\n\n\n\nRabbit Hole 🐰 🕳️\n\n\n\nMost psychology statistics books do not address this topic, so if you do not need to understand it to proceed with later pages.\n\n\nPermutations refer to specific combinations of outcomes. To calculate the likelihood of permutations you need to calculate the number of valid outcomes that matches it (e.g. how many ways can you flip a coin to make 2 heads and 1 tails) and divided by the number of all possible permutations. Thus we need to calculate 2 things:\n\n\\(validPerms\\): How many combinations are there in which heads was flipped 2 times and tails 1 times\n\\(allPerms\\): How many unique combinations/permutations can you get when flipping a coin 3 times (not restricted to 2 heads and 1 tails)\n\nWe then divide \\(validPerms\\) by \\(allOutcomes\\) to calculate the likelihood of getting 2 out of 3 coins flipped as heads:\n\\[\n\\frac{validPerms}{allOutcomes} = \\frac{nCr}{2^n}\n\\]\n\nValid permutations\n\n\n\n\n\n\n! means factorial\n\n\n\nWe start using the ! sign for factorial below. Factorial means that you multiply a number by every integer that is smaller than it before you reach 0. Some examples:\n5! = 5 * 4 * 3 * 2 * 1 = 120\n3! = 3 * 2 * 1 = 6\n1! = 1\nNote that languages like R use ! to indicate NOT, so make sure to use the appropriate function (e.g. “factorial” in R)\n\n\nThe formula for calculating how many valid permutations can be thought of as a mathematical shortcut to calculate how many permutations of all coins there are, and then a filter to only focus on unique permutations of heads and tails combinations:\n\\[\n\\frac{allPermutations}{uniqueValidCombinations} = \\frac{n!}{heads!tails!}\n\\]\n\n\\(n\\) is the number of flips total\n\\(heads\\) is the number of heads flips\n\\(tails\\) is the number of tail flips\n\nOne way to understand the formula (in admittedly broad terms), is that the top half of the equation needs to capture all the ways coins can be combined, and the bottom half controls for the fact that some coins are identical as heads or tails. To illustrate the first half, lets give each coin an identity, coin 1, 2 and 3:\n\n\n\nFlip 1\nFlip 2\nFlip 3\n\n\n\n\nCoin 1\nCoin 2\nCoin 3\n\n\nCoin 1\nCoin 3\nCoin 2\n\n\nCoin 2\nCoin 1\nCoin 3\n\n\nCoin 2\nCoin 3\nCoin 1\n\n\nCoin 3\nCoin 1\nCoin 2\n\n\nCoin 3\nCoin 2\nCoin 1\n\n\n\nSo there are 6 permutations. Fortunately, we don’t need to draw out all permutations each time, we can calculate this through a simple iterative process:\n\nThe first coin will have 3 positions it can be in\nThe second coin will have 2 positions it can be in\nThe third coin will only have 1 position it can be in\n\nThis means you can multiply these together to get the total possible combinations of coin positions: 3 * 2 * 1, or more elegantly written as 3! (see factorials above).\nNow that we’ve used this technique to get every permutation of coins, we can now allocate heads and tails to the coins we have, even though this will end with repetition (which we will deal with later). In this example, let’s say coins 1 and 2 are heads, and coin 3 is tails. Our grid will now look as follows:\n\n\n\nFlip 1\nFlip 2\nFlip 3\n\n\n\n\nHeads\nHeads\nTails\n\n\nHeads\nTails\nHeads\n\n\nHeads\nHeads\nTails\n\n\nHeads\nTails\nHeads\n\n\nTails\nHeads\nHeads\n\n\nTails\nHeads\nHeads\n\n\n\nWhilst this is somewhat helpful, there’s some redundant rows which are identical to each other. One way to calculate this is to calculate the permutations of coins within heads and tails separately, as the number of permutations there are within each of these captures how many repetitions there are driven by multiple instances of each type of coinflip. If that’s not clear, perhaps the following will help:\nCoins 1 and 2 are heads, so we can see that 2! (2 * 1 = 2) reflects the only 2 permutations:\n\n\n\nFlip 1\nFlip 2\n\n\n\n\nCoin 1\nCoin 2\n\n\nCoin 2\nCoin 1\n\n\n\nThese 2 permutations will be “heads” in the table above, and so we know that we can divide the total number of permutations by 2 as half the sequences have coin 1 first and coin 2 later, and the other half have coin 2 first and coin 1 later. As there is only 1 tail coin you would only divide the permutations by 1! which is 1 anyway. So back to the formula:\n\\[\n\\frac{n!}{heads!*tails} = \\frac{3!}{2! * 1!} = \\frac{3*2*1}{2*1 * 1} = \\frac{6}{2} = 3\n\\]\n3 permutations of 2 heads and 1 tail. Going back to the formula above:\n\\[\n\\frac{validPerms}{allOutcomes} = \\frac{3}{2^n} = \\frac{3}{8}\n\\]\nSo there’s a 3 in 8 chance of flipping 2 heads and 1 tails.\n\nQuestion 1\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/basicsQuestions.html",
    "href": "statsBasics/basicsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\nAlpha values are…\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Scientific notation is used to express very large or very small numbers in a concise format using exponents of 10 (\\(10^n\\)). So, rather than having to show all of the digits for \\(4,600,000,000\\) (4.6 Trillion) it can be expressed in scientific notation: \\(4.6 \\times 10^9\\). Likewise, very small numbers can be expressed using the same format, e.g. \\(0.0000005\\) = \\(5.0 \\times 10^{-7}\\).\nNote: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n3.1e3 == 3.1 * 10^3\n\n[1] TRUE\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n2.5e-3 == 2.5 * 10^(-3)\n\n[1] TRUE\n\n\nHowever, if you would like to turn off scientific notation in R you can type:\n\noptions(scipen=999)\n\n\nQuestion 1\nWhich is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/eNumbersQuestions.html",
    "href": "statsBasics/eNumbersQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "statsBasics/probabilityQuestions.html",
    "href": "statsBasics/probabilityQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "templates/ojs.html",
    "href": "templates/ojs.html",
    "title": "ojs",
    "section": "",
    "text": "Note that you won’t be able to preview figures created using this\n\ndata = FileAttachment(\"palmerpenguins.csv\").csv({ typed: true })\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\nathletes = FileAttachment(\"athletes.csv\").csv({typed: true})\n\n\ndotplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndotplot = Plot.dot(athletes, {x: \"weight\", y: \"height\", stroke: \"sex\"}).plot()\n\n\n//dotplot.legend(\"color\")\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"bill_depth_mm\", stroke: \"species\"}).plot()\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"count\", stroke: \"sex\"}).plot()\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  )\n).plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "templates/tabsets.html",
    "href": "templates/tabsets.html",
    "title": "tabsets template",
    "section": "",
    "text": "You can use Python and R within Quarto files\n\nRPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n\n1 + 2\n\n3"
  },
  {
    "objectID": "regressions/simpleRegressionsQuestions.html",
    "href": "regressions/simpleRegressionsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nIs an r-value a standardised or unstandardised estimate of the association between a predictor and an outcome?\n\nviewof simple_regression_1_response = Inputs.radio(['Standardised','Unstandardised']);\ncorrect_simple_regression_1 = 'Standardised';\nsimple_regression_1_result = {\n  if(simple_regression_1_response == correct_simple_regression_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "href": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "title": "Multi-collinearity (incomplete)",
    "section": "Measuring multi-collinearity using Variance Inflation Factor (VIF)",
    "text": "Measuring multi-collinearity using Variance Inflation Factor (VIF)\n\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\ngdp_pop_predict_lifeExp <- lm(\n  formula = lifeExp ~ pop + gdpPercap,\n  data = gapminder_2007\n    \n)\n\n\ngdp_pop_predict_lifeExp$coefficients\n\n (Intercept)          pop    gdpPercap \n5.920520e+01 7.000961e-09 6.416085e-04 \n\npred_lm <- lm(lifeExp ~ pop, gapminder_2007)\npred_lm$coefficients[2]\n\n         pop \n3.889069e-09 \n\n1-sqrt(pred_lm$coefficients[2])\n\n      pop \n0.9999376 \n\nvif(gdp_pop_predict_lifeExp)\n\n      pop gdpPercap \n 1.003109  1.003109"
  },
  {
    "objectID": "regressions/multipleRegressionsQuestions.html",
    "href": "regressions/multipleRegressionsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nA multiple regression allows you to control for overlap between predictors in how they predict an outcome variable\n\nviewof multiple_regressions_1_response = Inputs.radio(['True','False']);\ncorrect_multiple_regressions_1 = 'True';\nmultiple_regressions_1_result = {\n  if(multiple_regressions_1_response == correct_multiple_regressions_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "regressions/coding.html",
    "href": "regressions/coding.html",
    "title": "Coding (R)",
    "section": "",
    "text": "When running a linear model, such as a simple regression or multiple regression, it can be helpful to include categorical variables. However, these categorical variables will not (usually?) have sensible numbers that can be associated with them, and predictors in regressions need to be numeric. We can address this by using dummy or effect coding. We’ll start by addressing this for binary categorical variables."
  },
  {
    "objectID": "regressions/coding.html#dummy-coding",
    "href": "regressions/coding.html#dummy-coding",
    "title": "Coding (R)",
    "section": "Dummy coding",
    "text": "Dummy coding\nFor categorical variables, you can give them a value of 0 or 1 if they are binary to reflect which of the two options applies to a data point. For example, if you had female and male participants (we’ll deal with having more than 2 genders later), you could give 0 to females, and 1 to males. This would make females the default or baseline gender, and so your model would tell you how you expect height to increase (or decrease) for a male compared to a female.\n\ncoding_df = data.frame(\n  height = c(100,115,120,105),\n  gender = c(\"female\",\"male\",\"male\",\"female\"),\n  gender_dummy = c(0,1,1,0)\n)\nknitr::kable(coding_df)\n\n\n\n\nheight\ngender\ngender_dummy\n\n\n\n\n100\nfemale\n0\n\n\n115\nmale\n1\n\n\n120\nmale\n1\n\n\n105\nfemale\n0\n\n\n\n\n\nSo if we wanted to predict height of an individual by their gender, then we could generate a linear model with the dummy coded gender as a predictor:\n\ndummy_lm = lm(height ~ gender_dummy, coding_df)\nsummary(dummy_lm)\n\n\nCall:\nlm(formula = height ~ gender_dummy, data = coding_df)\n\nResiduals:\n   1    2    3    4 \n-2.5 -2.5  2.5  2.5 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   102.500      2.500  41.000 0.000594 ***\ngender_dummy   15.000      3.536   4.243 0.051317 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.536 on 2 degrees of freedom\nMultiple R-squared:    0.9, Adjusted R-squared:   0.85 \nF-statistic:    18 on 1 and 2 DF,  p-value: 0.05132\n\n\nThe Intercept is the mean height of females (as females are the default gender here). The estimate for gender dummy tells you how height increases by 15cm if you go from 0 (female) to 1 (male). This gives you similar insights into a t-test (see below). If you were to plot it, females would be at 0, and males at 1:\n\nplot(height ~ gender_dummy, coding_df)\nabline(lm(height ~ gender_dummy, coding_df))\n\n\n\n\nYou’ll be happy to know that you get the same p-values for this linear model (look at the Pr(>|t|) value for gender_dummy)…\n\nthis_summary = summary(dummy_lm)\nthis_summary$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)     102.5   2.500000 41.000000 0.0005943537\ngender_dummy     15.0   3.535534  4.242641 0.0513167019\n\n\n… as you do for a t-test:\n\nt.test(height ~ gender_dummy, coding_df, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  height by gender_dummy\nt = -4.2426, df = 2, p-value = 0.05132\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -30.2121746   0.2121746\nsample estimates:\nmean in group 0 mean in group 1 \n          102.5           117.5 \n\n\n\nEffect coding\nEffect coding is similar to dummy coding except that instead of giving 0 to one group you give -1 to it. This effect coded variable will act as a binary contrast between both levels, with neither level being the default/baseline (Some advantages/disadvantages of effect/dummy coding are addressed within the multiple regressions page). Effect coding gives you the same p-value as dummy coding:\n\ncoding_df$gender_effect = ifelse(coding_df$gender == \"female\", -1, 1)\neffect_lm = lm(height ~ gender_effect, coding_df)\nsummary(effect_lm)\n\n\nCall:\nlm(formula = height ~ gender_effect, data = coding_df)\n\nResiduals:\n   1    2    3    4 \n-2.5 -2.5  2.5  2.5 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    110.000      1.768  62.225 0.000258 ***\ngender_effect    7.500      1.768   4.243 0.051317 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.536 on 2 degrees of freedom\nMultiple R-squared:    0.9, Adjusted R-squared:   0.85 \nF-statistic:    18 on 1 and 2 DF,  p-value: 0.05132\n\n\nNote though that because there is no baseline level, the baseline is half way between females and males. The Intercept is now half way between the mean for females and males, and the estimate for gender effect is half the difference. This is because the contrast between females (-1) and males (1) is twice that for effect coding than dummy coding:\n\nplot(height ~ gender_effect, coding_df)\nabline(lm(height ~ gender_effect, coding_df))"
  },
  {
    "objectID": "regressions/simpleRegressions.html",
    "href": "regressions/simpleRegressions.html",
    "title": "Simple regression (R)",
    "section": "",
    "text": "this_page = \"simple_regression\""
  },
  {
    "objectID": "regressions/simpleRegressions.html#prediction-using-regression",
    "href": "regressions/simpleRegressions.html#prediction-using-regression",
    "title": "Simple regression (R)",
    "section": "Prediction using regression",
    "text": "Prediction using regression\nSimple regression, also known as linear regression, uses some overlapping concepts with correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\nFigure 1: A scatterplot of the association between GDP and Life Expectancy in 2007 using Gapminder data. A line of best fit is included (blue).\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression can’t tell us exactly what is influencing what (i.e. whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\ngradient reflects how steep the line is. This is also known as a beta value or estimate (of the beta) by linear model functions.\nintercept is the point at which the regression line crosses the y-axis, i.e. what the y-value is when x = 0.\n\nLet’s use coding magic to find out the intercept and the gradient (AKA slope):\n\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nCoefficients are either the intercept (\\(y\\)-value when \\(x\\) = 0) or the beta/gradient values for each predictor. The above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more intuitive terms, for every extra $10,000 dollars per person the GDP increases, the life expectancy goes up by 6 years).\nThe above equation allows us to predict y-values from the graph, but not perfectly, leaving residuals. A more complete formula for the outcome can be represented as follows:\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nresidual reflects what’s left over, and is not represented in the line of best fit formula because you can’t predict what’s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\nFigure 2: A scatterplot of the association between GDP and Life Expectancy in 2007 using Gapminder data. A line of best fit is included (blue) and residuals are highlighted with pink lines between this line of best fit and each data point. These lines visualise how inaccurate the model was.\n\n\n\n\nThese residuals can be thought of the error, i.e. what the model failed to predict. In more mathematical terminology, the model would be:\n\\[\nY = a + bX + e\n\\]\n\n\\(a\\) is the intercept\n\\(b\\) is the gradient (AKA beta AKA estimate)\n\\(e\\) is the error (i.e. the residual of what the formula doesn’t predict)\n\nRegressions capture how successful the prediction is compared to the error, by calculating the total variance, the variance explained by the model and thus the proportion of variance explained by the model."
  },
  {
    "objectID": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "href": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "title": "Simple regression (R)",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\nIn correlations we discussed how the strength of association is the proportion of variance of y explained by x. For simple regression, in which there is only 1 predictor, this is also the case:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply the above formula to see what R is for \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\ngdp_life_expectancy_r <- sum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap))\n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) \n  )\ngdp_life_expectancy_r\n\n[1] 0.6786624\n\n\nThis is r. If this is the same as a correlation, then we can just use a correlational function to get the same value:\n\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)\n\n[1] 0.6786624\n\n\nGreat! Now in regressions we tend to report the \\(R^2\\) rather than r.\n\n# r^2\ngdp_life_expectancy_r^2 \n\n[1] 0.4605827\n\n\nWe can confirm that squaring \\(r\\) gives \\(R^2\\) as from a linear model function:\n\nsummary(lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n))$r.squared\n\n[1] 0.4605827"
  },
  {
    "objectID": "regressions/simpleRegressions.html#how-to-get-gradientbetaestimate-values",
    "href": "regressions/simpleRegressions.html#how-to-get-gradientbetaestimate-values",
    "title": "Simple regression (R)",
    "section": "How to get gradient/beta/estimate values",
    "text": "How to get gradient/beta/estimate values\nYou may be wondering how the gradient/beta/estimate is calculated in the above figures. The formula for calculating the gradient is:\n\\[\ngradient = r * \\frac{sd(outcome)}{sd(predictor)}\n\\]\nWhich can be thought of as:\n\\[\ngradient = association * scale\n\\]\nYour r-value is a standardised value showing the strength of the association that ignores scale; i.e. it is a number between -1 and 1 regardless of what units and range of numbers are typical for the variables. The scale for the above correlation is years (life expectancy) per dollar (gdp), so by dividing the SD of life expectancy by the SD of gdp we get a measure of how these variables scale to each other. In terms of how life expectancy scales against gdp:\n\\[\n\\frac{sd(lifeExp)}{sd(gdp)} = \\frac{12.07302}{12859.94} = .0009388084\n\\]\n…and then apply the strength of the association to get your gradient/beta:\n\\[\ngradient = r * \\frac{sd(lifeExp)}{sd(gdp)} = .6786624 * .0009388084 = .000637134\n\\]\nIs this the same as the estimate of gradient/beta we get using a linear model function?\n\nsummary(lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n))\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.828  -6.316   1.922   6.898  13.128 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(>|t|)    \n(Intercept) 59.56565008  1.01040864   58.95 <0.0000000000000002 ***\ngdpPercap    0.00063713  0.00005827   10.93 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.899 on 140 degrees of freedom\nMultiple R-squared:  0.4606,    Adjusted R-squared:  0.4567 \nF-statistic: 119.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nYes (see estimate for gdpPercap).\n\nQuestion 1\nIs an r-value a standardised or unstandardised estimate of the association between a predictor and an outcome?\n\nviewof simple_regression_1_response = Inputs.radio(['Standardised','Unstandardised']);\ncorrect_simple_regression_1 = 'Standardised';\nsimple_regression_1_result = {\n  if(simple_regression_1_response == correct_simple_regression_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "regressions/multipleRegressions.html",
    "href": "regressions/multipleRegressions.html",
    "title": "Multiple Regressions (R)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started\n\n\n\n\nCreating a model to predict your outcome\nMultiple regressions build on the principles of simple regression, but allows you to investigate multiple predictors. Importantly, it allows you to control for overlap between the predictors on the outcome variables, and thus identify how much variance each predictor individually explains. To illustrate the principles of multiple regression we’ll use the gapminder data from 2007, to look at predictors of life expectancy.\n\n# to make numbers more readable we are turning off e-numbers\noptions(scipen = 999)\n\n# load gapminder data\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder, \n  year==2007\n)\nrmarkdown::paged_table(gapminder_2007)\n\n\n\n  \n\n\n\nLet’s investigate if life expectancy is well predicted by population and gdp of a country. The challenge is that if you do 2 simple regressions, you don’t have any insight into how much overlap there is in how population and GDP predict life expectancy. One way to visualise this is a venn diagram of overlapping variance:\n\nlibrary(eulerr)\nset.seed(3)\n\n# Overview\nplot(\n  euler(\n    pos_neg_1_vd <- list(\n      \"Life Expectancy\" = 0:50,\n      \"GDP\" = 45:90,\n      \"Population\" = c(40:45,90:130)\n    ),\n    shape = \"ellipse\"\n  ), \n  fills = list(\n    fill = c(\n      \"dark green\", # Positive\n      \"purple\",      # False Negative\n      \"light blue\"\n    ),\n    alpha = 0.5\n  ), \n)\n\n\n\n\nWe can see in the above figure that there is some covariance between GDP and population, and GDP and life expectancy, and that there is some covariance between all three. To identify how GDP predicts life expectancy after controlling for overlap of GDP and life expectancy with population, we need to capture:\n\nwhat is the variance in GDP that is not explained by population?\nwhat is the variance in life expectancy that is not explained by population?\n\nIf all goes well, our new venn diagram should look something like:\n\nIn which we’ve cancelled out variance in population’s overlap with life expectancy and GDP.\nTo calculate variance in GDP not explained by population, we will need to capture the residuals of GDP left over that are not explained by the population as a predictor.\n\npop_pred_gdp <- lm(gdpPercap ~ pop,gapminder_2007)\ngdp_without_pop = pop_pred_gdp$residuals # i.e. variance in GDP not explained by population\n\nTo confirm we have now captured variance in GDP that is not explained by population, we can test if there’s any association with this new variable with population (there shouldn’t be):\n\n# before controlling variance in gdp for population\ncor.test(gapminder_2007$pop, gapminder_2007$gdpPercap)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and gapminder_2007$gdpPercap\nt = -0.65979, df = 140, p-value = 0.5105\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2183999  0.1100611\nsample estimates:\n       cor \n-0.0556756 \n\n# after controlling variance in gdp for population\ncor.test(gapminder_2007$pop, gdp_without_pop)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and gdp_without_pop\nt = 0.00000000000000061609, df = 140, p-value = 1\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1647273  0.1647273\nsample estimates:\n                      cor \n0.00000000000000005206931 \n\n\nThe association is pretty close to zero once you’ve controlled for co-variance between gdp and population, showing that you have controlled for co-variance. The fact it isn’t quite zero is a symptom of a rounding error rather than any meaningful association left over. To visualise this (lack of) association:\n\nlibrary(\"ggplot2\")\nggplot(gapminder_2007, aes(x = pop, y = gdp_without_pop)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\")\n\n\n\n\nPretty flat line, confirming there’s no covariance/association between population and GDP once you’ve controlled for GDPs covariance with population. We will be calculating the variance of more variables after controlling for covariance with other variables, and if you were to do similar calculations of correlations and plots you would expect to find the same thing again and again, that there’s no remaining association between the newly calculated variable and the predictor that has been controlled for.\nNext, we need the residuals of life expectancy that isn’t predicted by population\n\npop_pred_life_exp <- lm(lifeExp ~ pop,gapminder_2007)\nlifeExp_without_pop = pop_pred_life_exp$residuals # i.e. variance in life expectancy not explained by population\n\nNow we can conduct a regression of the residuals for gdp as a predictor for the residuals of life expectancy:\n\nsummary(lm(lifeExp_without_pop ~ gdp_without_pop))\n\n\nCall:\nlm(formula = lifeExp_without_pop ~ gdp_without_pop)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                            Estimate           Std. Error t value\n(Intercept)     0.000000000000001335 0.741683948900451084    0.00\ngdp_without_pop 0.000641608516800057 0.000057968064012284   11.07\n                           Pr(>|t|)    \n(Intercept)                       1    \ngdp_without_pop <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.838 on 140 degrees of freedom\nMultiple R-squared:  0.4667,    Adjusted R-squared:  0.4629 \nF-statistic: 122.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nSo we now have calculated that GDP is a significant predictor of life expectancy even when controlling for overlapping variance it has with population. Let’s check if population is a significant predictor for life expectancy once you control for gdp. We’ll need to calculate the:\n\nvariance in population not explained by GDP\n\n\ngdp_pred_pop <- lm(pop ~ gdpPercap,gapminder_2007)\npop_without_gdp <- gdp_pred_pop$residuals # i.e. variance in population not explained by GDP \n\n\nvariance in life expectancy not explained by GDP\n\n\ngdp_pred_lifeExp <- lm(lifeExp ~ gdpPercap,gapminder_2007)\nlifeExp_without_gdp <- gdp_pred_lifeExp$residuals # i.e. variance in life expectancy not explained by GDP\n\nNow we can predict the residuals for life expectancy using the residuals for population that are not explained by GDP\n\nsummary(lm(lifeExp_without_gdp ~ pop_without_gdp))\n\n\nCall:\nlm(formula = lifeExp_without_gdp ~ pop_without_gdp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                               Estimate              Std. Error t value\n(Intercept)     -0.00000000000000003869  0.74168394890045108436   0.000\npop_without_gdp  0.00000000700096092366  0.00000000504984833741   1.386\n                Pr(>|t|)\n(Intercept)        1.000\npop_without_gdp    0.168\n\nResidual standard error: 8.838 on 140 degrees of freedom\nMultiple R-squared:  0.01354,   Adjusted R-squared:  0.006497 \nF-statistic: 1.922 on 1 and 140 DF,  p-value: 0.1678\n\n\nIt looks like population is not a significant predictor of life expectancy once you control for their overlap with GDP. (Which is actually also the case even without controlling for population)\nImportantly, we have now taken some steps to calculate the gradient for population (.000000007) and gdp (.0006416085) as predictors of life expectancy. We can compare the results of the above to a multiple regression function to see if the estimate is the same:\n\nsummary(lm(lifeExp ~ pop + gdpPercap, gapminder_2007))\n\n\nCall:\nlm(formula = lifeExp ~ pop + gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                   Estimate      Std. Error t value            Pr(>|t|)    \n(Intercept) 59.205198140717  1.040398672164  56.906 <0.0000000000000002 ***\npop          0.000000007001  0.000000005068   1.381               0.169    \ngdpPercap    0.000641608517  0.000058176209  11.029 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.87 on 139 degrees of freedom\nMultiple R-squared:  0.4679,    Adjusted R-squared:  0.4602 \nF-statistic: 61.11 on 2 and 139 DF,  p-value: < 0.00000000000000022\n\n\nThe estimates for population (.000000007001) and GDP (.000641608517) are the same for both the step-by-step regressions above and the multiple regression directly above.\nThe principles above can be extended to more and more complex multiple regressions - you address the covariance of one predictor at a time until you’re left with only variance in your predictor and outcome variable that is no longer explained by any other predictor. Ultimately you are working towards being able to predict the outcome from one or more predictors, making a formula with the following structure:\n\\[\nY = a + b_1X_i + b_2X_2 + e\n\\]\n\n\\(a\\) is the intercept, i.e. what Y is when \\(X_1\\) and \\(X_2\\) are 0. We won’t be going through how this is calculated as this value becomes less and less insightful the more predictors you have.\n\\(b_1\\) is the coefficient for the first variable\n\\(b_2\\) is the coefficient for the second variable\n\\(X_1\\) is the value of the first variables data point.\n\\(X_2\\) is the value of the second variables data point.\n\\(e\\) is error, i.e. the difference between your predicted \\(Y\\) value using the rest of the formula and the actual \\(Y\\) value.\n\nOr, using the coefficients from our calculations above, we could predict:\n\\[\nLifeExp = 59.205198140717 + .000000007001*X_i + .000641608517*X_i + e\n\\]\nNow that you have your model, the next challenge is to test whether it’s significantly better than the mean as a model.\n\n\nTesting how significant your model is\nNow that you have a model, we can investigate how much variance is explained by it to calculate \\(R^2\\). First, let’s establish the total variance we aim to explain, i.e. the Total Sum of Squares (\\(SSTotal\\)):\n\\[\nSSTotal = \\sum{(x_i-\\bar{x})}\n\\]\n\nsstotal = sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\nWhilst we want to capture how much variance is explained by the model, it can be easier to first of all summarise the variance that is not explained by it, i.e. the residuals left over. This is an extension of getting residuals in a simple regression, but is a bit harder to visualise as there are 2 predictors. However, the principle is the same, so to capture all the variance that isn’t explained we sum the square of the \\(e\\) in the above model, i.e. the residual left over that isn’t explained by the model.\nR can give us the Sum of Square Error (SSError) easily enough:\n\nsserror = sum(summary(lm(lifeExp ~ pop + gdpPercap, gapminder_2007))$residuals ^ 2)\n\nWe can now deduce that any variance that is not error is explained by the model, and so the Sum Square of the Model (SSM) is:\n\\[\nSSModel = SSTotal - SSError\n\\]\nSo let’s calculate that now:\n\nssmodel = sstotal - sserror\n\n\\(R^2\\) is our statistic for how much variance is explained by the model, and so the formula reflects the proportion of total variance that is explained by the model, i.e.:\n\\[\nR^2 = \\frac{SSModel}{SSTotal}\n\\]\nMaking our \\(R^2\\)\n\nssmodel/sstotal\n\n[1] 0.4678879\n\n\nLet’s compare the above to the output from a function to report the \\(R^2\\) from a multiple regression:\n\nsummary(lm(lifeExp ~ pop + gdpPercap, gapminder_2007))$r.squared\n\n[1] 0.4678879\n\n\nLooks the same!\n\nQuestion 1\nA multiple regression allows you to control for overlap between predictors in how they predict an outcome variable\n\nviewof multiple_regressions_1_response = Inputs.radio(['True','False']);\ncorrect_multiple_regressions_1 = 'True';\nmultiple_regressions_1_result = {\n  if(multiple_regressions_1_response == correct_multiple_regressions_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "excelIntro/averageIfs.html",
    "href": "excelIntro/averageIfs.html",
    "title": "Averageifs",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the averageifs tab at the bottom of excel.\nSometimes it will be helpful to average only specific values in a column. One way to do this is using an “averageifs” formula.\nLet us imagine we want separate average response times for male and female participants. We can use a formula that lets us use one column to identify which rows we want to average in another column. So if we want to average the response time for females, we’re only interested in the rows with female data:\n\nSo here’s how an “averageifs” formula could look:\n\nSo let’s break this down. This averageifs formula has three inputs:\n\nThe cells that will be averaged - B2:B11\nA set of cells which will be used to determine which rows are selected: C2:C11\nA value that is compared with the previous set of cells to determine which rows are selected: “Female”\n\nIn practice, the above formula does this:\n\nSo you now have a formula for calculating the mean scores based on one criterion, in this case gender. Let’s consolidate this by you answering the following questions\n\nviewof question_1_response = Inputs.number([0,500], {label: \"Mean female ms\", step:1});\ncorrect_female_ms = 413;\n\nquestion_1_result = { \n  if(question_1_response == correct_female_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_2_response = Inputs.number([0,500], {label: \"Mean male ms\", step:1});\ncorrect_male_ms = 408;\n\nquestion_2_result = { \n  if(question_2_response == correct_male_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \nNow that you have calculated the average score based on one criterion, we can calculate it based on two criterion! It’s like the original formula, but we add another range of cells, and another value to compare the cells to.\nSo let us imagine we want the average response time for females with a mobile phone? We would use our original formula (see above), but before closing it we would add the cells referring to whether a mobile phone is present (B2:B11), and then compare these cells to the word “yes”. Let’s do that:\n\nJust to really consolidate what is going on, like before, we focussed only on female participants:\n\nBut are now also focusing on rows in which a mobile phone is present (as indicated by “yes”)\n\nSo now you should be able to calculate the response times for all four groups of participants:\n\nviewof question_3_response = Inputs.number([0,700], {label: \"Mean female using phones\", step:1});\ncorrect_female_yes_ms = 385;\n\nquestion_3_result = { \n  if(question_3_response == correct_female_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_4_response = Inputs.number([0,700], {label: \"Mean female NOT using phones\", step:1});\ncorrect_female_no_ms = 457;\n\nquestion_4_result = { \n  if(question_4_response == correct_female_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_5_response = Inputs.number([0,700], {label: \"Mean male using phones\", step:1});\ncorrect_male_yes_ms = 612;\n\nquestion_5_result = { \n  if(question_5_response == correct_male_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_6_response = Inputs.number([0,700], {label: \"Mean male NOT using phones\", step:1});\ncorrect_male_no_ms = 103;\n\nquestion_6_result = { \n  if(question_6_response == correct_male_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is…"
  },
  {
    "objectID": "excelIntro/formulas.html",
    "href": "excelIntro/formulas.html",
    "title": "Formulas",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here.\nBy using formulas we can automate a lot of processing in excel.\nAll you need to do to make a cell a formula, is to put an “=” sign at the start of it.\nLet’s start by going to the worksheet Formulas (you might already be there, or need to scroll left in the tabs at the bottom of the page):\n\nYou’ll see that there’s a hypothetical situation in which someone has done two runs, and wants to calculate the total distance they’ve run. We can use the following formula to add the values in each of the cells together:\n\nYou can see that you can just add cell B2 to cell B3 to get the total added together.\n\nYour turn\nDo this yourself, and complete the following:\nThe total distance run was…\n\nviewof question_0_response = Inputs.number([0,100], {label: \"\", step:.01});\ncorrect_q0 = 33.37;\n\nquestion_0_result = { \n  if(question_0_response == correct_q0){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s use this logic to work out some other values on that sheet:\nThe total distance run by James was…\n\nviewof question_1_response = Inputs.number([0,100], {label: \"\", step:.0005});\ncorrect_q1 = 58.3535;\n\nquestion_1_result = { \n  if(question_1_response == correct_q1){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElaine ran…\n\nviewof question_2_response = Inputs.number([0,100], {label: \"\", step:.333});\ncorrect_q2 = 16.333;\n\nquestion_2_result = { \n  if(question_2_response == correct_q2){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe total distance run by Dianna was…\n\nviewof question_3_response = Inputs.number([0,100], {label: \"\", step:.002});\ncorrect_q3 = 32.184;\n\nquestion_3_result = { \n  if(question_3_response == correct_q3){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeel free to play around with these formulas a bit more. A really good way to learn excel is to play around with what you’ve learned to build up confidence with it."
  },
  {
    "objectID": "excelIntro/anchoring.html",
    "href": "excelIntro/anchoring.html",
    "title": "Anchoring",
    "section": "",
    "text": "This section will keep using the averageifs tab in excel. There are a couple of tricks that’ll allow you to copy and paste the formulas in a way that saves you having to write them out again with minor tinkering. The first is to make your formula as dynamic as possible. This means to refer to other cells where possible to make your formula do more for you.\nThis might be still be a bit abstract, so let’s do something concrete. You’ll see that we structured our output into a table:\n\nThis formula doesn’t yet make use of the information in the table. Here’s an example of one way that it could:\n\nSee how we’re referring to the cell with the word “Female” in, rather than having to write “Female”. In the Final assignment this sort of trick will be very helpful. Let’s do the same to look at whether there’s a mobile phone present:\n\nNow we’re nearly ready to start copying and pasting this cell into the other 3 cells. However, if we did that now, the cells would only be partially correctly aligned, as the relative locations would work a little, but not entirely. Let’s see that in practice. First, let’s copy the cell one up from it’s original location:\n\nSee that whilst it is good that one of the cells moved up to refer to “male” rather than “female”, another of the cells moved to refer to “mobile phone present” instead of “yes”, which is what we would have wanted.\nSimilarly, if we just copy the original cell one to the right instead, you might already guess what’s going to go wrong:\n\nWhilst it was good that one referred to cell moved to the right to refer to the “no” value, we didn’t want the other cell to move to the right to refer to “384.5503”, we wanted it to stay on “female”. You might also notice that all the columns we are referring to have moved over also, and are also misaligned!\nSo to prevent cells moving in ways you do not want them to, you anchor them. You anchor them by putting a dollar sign ($) before the row and/or column you want to stop changing. So let’s fix the original cell to be appropriately anchored:\n\nIn particular, notice the new dollar signs ($) next to their respective rows and columns\n\nThis will require practice to become confident with, but it will be an extremely useful excel skill."
  },
  {
    "objectID": "excelIntro/sum.html",
    "href": "excelIntro/sum.html",
    "title": "Sum and SumIf",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the sum tab at the bottom of excel:\n\nIn this example, we would like to calculate the total response times, known as sum. To start with, let’s calculate the SUM for all participants in this spreadsheet:\n\nSo to calculate the SUM (i.e. total of the selected cells added together), you write a formula as follows:\n=sum(START:END)\nNow because everything is lined up to have all the mobile users first, and then the non-mobile users, so you can just use the sum function to select the rows where it is “yes” for mobile phone users. It’s not always practical to align your data column by column, so sumifs is a helpful function. Here’s a formula you could run to calculate the sum for females:\n\nThe excel formula’s structure:\n=sumifs(cells_you_want_to_sum, selection_crition_cells_1, selction_criterion_1)\nYou can have as many criterion as you like. Let’s look at females with mobile phones:\n\nYou may have noticed 2 things:\n\nYou just add an extra criteria column (in this case column D) and a new criteria to compare it to (“yes”) to allow you to have an extra criteria to select your rows on\nThis example compares column C to I7 rather than the word “Females”. Referring to a specific cell can help make your formulas more efficient, as you don’t have to repeatedly type the same word again and again. In fact, arguably it would be better to have anchored I$7 so that you could copy the formula down."
  },
  {
    "objectID": "excelIntro/if.html",
    "href": "excelIntro/if.html",
    "title": "If function",
    "section": "",
    "text": "If formulas are very helpful for tidying data. Let us imagine in our experiment we want to remove all participants who were too slow, and we assume they just weren’t paying attention. Let’s remove all participants who took longer than 800ms to respond. We can do this participant by participant using “if” formulas.\nIn English, what we’re going to do is write a formula which looks at the participant’s response time, and if it is too slow, write “TOO SLOW”. If it’s not too slow, we’ll write down the respective response time. Let’s do this for one row:\n\nLet’s relate this to the structure of an If formula generally. An “if” formula’s structure goes like so:\n“=if(logical test,result if true, result if false)”\nThe logical test in our case was whether the cell B2 was greater than (>) 800:    B2 > 800\nThe result we wanted if B2 was greater than 800 was the text “TOO SLOW”:       “TOO SLOW”\nThe result we wanted if B2 was not greater than 800 was the value in B2:        B2\nSo as a next step, make this formula for each row and then complete the following statement:\nThere were _____ participants who took more than 800ms, making them too slow. \n\nviewof if_q1_response = Inputs.number({label: ''});\ncorrect_if_q1 = '3';\nif_q1_result = {\n  if(if_q1_response == correct_if_q1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s do another formula to remove participants who were too fast. We don’t think a participant can react quicker than 100ms in reality, so in the Too Fast column, write a formula for each row that will write “TOO FAST” if the response time is less than 100ms, or write the response time if it is not “TOO FAST”.\nAfter writing these formulas, complete the following statement:\nThere were _____ participants who took less than 100ms, making them too fast.\n\nviewof if_q2_response = Inputs.number({label: ''});\ncorrect_if_q2 = '2';\nif_q2_result = {\n  if(if_q2_response == correct_if_q2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "excelIntro/count.html",
    "href": "excelIntro/count.html",
    "title": "Count and countifs function",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. You can find the relevant tab below:\n\nThis function allows you to count the number of cells in a selection.\nLike all formulas, it begins with an “=” sign.\nLet’s use this to count the number of participants you have (even though in this case we already know it is 10). The general formula is:\n=count(START:END)\nSo to calculate the number of participants, we would write:\n\nAn important thing to know about count formulas is that they only count cells with numbers in them. So the following would get zero:\n\nIf you want to count the number of phone and non-phone users, you can use countifs. This allows you to count how many occurrences there are of a value you are looking for. You can do this for just one column looking for just one value, or multiple columns looking for multiple values.\nThe general formula if you are just looking for one value in one column:\n=countifs(column_1,value_1)\nIf you wanted to know the number of phone users, you could type:\n=countifs(D2:D11,\"yes\")\nHowever, as there is nothing else in column D, it would be more elegant to refer to the whole column instead:\n=countifs(D:D,\"yes\")\nNow if we wanted to calculate how many females had a phone, you could use add a second column and compare a second value to it:\n=countifs(D:D,\"yes\",C:C,\"Female\")\n\nYour turn\nUse countif formulas to calculate the following. Your forumas should get you the same numbers as listed below:\n\n\n\nSex\nPhones\nNo Phones\nTotal\n\n\n\n\nFemales\n3\n2\n5\n\n\nMales\n3\n2\n5"
  },
  {
    "objectID": "jast_setup.html",
    "href": "jast_setup.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "distributions/normal_questions.html",
    "href": "distributions/normal_questions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamie’s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  },
  {
    "objectID": "distributions/transformingQuestions.html",
    "href": "distributions/transformingQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/skewnessQuestions.html",
    "href": "distributions/skewnessQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is"
  },
  {
    "objectID": "distributions/binomialQuestions.html",
    "href": "distributions/binomialQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nDo binomial distributions only work if there is an equal likelihood of either outcome?\n\nviewof binomial_1_response = Inputs.radio(['Yes','No']);\ncorrect_binomial_1 = 'No';\nbinomial_1_result = {\n  if(binomial_1_response == correct_binomial_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of flipping 2 heads in a row if your coin is .6 biased towards heads\n\nviewof binomial_2_response = Inputs.number();\ncorrect_binomial_2 = '.36';\nbinomial_2_result = {\n  if(binomial_2_response == correct_binomial_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/skewness.html",
    "href": "distributions/skewness.html",
    "title": "Skewness (R,Python)",
    "section": "",
    "text": "Parametric analyses are based on the assumption that the data you are analysing is normally distributed (see below):\nIf your data fits a normal distribution, then you can draw conclusions based on certain facts about this distribution, e.g. the fact that 97.7% of your population should have a score that is more negative than +2 standard deviations above the mean (because Z-scores represent standard deviations from the mean). As a result, if your data is skewed:\nWhite represents the median in the figure above. As you can see from the above skewed distribution, the median is below the mean, consistent with the data being skewed. Importantly, the assumptions that we can make about what proportion of the population are 1 standard deviation above and below the mean are no longer valid, as more than half the population are below the mean in this case. This would suggest that non-parametric analyses could be more appropriate if your data is skewed.\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is.\nThe next section is a breakdown of the formula for those interested in it (but this is not crucial)."
  },
  {
    "objectID": "distributions/skewness.html#consolidation-questions",
    "href": "distributions/skewness.html#consolidation-questions",
    "title": "Skewness (R,Python)",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "Kurtosis refers to how influenced a distribution is by its tails.\n\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\nIs Platykurtic vs. leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "distributions/binomial.html",
    "href": "distributions/binomial.html",
    "title": "Binomial Distribution (R)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "distributions/binomial.html#what-is-a-binomial-distribution",
    "href": "distributions/binomial.html#what-is-a-binomial-distribution",
    "title": "Binomial Distribution (R)",
    "section": "What is a Binomial distribution?",
    "text": "What is a Binomial distribution?\nImagine that you want to predict the likelihood of flipping a coin on heads or tails a certain number of times in a row. The outcome of each coin flip is binary, i.e. there is only 1 possible outcome out of 2 options. If this isn’t a biased coin, we can calculate some basic expectations about what will happen after 2 flips of the coin:\n\nEach flip of the coin has a 50% or 0.5 chance of landing on Heads.\nIf we wanted to calculate the likelihood of 2 flips of heads, there’s a 0.5 chance we get the first heads flip, and then another 0.5 chance we’ll get the second head flip. To summarise this, we can multiply both 0.5 chances together to get .025.\nTo summarise the likelihood of all combinations we could make the following table:\n\n\n\n\n\n\n\n\n\nFirst flip and likelihood\nSecond flip and likelihood\nOverall likelihood\n\n\nHeads (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nHeads (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nHeads (.5)\n0.5 * 0.5 = .25\n\n\nTails (.5)\nTails (.5)\n0.5 * 0.5 = .25\n\n\n\nLikelihood of one of the above happening\n.25 + .25 + .25 + .25 = 1\n\n\n\n\nlibrary(ggplot2)\nd=data.frame(\n  x1=c(0,  .5,0, .5), \n  x2=c(0.5,1 ,.5,1), \n  y1=c(0,  .5,.5,0), \n  y2=c(0.5,1 ,1 ,.5), \n # t=c('Both heads','Both tails','Mixed','Mixed'), \n  r=c('Both heads','Both tails','Mixed','Mixed'))\nggplot() + \nscale_x_continuous(name=\"Flip 1\") + \nscale_y_continuous(name=\"Flip 2\") +\ngeom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=r), color=\"black\", alpha=0.5) +\ngeom_text(data=d, aes(x=x1+(x2-x1)/2, y=y1+(y2-y1)/2, label=r), size=4) \n\n\n\n\nFigure 1: a summary of the likelihood of all outcomes, with the propotion of area covered by an outcome reflecting its likelihood. For example, the likelihood of flipping coins heads is .5 * .5 reflecting a .25 (or 25%) chance of flipping heads twice.\n\n\n\n\nOut of these 4 combinations, the likelihood of each of them in their precise order is exactly .25 (or a 25% chance). However, we often don’t care about order, so the odds of getting 1 head and 1 tail flip would be .25 + .25 = 50% chance. This allows us to create a very simple binomial distribution of head flips:\n\nlibrary(ggplot2)\n\ncoin_flips <- data.frame(\n  first  = c(\"heads\",\"heads\",\"tails\",\"tails\"),\n  second = c(\"heads\",\"tails\",\"heads\",\"tails\"),\n  heads  = c(2,1,1,0)\n)\n\nggplot(coin_flips, aes(heads)) + geom_histogram(bins = 3)\n\n\n\n\nFigure 2: a binomial distribution reflecting how frequently you should expect each number of head flips.\n\n\n\n\nWe can calculate the likelihood of how many heads you will flip using the above distribution. Let’s say that you wanted to know the likelihood of getting at least 1 flip of heads:\n\nggplot(coin_flips, aes(heads, fill = heads >= 1)) + geom_histogram(bins = 3)\n\n\n\n\nFigure 3: a binomial distribution reflecting how frequently you should expect to flip heads at least once.\n\n\n\n\nYou can calculate the area under this distribution this is true for, and divide it by the total area of the distribution:\n\nsum(coin_flips$heads >= 1) / length(coin_flips$heads)\n\n[1] 0.75\n\n\nThis suggests you have a 75% chance of flipping heads at least once. Looking at the table above will confirm that there is ony 1 out of 4 combinations in which you never flip heads, also supporting the fact that you will flip heads at least once the other 3 out of 4 times (i.e. 75% of the time)."
  },
  {
    "objectID": "distributions/binomial.html#what-if-the-odds-of-each-option-are-not-equal-to-each-other-e.g.-you-have-a-biased-coin",
    "href": "distributions/binomial.html#what-if-the-odds-of-each-option-are-not-equal-to-each-other-e.g.-you-have-a-biased-coin",
    "title": "Binomial Distribution (R)",
    "section": "What if the odds of each option are not equal to each other (e.g. you have a biased coin)",
    "text": "What if the odds of each option are not equal to each other (e.g. you have a biased coin)\nLet’s now imagine that we have a coin that will flip heads 75% of the time, and tails 25% of the time. It’s a bit more complicated to draw a distribution that captures this as the likelihood of either side is no longer equal.\n\nbiased_coin_flips <- data.frame(\n  first_flip = c(\"heads\",\"heads\",\"tails\",\"tails\"),\n  first_likelihood = c(.75,.75,.25,.25),\n  second_flip = c(\"heads\",\"tails\",\"heads\",\"tails\"),\n  second_likelihood = c(.75,.25,.75,.25),\n  heads = c(2,1,1,0)\n)\nbiased_coin_flips$outcome_likelihood = biased_coin_flips$first_likelihood * \n  biased_coin_flips$second_likelihood\n# Checking if all outcomes captured (should get 1) and no overlapping outcomes (should not get more than 1). See the page on *probabilities* for more information\nsum(biased_coin_flips$outcome_likelihood)\n\n[1] 1\n\n\n\nknitr::kable(biased_coin_flips)\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_flip\nfirst_likelihood\nsecond_flip\nsecond_likelihood\nheads\noutcome_likelihood\n\n\n\n\nheads\n0.75\nheads\n0.75\n2\n0.5625\n\n\nheads\n0.75\ntails\n0.25\n1\n0.1875\n\n\ntails\n0.25\nheads\n0.75\n1\n0.1875\n\n\ntails\n0.25\ntails\n0.25\n0\n0.0625\n\n\n\n\n\nIf we want a histogram, we can get a sum of the outcome_likelihood for each number of heads (thus ignoring the order):\n\nbiased_heads <- data.frame(\n  heads = c(0,1,2),\n  freq  = c(\n    sum(biased_coin_flips$outcome_likelihood[biased_coin_flips$heads == 0]),\n    sum(biased_coin_flips$outcome_likelihood[biased_coin_flips$heads == 1]),\n    sum(biased_coin_flips$outcome_likelihood[biased_coin_flips$heads == 2])\n  )\n)\nggplot(biased_heads, aes(x=heads,y=freq)) +\n  geom_col() + \n  xlab(\"Number of head flips\") +\n  ylab(\"Frequency\")\n\n\n\n\nFigure 4: a binomial distribution reflecting how frequently you should expect to flip heads with a coin that has a .75 bias for heads.\n\n\n\n\nHow likely am I to get at least 1 head flip with this biased coin?\n\nsum(biased_heads$freq[biased_heads$heads >= 1])\n\n[1] 0.9375\n\n\nWhich can be visualised as:\n\nggplot(biased_heads, aes(x=heads,y=freq, fill = heads >= 1)) +\n  geom_col() + \n  xlab(\"Number of head flips\") +\n  ylab(\"Frequency\")\n\n\n\n\nFigure 5: a binomial distribution reflecting how likely it you will flip head at least once when the coin has a .75 bias for heads."
  },
  {
    "objectID": "distributions/binomial.html#bivariate-distributions",
    "href": "distributions/binomial.html#bivariate-distributions",
    "title": "Binomial Distribution (R)",
    "section": "Bivariate distributions",
    "text": "Bivariate distributions\nNote that the above distributions we’ve calculated, and thus the p-values associated with certain outcomes (or greater or lesser), use the same principles for other distributions such as t-distributions.\n\nQuestion 1\nDo binomial distributions only work if there is an equal likelihood of either outcome?\n\nviewof binomial_1_response = Inputs.radio(['Yes','No']);\ncorrect_binomial_1 = 'No';\nbinomial_1_result = {\n  if(binomial_1_response == correct_binomial_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of flipping 2 heads in a row if your coin is .6 biased towards heads\n\nviewof binomial_2_response = Inputs.number();\ncorrect_binomial_2 = '.36';\nbinomial_2_result = {\n  if(binomial_2_response == correct_binomial_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/transforming.html",
    "href": "distributions/transforming.html",
    "title": "Transforming Data (R)",
    "section": "",
    "text": "A lot of analyses is dependent on data being normally distributed. One problem with your data might be that it is skewed. Lets focus on the gapminder data from 2007 to see if the gdp and life expectancy data is skewed, and how this could be addressed.\nSo it looks like both the gdp and life expectancy are skewed (as their z-scores are greater than 1.96). Lets double check with a quick plot:\nIt’s relatively easy to see the skewness of gdp, but life expectancy is a bit more subtle. As the data is skewed, we may want to transform it to make it less skewed.\nWe can complete a logarithmic transformation to reduce the skewness, so lets do that to both variables and then replot the data:\nLets check if the skewness has changed for the gdp:\nSo, transforming the gdp did reduce skewness but increased kurtsosis, so beware that applying a transformation may cause other problems! Lets check whether the log transformation reduced skewness for life expectancy:\nSeems like the answer is no.\nAn important question is whether the associations between your variables change after transformation, so let’s check that next:\nThe log transformed data is more strongly associated with each other than the original data. However, not all transformations will change associations between variables."
  },
  {
    "objectID": "distributions/transforming.html#linear-vs.-non-linear-transformations",
    "href": "distributions/transforming.html#linear-vs.-non-linear-transformations",
    "title": "Transforming Data (R)",
    "section": "Linear vs. non-linear transformations",
    "text": "Linear vs. non-linear transformations\nLinear transformation includes adding, subtracting from, multiplying or dividing variables. These transformations change the absolute value, but not pattern of the distribution of the variable. Let’s use life expectancy to illustrate how linear transformations change the absolute values without changing the distribution.\n\nAdditive transformations\nIf you added 100 to the life expectancy for all countries, you would change the absolute value:\n\nRPython\n\n\n\n# before transformation\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n# after transformation\nmean(gapminder_2007$lifeExp + 100)\n\n[1] 167.0074\n\n\n\n\n\n# before transformation\ngapminder_2007['lifeExp'].mean()\n\n# after transformation\nnp.mean(gapminder_2007['lifeExp'] + 100)\n\n67.00742253521126\n167.00742253521128\n\n\n\nThere’s a big difference between the means, but all we’ve done is shift the distribution up 100, we haven’t made it wider or thinner:\n\nRPython\n\n\n\n# before transformation\nsd(gapminder_2007$lifeExp)\n\n[1] 12.07302\n\n# after transformation\nsd(gapminder_2007$lifeExp + 100)\n\n[1] 12.07302\n\n\n\n\n\n# before transformation [use ddof =1 for sample sd, and ddof=0 for population sd]\nnp.std(gapminder_2007['lifeExp'], ddof=1) \n\n# after transformation [use ddof =1 for sample sd, and ddof=0 for population sd]\nnp.std(gapminder_2007['lifeExp'] + 100, ddof=1)\n\n12.07302050222512\n12.07302050222512\n\n\n\nIf we were to visualise this transformation\n\nRPython\n\n\n\nlife_exp_before_after <- data.frame(\n  life_exp = c(gapminder_2007$lifeExp, gapminder_2007$lifeExp + 100),\n  tranformed = c(rep(\"before\", each = 142), rep(\"after\", each =142))\n)\n\nggplot(life_exp_before_after, aes(x=life_exp, fill=tranformed)) +\n  geom_histogram(binwidth = 2, alpha=.5, position = \"identity\") +\n  ggtitle(\"Before vs. after additive transformation\") \n\n\n\n\n\n\n\n# Create a DataFrame for 'lifeExp' before and after the transformation\nlife_exp_before_after = pd.DataFrame({\n    'life_exp': np.concatenate([gapminder_2007['lifeExp'], gapminder_2007['lifeExp'] + 100]),\n    'transformed': np.concatenate([np.repeat('before', len(gapminder_2007)), np.repeat('after', len(gapminder_2007))])\n})\n\n# Create a histogram\nplt.figure(figsize=(10, 6))\nplt.hist(life_exp_before_after[life_exp_before_after['transformed'] == 'before']['life_exp'],\n         bins=range(0, 120, 2), alpha=0.5, label='Before Transformation', color='blue')\nplt.hist(life_exp_before_after[life_exp_before_after['transformed'] == 'after']['life_exp'],\n         bins=range(0, 220, 2), alpha=0.5, label='After Transformation', color='green')\nplt.title(\"Before vs. After Additive Transformation\")\nplt.xlabel(\"Life Expectancy\")\nplt.legend()\n\n# Show the histogram\nplt.show()\n\n\n\n\nHistogram before and after additive transformation of ‘Life expectancy’\n\n\n\n\n\nWe can see above that there is no difference in the shape of the distributions, but a shift. You would get the same pattern shifted also if you had subtracted from the original data. As a result, any association between the transformed variable and another variable will be the same as it was before the transformation as the shapes of the distributions are still the same.\n\n\nMultiplicative transformations\nIf you multiplied the life expectancy by 1.5 then you would change both the mean\n\nRPython\n\n\n\n# before transformation\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n# after transformation\nmean(gapminder_2007$lifeExp * 1.5)\n\n[1] 100.5111\n\n\n\n\n\n# before transformation\ngapminder_2007['lifeExp'].mean()\n\n# after transformation\nnp.mean(gapminder_2007['lifeExp'] * 1.5)\n\n67.00742253521126\n100.51113380281689\n\n\n\nand SD of life expectancy\n\nRPython\n\n\n\n# before transformation\nsd(gapminder_2007$lifeExp)\n\n[1] 12.07302\n\n# after transformation\nsd(gapminder_2007$lifeExp * 1.5)\n\n[1] 18.10953\n\n\n\n\n\n# before transformation [use ddof =1 for sample sd, and ddof=0 for population sd]\nnp.std(gapminder_2007['lifeExp'], ddof=1) \n\n# after transformation [use ddof =1 for sample sd, and ddof=0 for population sd]\nnp.std(gapminder_2007['lifeExp'] * 1.5, ddof=1)\n\n12.07302050222512\n18.109530753337683\n\n\n\nWe established above that changing the mean isn’t sufficient to change the shape of a distribution, but would changing the standard deviation change the shape of the distribution. Let’s put two histograms of each distribution side by side to evaluate this:\n\nRPython\n\n\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$lifeExp, breaks = seq(min(gapminder_2007$lifeExp), max(gapminder_2007$lifeExp), length.out = 11), main = \"Original\")\nhist(gapminder_2007$lifeExp*1.5, breaks = seq(min(gapminder_2007$lifeExp*1.5), max(gapminder_2007$lifeExp*1.5), length.out = 11), main = \"Original * 1.5\")\n\n\n\n\n\n\n\n# Create subplots with two histograms\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot the original 'lifeExp' histogram\naxs[0].hist(gapminder_2007['lifeExp'],bins=range(0, 120, 2), alpha=0.5, label='Before Transformation', color='blue')\naxs[0].set_title(\"Original\")\naxs[0].set_xlabel(\"Life Expectancy\")\naxs[0].set_ylabel(\"Frequency\")\n\n# Plot the 'lifeExp' * 1.5 histogram\naxs[1].hist(gapminder_2007['lifeExp'] * 1.5, bins=range(0, 120, 2), color='green', alpha=0.5)\naxs[1].set_title(\"Original * 1.5\")\naxs[1].set_xlabel(\"Life Expectancy\")\naxs[1].set_ylabel(\"Frequency\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the histograms\nplt.show()\n\n\n\n\nHistogram before and after multiplicative transformation of ‘Life expectancy’\n\n\n\n\n\nWe can see that the shape/pattern of the distribution is the same, and so the association between the transformed variable and other variables will stay the same after transformation. This is because associations between variables ignore the scale of either variable.\n\n\nNon-linear transformations\nUnlike linear transformations, non-linear transformations change the shape of distributions. There are a wide variety of non-linear transformations you could apply to a variable, such as…\n\nSquare (\\(^2\\))\n\nRPython\n\n\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(gapminder_2007$gdpPercap^2, main = \"Squared\")\n\n\n\n\n\n\n\n# Create subplots with two histograms\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot the original 'gdpPercap' histogram\naxs[0].hist(gapminder_2007['gdpPercap'], alpha=0.5, label='Before Transformation', color='blue')\naxs[0].set_title(\"Original\")\naxs[0].set_xlabel(\"Life Expectancy\")\naxs[0].set_ylabel(\"Frequency\")\n\n# Plot the squared 'gdpPercap' histogram\naxs[1].hist(np.square(gapminder_2007['gdpPercap']), color='green', alpha=0.5)\naxs[1].set_title(\"Squared\")\naxs[1].set_xlabel(\"Life Expectancy\")\naxs[1].set_ylabel(\"Frequency\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the histograms\nplt.show()\n\n\n\n\nHistogram before and after squared transformation of ‘GDP per Capita’\n\n\n\n\n\nSquaring data is likely to make the distributions more extreme, and so isn’t often a pragmatic solution to try to make your data less skewed.\n\n\nSquare root (\\(\\sqrt{}\\))\n\nRPython\n\n\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(sqrt(gapminder_2007$gdpPercap), main = \"Square root\")\n\n\n\n\n\n\n\n# Create subplots with two histograms\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot the original 'gdpPercap' histogram\naxs[0].hist(gapminder_2007['gdpPercap'], alpha=0.5, label='Before Transformation', color='blue')\naxs[0].set_title(\"Original\")\naxs[0].set_xlabel(\"GDP per Capita\")\naxs[0].set_ylabel(\"Frequency\")\n\n# Plot the square root 'gdpPercap' histogram\naxs[1].hist(np.sqrt(gapminder_2007['gdpPercap']), color='green', alpha=0.5)\naxs[1].set_title(\"Square root\")\naxs[1].set_xlabel(\"GDP per Capita\")\naxs[1].set_ylabel(\"Frequency\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the histograms\nplt.show()\n\n\n\n\nHistogram before and after square root transformation of ‘GDP per Capita’\n\n\n\n\n\nThis transformation appears to have reduced the skewness of the distribution. Calculating the square root of a variable will disproportionately reduce extreme values compared to less extreme values. This might be more clearly shown by looking at the change in the individual data points:\n\nRPython\n\n\n\n# focusing on 5 countries to make it visually easier\ngapminder_sqrt <- data.frame(\n  country = gapminder_2007$country[1:5],\n  transformed = c(\n    rep(\"Original\",5),\n    rep(\"Square Root\",5)\n  ),\n  # gdp has been divided by 500 to make the comparisons more visible\n  gdp  = c(gapminder_2007$gdpPercap[1:5]/500, sqrt(gapminder_2007$gdpPercap[1:5]/500))\n)\nggplot(gapminder_sqrt, aes(x=country, y = gdp, color = transformed)) +\n  geom_point(size=5) +\n  xlab(\"Country index\") +\n  ylab(\"GDP (before and after transformation)\") \n\n\n\n\n\n\n\n# Sample data for 5 countries\ndata = pd.DataFrame({\n    'Country': gapminder_2007['country'].iloc[:5].tolist() * 2,\n    'Transformation': ['Original'] * 5 + ['Square Root'] * 5,\n    'GDP': (gapminder_2007['gdpPercap'].iloc[:5] / 500).tolist() + (np.sqrt(gapminder_2007['gdpPercap'].iloc[:5] / 500)).tolist()\n})\n\n# Create a scatter plot\nplt.figure(figsize=(12, 5))\ncolors = ['blue', 'green']\nmarkers = ['o', 's']\nfor i, transformation in enumerate(['Original', 'Square Root']):\n    subset = data[data['Transformation'] == transformation]\n    plt.scatter(subset['Country'], subset['GDP'], label=transformation, color=colors[i], s=100)\n\nplt.title(\"Comparison of GDP (Original vs. Square Root Transformation)\")\nplt.xlabel(\"Country Index\")\nplt.ylabel(\"GDP (before and after transformation)\")\nplt.legend(title='Transformation')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nComparison of GDP (Original vs. Square Root Transformation)\n\n\n\n\n\nAs you can see above, the original values (pink) that are higher are much more heavily reduced by square root transforming them than lower original values.\n\n\nLogarithmic (\\(\\log\\))\n\nRPython\n\n\n\npar(mfrow = c(1,2), \n    mar = c(0,0,2,1))\nhist(gapminder_2007$gdpPercap, main = \"Original\")\nhist(log(gapminder_2007$gdpPercap), main = \"Logarithmic\")\n\n\n\n\n\n\n\n# Create subplots with two histograms\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot the original 'gdpPercap' histogram\naxs[0].hist(gapminder_2007['gdpPercap'], alpha=0.5, label='Before Transformation', color='blue')\naxs[0].set_title(\"Original\")\naxs[0].set_xlabel(\"GDP per Capita\")\naxs[0].set_ylabel(\"Frequency\")\n\n# Plot the 'gdpPercap' * 1.5 histogram\naxs[1].hist(np.log(gapminder_2007['gdpPercap']), color='green', alpha=0.5)\naxs[1].set_title(\"Logarithmic\")\naxs[1].set_xlabel(\"GDP per Capita\")\naxs[1].set_ylabel(\"Frequency\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the histograms\nplt.show()\n\n\n\n\nHistogram before and after log transformation of ‘GDP per Capita’\n\n\n\n\n\nThis transformation seems very successful in changing the distribution shape to be less skewed. Let’s see if the log transformation follows a similar pattern as the sqrt in disproportionately impacting larger values than smaller values.\n\nRPython\n\n\n\n# focusing on 5 countries to make it visually easier\ngapminder_log <- data.frame(\n  country = gapminder_2007$country[1:5],\n  transformed = c(\n    rep(\"Original\",5),\n    rep(\"Log Transformation\",5)\n  ),\n  # gdp has been divided by 500 to make the comparisons more visible\n  gdp  = c(gapminder_2007$gdpPercap[1:5]/500, log(gapminder_2007$gdpPercap[1:5]/500))\n)\nggplot(gapminder_log, aes(x=country, y = gdp, color = transformed)) +\n  geom_point(size=5) +\n  xlab(\"Country index\") +\n  ylab(\"GDP (before and after transformation)\") \n\n\n\n\n\n\n\n# Sample data for 5 countries\ndata = pd.DataFrame({\n    'Country': gapminder_2007['country'].iloc[:5].tolist() * 2,\n    'Transformation': ['Original'] * 5 + ['Log Transformation'] * 5,\n    'GDP': (gapminder_2007['gdpPercap'].iloc[:5] / 500).tolist() + (np.log(gapminder_2007['gdpPercap'].iloc[:5] / 500)).tolist()\n})\n\n# Create a scatter plot\nplt.figure(figsize=(12, 5))\ncolors = ['blue', 'green']\nmarkers = ['o', 's']\nfor i, transformation in enumerate(['Original', 'Log Transformation']):\n    subset = data[data['Transformation'] == transformation]\n    plt.scatter(subset['Country'], subset['GDP'], label=transformation, color=colors[i], s=100)\n\nplt.title(\"Comparison of GDP (Original vs. Log Transformation)\")\nplt.xlabel(\"Country Index\")\nplt.ylabel(\"GDP (before and after transformation)\")\nplt.legend(title='Transformation')\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\nComparison of GDP (Original vs. Log Transformation)\n\n\n\n\n\nYep, log also reduces skewness by disproportionately reducing higher values."
  },
  {
    "objectID": "distributions/transforming.html#linear-transformations-will-not-change-the-association-between-variables",
    "href": "distributions/transforming.html#linear-transformations-will-not-change-the-association-between-variables",
    "title": "Transforming Data (R)",
    "section": "Linear transformations will not change the association between variables",
    "text": "Linear transformations will not change the association between variables\nYou can transform a single variable by adding and multiplying it, but as these are linear transformations they do not change the shape of the distributions of the original variables, and thus do not change the association between variables. For example:\n\nRPython\n\n\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data + 5 to one variable (an additive change)\ncor.test(\n  gapminder_2007$gdpPercap + 5,\n  gapminder_2007$lifeExp + 5\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap + 5 and gapminder_2007$lifeExp + 5\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data - 10 to one variable (an additive change)\ncor.test(\n  gapminder_2007$gdpPercap - 10,\n  gapminder_2007$lifeExp - 10\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap - 10 and gapminder_2007$lifeExp - 10\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with multiplication of 5 to one variable (multiplicative)\ncor.test(\n  gapminder_2007$gdpPercap * 5,\n  gapminder_2007$lifeExp * 5\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap * 5 and gapminder_2007$lifeExp * 5\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# grid comparing the 4 transformations\npar(mfrow = c(2,2), \n    mar = c(2,2,2,1))\nplot(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, main=\"original correlation\")\nplot(gapminder_2007$gdpPercap + 5, gapminder_2007$lifeExp +5, main=\"added 5 to gdp\")\nplot(gapminder_2007$gdpPercap - 10, gapminder_2007$lifeExp - 10, main = \"took 10 away from gdp\")\nplot(gapminder_2007$gdpPercap *5, gapminder_2007$lifeExp *5, main = \"multiplied gdp by 5\")\n\n\n\n\n\n\n\nfrom scipy.stats import pearsonr\n\n# Define the transformations\ngdp_plus_5 = gapminder_2007['gdpPercap'] + 5\ngdp_minus_10 = gapminder_2007['gdpPercap'] - 10\ngdp_times_5 = gapminder_2007['gdpPercap'] * 5\n\nlife_plus_5 = gapminder_2007['lifeExp'] + 5\nlife_minus_10 = gapminder_2007['lifeExp'] - 10\nlife_times_5 = gapminder_2007['lifeExp'] * 5\n\n# correlation with original data\ncorrelation_original, pvalue_original = pearsonr(gapminder_2007['gdpPercap'], gapminder_2007['lifeExp'])\nprint(\"Correlation with original data:\", correlation_original)\nprint(\"p-value of correlation with original data:\", pvalue_original)\n\n# correlation with original data + 5 to both variables (an additive change)\ncorrelation_plus_5, pvalue_plus_5 = pearsonr(gdp_plus_5, life_plus_5)\nprint(\"Correlation with original data + 5:\", correlation_plus_5)\nprint(\"p-value of correlation with original data +5:\", pvalue_plus_5)\n\n# correlation with original data - 10 to both variables (an additive change)\ncorrelation_minus_10, pvalue_minus_10 = pearsonr(gdp_minus_10, life_minus_10)\nprint(\"Correlation with original data - 10:\", correlation_minus_10)\nprint(\"p-value of correlation with original data -10:\", pvalue_minus_10)\n\n# correlation with multiplication of 5 to both variables (multiplicative)\ncorrelation_times_5, pvalue_times_5 = pearsonr(gdp_times_5, life_times_5)\nprint(\"Correlation with multiplication of 5:\", correlation_times_5)\nprint(\"p-value of correlation with multiplication of 5:\", pvalue_times_5)\n\n# Create a grid of scatter plots\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 2, 1)\nplt.scatter(gapminder_2007['gdpPercap'], gapminder_2007['lifeExp'])\nplt.title(\"Original Correlation\")\n\nplt.subplot(2, 2, 2)\nplt.scatter(gdp_plus_5, life_plus_5)\nplt.title(\"Added 5P\")\n\nplt.subplot(2, 2, 3)\nplt.scatter(gdp_minus_10, life_minus_10)\nplt.title(\"Took 10 Away\")\n\nplt.subplot(2, 2, 4)\nplt.scatter(gdp_times_5, life_times_5)\nplt.title(\"Multiplied by 5\")\n\nplt.tight_layout()\nplt.show()\n\n \n\n\n\nCorrelation with original data - 10 to both variables (an additive change)\n\n\n \n\n\n\nYou can see that the transformations being linear haven’t changed the nature of the associations."
  },
  {
    "objectID": "distributions/transforming.html#non-linear-transformations-do-change-associations",
    "href": "distributions/transforming.html#non-linear-transformations-do-change-associations",
    "title": "Transforming Data (R)",
    "section": "Non-linear transformations do change associations",
    "text": "Non-linear transformations do change associations\nIf you apply non-linear transformations to one or both variables this will change the direction and strength of the associations. Below are some examples when you transform both variables:\n\nRPython\n\n\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with log of variables\ncor.test(\n  log(gapminder_2007$gdpPercap),\n  log(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  log(gapminder_2007$gdpPercap) and log(gapminder_2007$lifeExp)\nt = 14.752, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7060729 0.8372165\nsample estimates:\n      cor \n0.7800706 \n\n# correlation with both variables squared\ncor.test(\n  gapminder_2007$gdpPercap ^ 2,\n  gapminder_2007$lifeExp ^ 2\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap^2 and gapminder_2007$lifeExp^2\nt = 8.6437, df = 140, p-value = 1.123e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4709220 0.6877841\nsample estimates:\n      cor \n0.5898894 \n\n# correlation with square root of both variable\ncor.test(\n  sqrt(gapminder_2007$gdpPercap),\n  sqrt(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(gapminder_2007$gdpPercap) and sqrt(gapminder_2007$lifeExp)\nt = 12.981, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6539524 0.8057025\nsample estimates:\n      cor \n0.7390648 \n\n# grid comparing the 4 transformations\npar(mfrow = c(2,2), \n    mar = c(2,2,2,1))\nplot(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, main = \"original correlation\")\nplot(log(gapminder_2007$gdpPercap),log(gapminder_2007$lifeExp), main = \"log applied\")\nplot(gapminder_2007$gdpPercap ^ 2,gapminder_2007$lifeExp ^ 2, main = \"data squared\")\nplot(sqrt(gapminder_2007$gdpPercap),sqrt(gapminder_2007$lifeExp), main = \"square root of data\")\n\n\n\n\n\n\n\nfrom scipy.stats import pearsonr\n\n# Define the transformations\ngdp_log = np.log(gapminder_2007['gdpPercap'])\ngdp_sqrt = np.sqrt(gapminder_2007['gdpPercap'])\ngdp_squared = np.square(gapminder_2007['gdpPercap'])\n\nlife_log = np.log(gapminder_2007['lifeExp'])\nlife_sqrt = np.sqrt(gapminder_2007['lifeExp'])\nlife_squared = np.square(gapminder_2007['lifeExp'])\n\n# correlation with original data\ncorrelation_original, pvalue_original = pearsonr(gapminder_2007['gdpPercap'], gapminder_2007['lifeExp'])\nprint(\"Correlation with original data:\", correlation_original)\nprint(\"p-value of correlation with original data:\", pvalue_original)\n\n# correlation with both variables log transformed\ncorrelation_log, pvalue_log = pearsonr(gdp_log, life_log)\nprint(\"Correlation with both variables log transformed:\", correlation_log)\nprint(\"p-value of correlation with both variables log transformed:\", pvalue_log)\n\n# correlation with both variables square rooted\ncorrelation_sqrt, pvalue_sqrt = pearsonr(gdp_sqrt, life_sqrt)\nprint(\"Correlation with both variables square rooted:\", correlation_sqrt)\nprint(\"p-value of correlation with both variables square rooted:\", pvalue_sqrt)\n\n# correlation with both variables squared\ncorrelation_squared, pvalue_squared = pearsonr(gdp_squared, life_squared)\nprint(\"Correlation with both variables squared:\", correlation_squared)\nprint(\"p-value of correlation with multiplication of 5:\", pvalue_squared)\n\n# Create a grid of scatter plots\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 2, 1)\nplt.scatter(gapminder_2007['gdpPercap'], gapminder_2007['lifeExp'])\nplt.title(\"Original Correlation\")\n\nplt.subplot(2, 2, 2)\nplt.scatter(gdp_log, life_log)\nplt.title(\"Log Transformation\")\n\nplt.subplot(2, 2, 3)\nplt.scatter(gdp_sqrt, life_sqrt)\nplt.title(\"Square Root Transformation\")\n\nplt.subplot(2, 2, 4)\nplt.scatter(gdp_squared, life_squared)\nplt.title(\"Squared Transformation\")\n\nplt.tight_layout()\nplt.show()\n\n    \n\n\n\n\n\n\n\n\n\nTransforming is not universally accepted\n\n\n\nWhilst this page describes ways to transform the data and how they impact the distributions, this isn’t universally accepted practice as it is changing your data (perhaps similar to arguments that you shouldn’t remove outliers if they reflect real data points). However, transformation does not necessarily bias your data towards or against hypotheses if done appropriately, and is in fact used in mainstream analyses such Spearman’s Rank correlations. There are at least two major possible problems from inappropriately transforming your data:\n\nIf it becomes a form of fishing for data through multiple comparisons.\nTreating your data unevenly to create meaningless differences between conditions. For example, if you ran a t-test between 2 conditions, but one of them wasn’t normally distributed, then transforming only one of the conditions could make your comparison less meaningful. Let’s use the gapminder data to illustrate this by comparing gdp per capita between Europe and Africa:\n\n\nRPython\n\n\n\nspssSkewKurtosis(gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Europe\"])\n\n           estimate        se     zScore\nskew     -0.1028377 0.4268924 -0.2408985\nkurtosis -1.0441533 0.8327456 -1.2538683\n\nspssSkewKurtosis((gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Africa\"]))\n\n         estimate        se   zScore\nskew     1.707376 0.3304137 5.167389\nkurtosis 1.793325 0.6500932 2.758566\n\n\n\n\n\nspssSkewKurtosis(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Europe\"])\nspssSkewKurtosis(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Africa\"])\n\n \n\n\n\nSo We can see that there’s a significant problem with skewness for countries from Africa (zScore > 1.96) but not from Europe. The correct thing to do is to apply any transformation (to address skewness) to Africa to Europe also to avoid differences between the groups reflecting bias from the distortions. Logarithmic transformations can help reduce skewness, so let’s see how the means compare after applying the log transformation to both groups:\n\nRPython\n\n\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Europe\"])\n\n[1] 9.985978\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n[1] 7.486539\n\n\n\n\n\nnp.log(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Europe\"]).mean()\nnp.log(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Africa\"]).mean()\n\n9.985978130053216\n7.486539235254513\n\n\n\nThere’s a difference, in which Europe had a higher GDP per capita in 2007. Let’s check if both Europe and Africa are less skewed in their distributions after log-transforming their data:\n\nRPython\n\n\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Europe\"])\n\n           estimate        se     zScore\nskew     -0.7604293 0.4268924 -1.7813138\nkurtosis -0.6776747 0.8327456 -0.8137835\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n           estimate        se    zScore\nskew      0.5431380 0.3304137  1.643812\nkurtosis -0.6719277 0.6500932 -1.033587\n\n\n\n\n\nspssSkewKurtosis(np.log(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Europe\"]))\nspssSkewKurtosis(np.log(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Africa\"]))\n\n \n\n\n\nLooking good. Now, what would have happened if we had only log-transformed Africa’s data as it had the skewed distribution:\n\nRPython\n\n\n\nmean(gapminder_2007$gdpPercap[gapminder_2007$continent ==  \"Europe\"])\n\n[1] 25054.48\n\nmean(gapminder_2007$gdpPercap_log[gapminder_2007$continent ==  \"Africa\"])\n\n[1] 7.486539\n\n\n\n\n\ngapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Europe\"].mean()\nnp.log(gapminder_2007['gdpPercap'].loc[gapminder_2007['continent'] ==  \"Africa\"])\n\n25054.481635933338\n7.486539235254513\n\n\n\nIt now looks like Europeans have 3346.604 times as much GDP per capita as Africa!?\nThe above example hopefully is quite intuitive how problems arise if you apply transformations mindlessly. If you will be comparing differences in magnitudes between conditions, then it is important that transformations are applied equally to avoid bias. If you are correlating between conditions (or conducting a regression), then you do not have the same issue of bias.\n\n\n\nQuestion 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "distributions/normal.html",
    "href": "distributions/normal.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "distributions/normal.html#bell-curve-aka-normal-distribution",
    "href": "distributions/normal.html#bell-curve-aka-normal-distribution",
    "title": "Normal Distribution",
    "section": "Bell curve (AKA normal distribution)",
    "text": "Bell curve (AKA normal distribution)\nParametric statistics often compare values to a normal distribution of expected data, based on the estimated mean and SD. Lets start by showing a (made up) normal distribution of heights in centimeters:\nSo lets say the average person’s height is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:\n\nRPythonExcelJASPSPSS\n\n\n\n# Plot a normal distribution of heights\npopulation_heights_x <- seq(\n  120,    # min\n  180,    # max\n  by = 1  \n)\npopulation_heights_y <- dnorm(\n  population_heights_x,\n  mean = 150,\n  sd   = 10\n)\nplot(\n  population_heights_x,\n  population_heights_y,\n  xlab = \"height\",\n  ylab = \"frequency\"\n)\n# Add line to show mean and median\nabline(\n  v=150,  # where the line for the mean will be \n  lwd=5\n)\n\n\n\n\n\n\n\n# Plot a normal distribution of heights\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\npopulation_heights_x = [x for x in range(120, 181, 1)]\n\n# vector for the y-axis\npopulation_heights_y = norm.pdf(population_heights_x, loc=150, scale=10)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n\n# plot\nplt.scatter(population_heights_x, population_heights_y, color='w', edgecolors='black')\n\n# Add line to show mean and median\nplt.axvline(x=150, color='black', ls='-', lw=5)\n\n# add title on the x-axis\nplt.xlabel(\"height\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n# show the plot\nplt.show()\n\n# show plot\nplt.show()\n\n\n\n\nNormal Distribution\n\n\n\n\nDownload and open the normal.xlsx file in this repository to see data being used to create the below figure:\n(note that making a vertical line to reflect the mean didn’t seem as easy in Excel as other languages, so this hasn’t been added)\n\n\n\nNormal Distribution using Excel\n\n\n\n\nDownload and open height.csv in JASP, and then complete the following steps to generate the figure below:\n\n\nClick on the Descriptives panel\nadd height as a variable\nopen the Basic plots interface and then select Distribution plots and Display density\n\n\n\nDownload and open height.sav in SPSS and then complete the following steps to generate the figure below:\n\n\nSelect Analyze –> Descriptive Statistics –> Explore…\n\n\n\nMove the height variable to the dependent list, then select Plots… and choose Histogram as a descriptive figure\n\n\n\n\n\nYou can see that the above fits a bell-curve, and the middle represents both the mean and the median as the data is symmetrical. In reality, almost no data is a perfect bell-curve, but there are ways to test if the data isn’t sufficiently normal to use parametric tests with.\nNext, we will look at how normal distributions allow you to transform your data to z-scores to compare to a z-distribution."
  },
  {
    "objectID": "distributions/normal.html#z-scores-and-the-z-distribution",
    "href": "distributions/normal.html#z-scores-and-the-z-distribution",
    "title": "Normal Distribution",
    "section": "Z-scores and the z-distribution",
    "text": "Z-scores and the z-distribution\nA z-score is a standardised value that captures how many standard deviations above or below the mean an individual value is. Thus, to calculate the z-score\n\\[\nZ = \\frac{individualScore-meanScore}{StandardDeviation}\n\\]\nOr in formal terminology:\n\\[\nZ = \\frac{x-\\bar{x}}{\\sigma}\n\\]\nThe calculated score can then be applied to a z-distribution, which is parametric/normally distributed. Lets have a look at a z-distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# vector for the x-axis\nz_score_x <- seq(\n  -3,    # min\n  3,    # max\n  by = .1  \n)\n\n# vector for the y-axis\nz_score_y <- dnorm(\n  z_score_x,\n  mean = 0,\n  sd   = 1\n)\n\nplot(\n  z_score_x,\n  z_score_y,\n  xlab = \"z-score (SDs from the mean)\",\n  ylab = \"frequency\"\n)\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\nz_score_x = np.arange(-3.0, 3.1, 0.1)\n\n# vector for the y-axis\nz_score_y = norm.pdf(z_score_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\n# plot\nplt.scatter(z_score_x, z_score_y, color='w', edgecolors='black')\n\n# add title on the x-axis\nplt.xlabel(\"z-score (SDs from the mean)\")\n\n# add title on the y-axis\nplt.ylabel(\"frequency\")\n\n# show plot\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\nIt’s not clear that generating a z-distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\nIt’s not clear that generating a z-distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\n\nIf you compare the height distribution above to the z-score distribution, you should see that they are identically distributed. This is useful, as we know what percentage of a population fits within each standard deviation of a normal distribution:\n\nRPythonExcelJASPSPSS\n\n\n\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_13.6 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n p +\n   \n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6, \n     aes(\n       x, \n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   \n   annotate(\n     \"text\", \n     x=0.5, \n     y=0.01, \n     label= \"34.1%\"\n   ) + \n   annotate(\n     \"text\", \n     x=1.5, \n     y=0.01, \n     label= \"13.6%\"\n   ) + \n   annotate(\n     \"text\", \n     x=2.3, \n     y=0.01, \n     label= \"2.3%\"\n   ) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='red', alpha=.4)\n\nplt.text(2.05, 0.001, '2.3%', fontsize = 10, color='black',weight='bold')\nplt.text(1.1, 0.001, '13.6%', fontsize = 10, color='black',weight='bold')\nplt.text(0.1, 0.001, '34.1%', fontsize = 10, color='black',weight='bold')\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_0_plus.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\n\nThe above visualises how 34.1% of a population’s scores will be between 0 and 1 standard deviation from the mean, 13.6% of the population’s scores will be between 1 to 2 standard deviations above the mean, and 2.3% of the population will be more then 2 standard deviations above the mean. Remember that the normal distribution is symmetrical, so we also know that 34.1% of the population’s score will be between 0 to 1 standard deviations below the mean (or 0 to -1 SDs), 13.6% of the population’s score will be between -2 to -1 standard deviations from the mean, and 2.3% of the population’s score will be more negative than -2 standard deviations from the mean. Lets look at this cumulative distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"2.3%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"15.9%\") + \n   annotate(\"text\", x=-0.4, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (norm_x < -2), color='gold', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-2 <= norm_x)&(norm_x <= -1), color='red', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-1 <= norm_x)&(norm_x <= 0), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='cyan', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='purple', alpha=.4)\n\nplt.text(2.05, 0.001, '100%', fontsize = 10,color='black',weight='bold')\nplt.text(1.2, 0.001, '97.7%', fontsize = 10,color='black',weight='bold')\nplt.text(0.2, 0.001, '84.1%', fontsize = 10,color='black',weight='bold')\nplt.text(-0.8, 0.001, '50%', fontsize = 10,color='black',weight='bold')\nplt.text(-1.85, 0.001, '15.9%', fontsize = 10,color='black',weight='bold')\nplt.text(-2.7, 0.001, '2.3%', fontsize = 10,color='black',weight='bold')\n\nplt.yticks(np.arange(0, 0.5, step=0.1))\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_percentages.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do with JASP, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do in SPSS, so the Excel figure is included below:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\n\nThe above figure visualises how 13.6% of the population have score that is more negative than -2 standard deviations from the mean, 34.1% of the population have a standard deviation that is more negative than -1 standard deviations from the mean (this also include all the people who are more than -2 standard deviations from the mean), etc.\nWe can now use the above information to identify which percentile an individual is within a distribution.\nFor example, let’s imagine that an individual called Jane wants to know what percentile she’s at with her height. Lets imagine she is 170cm tall, the mean height of people 150cm, and the SD 10cm. That would make her z-score:\n\\[\nZ_{score} = \\frac{170 - 150}{10} = 2\n\\]\nAs we can see from the figure above, that puts her above 97.7% of the population, putting her in the top 2.3%."
  },
  {
    "objectID": "distributions/normal.html#consolidation-questions",
    "href": "distributions/normal.html#consolidation-questions",
    "title": "Normal Distribution",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamie’s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  },
  {
    "objectID": "permutations/permVsTtest.html",
    "href": "permutations/permVsTtest.html",
    "title": "Permutation vs. t tests (incomplete)",
    "section": "",
    "text": "As you may have noticed above, sometimes the p-value was higher or lower for the permutation analysis than it was for the original analysis. Let’s use some simulations to capture what you can generally expect when using permutation analysis on data you can run a t-test on.\nAs most studies are on 30 participants or more, and we’ve established it is computationally unrealistic to compute all permutations of 30+ participants, we will use Monte Carlo simulations. We will conduct 50 simulations for Cohen’s d effect sizes of .1,.3,.5 of 1000 permutations for between-subject t-tests on 100 participants per group. All tests will be 2-tailed.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nsimulations = 50\npermutations_n = 1000\nsample_size = 100\nthese_effect_sizes = c(.1,.2,.3,.4,.5)\nsimulations_df <- data.frame(\n  effect_size = rep(rep(these_effect_sizes, simulations)),\n  simulation  = rep(rep(c(1:simulations)),length(these_effect_sizes)),\n  # permutation = rep(c(1:permutations_n), length(these_effect_sizes) * simulations),\n  ttest = NA,\n  perm_test = NA\n)\n\n# could take a while, here's a progess bar\npb = txtProgressBar(min = 0, max = dim(simulations_df)[1], initial = 0) \nfor(i in 1:dim(simulations_df)[1]){\n  setTxtProgressBar(pb,i)\n  this_effect_size = simulations_df$effect_size[i]\n  group_1_data = rnorm(sample_size)\n  group_2_data = rnorm(sample_size)+ this_effect_size\n  simulations_df$ttest[i] = t.test(group_1_data, group_2_data, var.equal = T)$p.value\n  \n  # permutation analysis\n  perm_vector = numeric(permutations_n)\n  \n  for(j in 1:permutations_n){\n    all_sample = c(group_1_data,group_2_data)\n    group_1_sample = sample(all_sample, sample_size, replace = FALSE)\n    \n    # select opposite data for group 2\n    group_2_index  = !(all_sample %in% group_1_sample)\n    group_2_sample = all_sample[group_2_index]\n\n    # compare the medians\n    perm_vector[j] = t.test(group_1_sample, group_2_sample, var.equal = T)$p.value\n  }\n  simulations_df$perm_test[i] = mean(abs(simulations_df$ttest[i]) > abs(perm_vector))\n}\n\n================================================================================\n\nsimulations_df %>% \n  group_by(effect_size) %>% \n  summarise(\n    ttest_mean = mean(ttest),\n    ttest_sd   = sd(ttest),\n    perm_mean  = mean(perm_test),\n    perm_sd    = sd(perm_test),\n  ) -> simulations_summary\n\nsimulations_plot_df = data.frame(\n  test = rep(c(\"t-test\",\"permutation\"),each = 5),\n  effect_size = rep(as.character(these_effect_sizes), 2),\n  mean = c(\n    mean(simulations_df$ttest[simulations_df$effect_size == .1]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .2]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .3]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .4]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .5]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .1]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .2]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .3]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .4]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .5])\n  ),\n  sd   = c(\n    sd(simulations_df$ttest[simulations_df$effect_size == .1]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .2]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .3]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .4]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .5]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .1]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .2]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .3]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .4]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .5])\n  )\n)\n\nggplot(simulations_plot_df, aes(x=effect_size, y=mean, fill=test)) +\n  geom_bar(stat=\"identity\", position=position_dodge()) +\n  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,position=position_dodge(.9)) +\n  xlab(\"Effect Size\") +\n  ylab(\"P-value\") \n\n\n\n\nAs shown above, you get almost identical p-values from a single t-test on the original data to those from permutation analyses. However, arguably it’s redundant to do a permutation analysis of t-tests if the data is normal. The next question is whether a permutation analysis of median comparisons is equally powerful as t-tests on normal data? If so, then you could arguably just use permutation analyses of medians all times without having to worry about whether your data is parametric. Let’s see now how the p-values compare:\n\nsimulations_df <- data.frame(\n  effect_size = rep(rep(these_effect_sizes, simulations)),\n  simulation  = rep(rep(c(1:simulations)),length(these_effect_sizes)),\n  # permutation = rep(c(1:permutations_n), length(these_effect_sizes) * simulations),\n  ttest = NA,\n  perm_test = NA\n)\n\n# could take a while, here's a progess bar\npb = txtProgressBar(min = 0, max = dim(simulations_df)[1], initial = 0) \nfor(i in 1:dim(simulations_df)[1]){\n  setTxtProgressBar(pb,i)\n  this_effect_size = simulations_df$effect_size[i]\n  group_1_data = rnorm(sample_size)\n  group_2_data = rnorm(sample_size)+ this_effect_size\n  simulations_df$ttest[i] = t.test(group_1_data, group_2_data, var.equal = T)$p.value\n  \n  # permutation analysis\n  perm_vector = numeric(permutations_n)\n  \n  median_diff = median(group_1_data) - median(group_2_data)\n  for(j in 1:permutations_n){\n    all_sample = c(group_1_data,group_2_data)\n    group_1_sample = sample(all_sample, sample_size, replace = FALSE)\n    \n    # select opposite data for group 2\n    group_2_index  = !(all_sample %in% group_1_sample)\n    group_2_sample = all_sample[group_2_index]\n\n    # compare the medians\n    perm_vector[j] = median(group_1_sample) - median(group_2_sample)\n  }\n  simulations_df$perm_test[i] = mean(abs(perm_vector) > abs(median_diff))\n}\n\n================================================================================\n\nsimulations_df %>% \n  group_by(effect_size) %>% \n  summarise(\n    ttest_mean = mean(ttest),\n    ttest_sd   = sd(ttest),\n    perm_mean  = mean(perm_test),\n    perm_sd    = sd(perm_test),\n  ) -> simulations_summary\n\nsimulations_plot_df = data.frame(\n  test = rep(c(\"t-test\",\"permutation\"),each = 5),\n  effect_size = rep(as.character(these_effect_sizes), 2),\n  mean = c(\n    mean(simulations_df$ttest[simulations_df$effect_size == .1]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .2]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .3]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .4]),\n    mean(simulations_df$ttest[simulations_df$effect_size == .5]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .1]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .2]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .3]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .4]),\n    mean(simulations_df$perm_test[simulations_df$effect_size == .5])\n  ),\n  sd   = c(\n    sd(simulations_df$ttest[simulations_df$effect_size == .1]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .2]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .3]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .4]),\n    sd(simulations_df$ttest[simulations_df$effect_size == .5]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .1]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .2]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .3]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .4]),\n    sd(simulations_df$perm_test[simulations_df$effect_size == .5])\n  )\n)\n\nggplot(simulations_plot_df, aes(x=effect_size, y=mean, fill=test)) +\n  geom_bar(stat=\"identity\", position=position_dodge()) +\n  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,position=position_dodge(.9)) +\n  xlab(\"Effect Size\") +\n  ylab(\"P-value\") \n\n\n\n\nIt does seem that with effect sizes greater than .1 you lose a little power in your analyses using permutation tests."
  },
  {
    "objectID": "permutations/permutations.html",
    "href": "permutations/permutations.html",
    "title": "Permutations (R)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n  \n\nancova\n\n  ANCOVA     \n\npermutation\n\n  Permutations     \n\nancova->permutation\n\n   \n\nbetween_anova\n\n  One-way ANOVA     \n\nbetween_anova->ancova\n\n   \n\nbetween_anova->permutation\n\n   \n\nbinomial\n\n  Binomial Distribution     \n\ncontingency\n\n  Contingency     \n\nbinomial->contingency\n\n   \n\nnormal\n\n  Normal Distribution     \n\nbinomial->normal\n\n   \n\ncentral_tendency\n\n  Central Tendency     \n\ndispersion\n\n  Dispersion     \n\ncentral_tendency->dispersion\n\n   \n\ncoding\n\n  Dummy and effect coding     \n\nmultiple_regression\n\n  Multiple Regression     \n\ncoding->multiple_regression\n\n   \n\nttests\n\n  T-Tests     \n\ncoding->ttests\n\n   \n\ncorrelations\n\n  Correlations     \n\npartial_correlations\n\n  Partial Correlations     \n\ncorrelations->partial_correlations\n\n   \n\ncorrelations->permutation\n\n   \n\nsimple_regression\n\n  Simple Regression     \n\ncorrelations->simple_regression\n\n   \n\ndispersion->normal\n\n   \n\nfdr\n\n  False Discovery Rate     \n\nfwer_vs_fdr\n\n  FWER vs. FDR     \n\nfdr->fwer_vs_fdr\n\n   \n\nfwer\n\n  Family-Wise Error Rate     \n\nfwer->fwer_vs_fdr\n\n   \n\nglm\n\n  General Linear Models     \n\nglm->ttests\n\n   \n\nmediation\n\n Mediation   \n\nmixed_anova\n\n  Mixed ANOVA     \n\nmixed_anova->ancova\n\n   \n\nmultiple_regression->mediation\n\n   \n\nnormal->contingency\n\n   \n\nnormal->correlations\n\n   \n\nnormal->glm\n\n   \n\nskewness\n\n  Skewness     \n\nnormal->skewness\n\n   \n\nprobability\n\n  Probability     \n\nprobability->contingency\n\n   \n\nprobability->fdr\n\n   \n\nprobability->fwer\n\n   \n\nrepeated_measures_anova\n\n  Repeated Measures     \n\nrepeated_measures_anova->ancova\n\n   \n\nrepeated_measures_anova->permutation\n\n   \n\nsimple_regression->coding\n\n   \n\nsimple_regression->multiple_regression\n\n   \n\ntransforming\n\n  Transforming Data     \n\nskewness->transforming\n\n   \n\nstats_basics\n\n  Statistics Basics     \n\nstats_basics->central_tendency\n\n   \n\nstats_basics->probability\n\n   \n\ntransforming->correlations\n\n   \n\nttests->between_anova\n\n   \n\nttests->mixed_anova\n\n   \n\nttests->permutation\n\n   \n\nttests->repeated_measures_anova\n\n  \n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "permutations/permutations.html#simple-comparisons-between-2-conditions",
    "href": "permutations/permutations.html#simple-comparisons-between-2-conditions",
    "title": "Permutations (R)",
    "section": "Simple comparisons between 2 conditions",
    "text": "Simple comparisons between 2 conditions\nWe have used the word permutation before in calculating probabilities and Fisher’s exact score within contingency, in that it refers to a specific combination of items. In the context of permutation analysis, you compare the original permutation of items within categories and a range of other permutations of items within categories. For example, let’s imagine we want to see if cats are slower at running than dogs. We could measure the amount of time it took (in seconds) for cats and dogs to run 100 meters (assuming you are able to herd cats). We might get some data as follows:\n\ncat_dog_df <- data.frame(\n  animal = c(\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"dog\"),\n  time   = c(20,9,9,5,6,7)\n)\nrmarkdown::paged_table(cat_dog_df)\n\n\n\n  \n\n\n\nTo compare between these groups we would like to do a t-test, but would only be able to do this if the data is parametric. Let’s check whether there’s any problem with this assumption by using a Shapiro-Wilk test for dogs and cats:\n\nshapiro.test(cat_dog_df$time[cat_dog_df$animal==\"cat\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  cat_dog_df$time[cat_dog_df$animal == \"cat\"]\nW = 0.75, p-value < 2.2e-16\n\nshapiro.test(cat_dog_df$time[cat_dog_df$animal==\"dog\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  cat_dog_df$time[cat_dog_df$animal == \"dog\"]\nW = 1, p-value = 1\n\n\nFor cats there is a significant problem. As t-tests are dependent on the distributions of each group being parametric, we will use permutation analysis as a non-parametric solution. First, we want to capture how big the difference in means is, so that we can compare it to permutations we will describe later:\n\nmean_diff = mean(cat_dog_df$time[cat_dog_df$animal==\"cat\"]) - mean(cat_dog_df$time[cat_dog_df$animal==\"dog\"])\nmean_diff\n\n[1] 6.666667\n\n\nSo cats are \\(6.\\overline{6}\\) seconds slower than dogs on average. To capture how significant this difference is, we can create a distribution of possible differences that could have happened if we consider all permutations of data points being allocated to dogs and cats. Don’t woryr if this isn’t clear yet, the next steps should clarify the principle.\nWe currently have participants 1 to 3 as cats, and participants 4 to 6 as dogs. We need to identify all permutations participants 1 to 6 could have had as cats and dogs, and see how big the mean difference between cats and dogs would have been in each permutation.\n\n# all permutations of participant numbers for cats, with each permutation as a column\ncat_perms = combn(6, 3)\n\n# permutation 1 as an example (this is the original permutation)\ncat_perms[,1]\n\n[1] 1 2 3\n\n# permutation 2 as an example (participants 1, 2 and 4s data is \"cat\" in this permutation)\ncat_perms[,2]\n\n[1] 1 2 4\n\n#preparing vector to store the mean\nmean_diff_perms = numeric(dim(cat_perms)[2])\n\n# loop through all permutations \nfor(i in 1:ncol(cat_perms)){\n  # select the relevant permutation\n  these_cats = cat_perms[,i]\n  \n  # allocate the data points for these_cats to cats\n  cats_rt = cat_dog_df$time[these_cats]\n  \n  # identify dogs\n  dogs_index = !(1:6 %in% these_cats)\n  dogs_rt = cat_dog_df$time[dogs_index]\n  \n  # store mean difference\n  mean_diff_perms[i] = mean(cats_rt) - mean(dogs_rt)\n}\n\n# order the means from each permutation from most negative to most positive\nsort(mean_diff_perms)\n\n [1] -6.666667 -5.333333 -5.333333 -4.666667 -4.666667 -4.000000 -4.000000\n [8] -3.333333 -2.666667 -2.000000  2.000000  2.666667  3.333333  4.000000\n[15]  4.000000  4.666667  4.666667  5.333333  5.333333  6.666667\n\n\nWe now have the mean difference from every possible permutation, we have a distribution of all possible outcomes. The question now becomes, was the outcome from the original data (or a more extreme outcome) unusually unlikely? Well, the original outcome was that cats were \\(6.\\overline{6}\\) seconds slower than dogs. Out of all the possible outcomes, there are only 2 that have that size of absolute difference or bigger (but in this case there aren’t any bigger absolute differences). The likelihood of getting this size of difference then is 2 out of all permutations, which is 20, i.e. \\(\\frac{2}{20} = \\frac{1}{10} = .1\\) . So this wouldn’t be significant evidence. Note that if we had had a 1-tailed hypothesis that cats would be slower (or that dogs would be faster), then we wouldn’t have compared \\(6.\\overline{6}\\) to the absolute values of all outcomes, and so we would have found only a 1 in 20 chance of getting our outcome. You could summarise this in a table if it helps:\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\ndata.frame(\n  \"permutation difference\" = sort(mean_diff_perms),\n  \"permutation absolute difference\" = abs(sort(mean_diff_perms)),\n  \"original_difference\" = mean_diff,\n  \"1 tailed\" = ifelse((sort(mean_diff_perms))<mean_diff,\"lower\",\"equal to or higher\"),\n  \"2 tailed\" = ifelse(abs(sort(mean_diff_perms))<mean_diff,\"lower\",\"equal to or higher\"),\n  check.names = F\n) %>% \n  mutate_all(~cell_spec(.x, color = ifelse(.x == \"equal to or higher\", \"red\",\" black\"))) %>%\n  kable(escape = F) %>%\n  kable_styling()\n\n\n\n \n  \n    permutation difference \n    permutation absolute difference \n    original_difference \n    1 tailed \n    2 tailed \n  \n \n\n  \n    -6.66666666666667 \n    6.66666666666667 \n    6.66666666666667 \n    lower \n    equal to or higher \n  \n  \n    -5.33333333333333 \n    5.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -5.33333333333333 \n    5.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -4.66666666666667 \n    4.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -4.66666666666667 \n    4.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -4 \n    4 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -4 \n    4 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -3.33333333333333 \n    3.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -2.66666666666667 \n    2.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    -2 \n    2 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    2 \n    2 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    2.66666666666667 \n    2.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    3.33333333333333 \n    3.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    4 \n    4 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    4 \n    4 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    4.66666666666667 \n    4.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    4.66666666666667 \n    4.66666666666667 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    5.33333333333333 \n    5.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    5.33333333333333 \n    5.33333333333333 \n    6.66666666666667 \n    lower \n    lower \n  \n  \n    6.66666666666667 \n    6.66666666666667 \n    6.66666666666667 \n    equal to or higher \n    equal to or higher \n  \n\n\n\n\n\nThe above table hopefully visualises how the original outcome only will occur 1 in 20 times, and the size of the original outcome will only occur 1 in 10 times.\nWhilst we looked at mean differences above, you could also have done a t-test on each permutation to achieve the same goal:\n\n#preparing vector to store the mean\nttest_perms = numeric(dim(cat_perms)[2])\n\n# loop through all permutations \nfor(i in 1:ncol(cat_perms)){\n  # select the relevant permutation\n  these_cats = cat_perms[,i]\n  \n  # allocate the data points for these_cats to cats\n  cats_rt = cat_dog_df$time[these_cats]\n  \n  # identify dogs\n  dogs_index = !(1:6 %in% these_cats)\n  dogs_rt = cat_dog_df$time[dogs_index]\n  \n  # store t-test value (note that it can also be negative to reflect direction)\n  ttest_perms[i] = t.test(cats_rt,dogs_rt)$statistic\n}\n\n# order the means from each permutation from most negative to most positive\nsort(ttest_perms)\n\n [1] -1.7960530 -1.2649111 -1.2649111 -1.0583005 -1.0583005 -0.8751899\n [7] -0.8751899 -0.7088812 -0.5547002 -0.4091966  0.4091966  0.5547002\n[13]  0.7088812  0.8751899  0.8751899  1.0583005  1.0583005  1.2649111\n[19]  1.2649111  1.7960530\n\n\nThe original t-test value is 1.796 (see below)\n\nt.test(time ~ animal, cat_dog_df, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  time by animal\nt = 1.7961, df = 4, p-value = 0.1469\nalternative hypothesis: true difference in means between group cat and group dog is not equal to 0\n95 percent confidence interval:\n -3.639061 16.972395\nsample estimates:\nmean in group cat mean in group dog \n         12.66667           6.00000 \n\n\nconfirming that the result is just as significant (1 in 10 for a 2-tailed hypothesis, 1 in 20 for a correct 1-tailed hypothesis) regardless of whether you use t-tests or mean difference. This is useful/important because sometimes the analysis you want to do is going to be more complicated than a single comparison (e.g. an ANOVA), and so below we’ll address this.\nBut first, some of you might have been concerned about whether the extreme value of 20 was being disproportionately influential. To address this, we could do an analysis that compares medians rather than means:\n\n# original comparison\nmedian(cat_dog_df$time[cat_dog_df$animal==\"cat\"]) - median(cat_dog_df$time[cat_dog_df$animal==\"dog\"])\n\n[1] 3\n\n#preparing vector to store the mean\nmedian_diff_perms = numeric(dim(cat_perms)[2])\n\n# loop through all permutations \nfor(i in 1:ncol(cat_perms)){\n  # select the relevant permutation\n  these_cats = cat_perms[,i]\n  \n  # allocate the data points for these_cats to cats\n  cats_rt = cat_dog_df$time[these_cats]\n  \n  # identify dogs\n  dogs_index = !(1:6 %in% these_cats)\n  dogs_rt = cat_dog_df$time[dogs_index]\n  \n  # store mean difference\n  median_diff_perms[i] = median(cats_rt) - median(dogs_rt)\n}\n\n# order the means from each permutation from most negative to most positive\nsort(median_diff_perms)\n\n [1] -3 -3 -3 -3 -2 -2 -2 -2 -2 -2  2  2  2  2  2  2  3  3  3  3\n\n\nWe can see now that our original outcome of 3 was quite likely. If we have a 2-tailed hypothesis then the likelihood of an absolute value of 3 (or greater) is:\n\nsum(abs(median_diff_perms) >= 3) / length(median_diff_perms)\n\n[1] 0.4\n\n# or more elegantly\nmean(abs(median_diff_perms) >= 3)\n\n[1] 0.4\n\n\n40% (p-value of .4). Even if you correctly predicted dogs would be quicker…\n\nsum((median_diff_perms) >= 3) / length(median_diff_perms)\n\n[1] 0.2\n\n# or more elegantly\nmean((median_diff_perms) >= 3)\n\n[1] 0.2\n\n\nyou would only get a p-value of .2, which is not approaching significant.\n\nUsing permutations to compare of correlations between groups\nYou can use a variety of outcomes, not just comparisons of means, medians and t-values. Let’s use correlation r-values to illustrate this. Let’s imagine if we wanted to test whether height and speed were differently associated within cats and dogs. Let’s add heights (in centimeters) to our data above with dogs and cats:\n\ncat_dog_df <- data.frame(\n  animal = c(\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"dog\"),\n  time   = c(20,9,9,5,6,7),\n  height = c(50,45,50,80,70,75)\n)\nrmarkdown::paged_table(cat_dog_df)\n\n\n\n  \n\n\n\nFirst, let’s check a Pearson correlation between time and height for each animal:\n\n# separate r-values for cats and dogs\ncat_dog_df %>% \n  group_by(animal) %>% \n  summarise(r_value = cor.test(height,time)$estimate) -> cats_dogs_correlations\ncats_dogs_correlations$r_value[cats_dogs_correlations$animal == \"cat\"]\n\ncor \n0.5 \n\ncats_dogs_correlations$r_value[cats_dogs_correlations$animal == \"dog\"]\n\n cor \n-0.5 \n\ncats_dogs_correlations$r_value[cats_dogs_correlations$animal == \"cat\"] - \n  cats_dogs_correlations$r_value[cats_dogs_correlations$animal == \"dog\"]\n\ncor \n  1 \n\n\nNow, we can see that there seems to be a positive correlation between height and time for cats, and a negative association for dogs. But to see if the difference between these correlations is significant lets capture all possible permutation outcomes of this comparison between correlations\n\n#preparing vector to store the mean\ncorr_diff_perms = numeric(dim(cat_perms)[2])\n\n# loop through all permutations \nfor(i in 1:ncol(cat_perms)){\n  # select the relevant permutation\n  these_cats = cat_perms[,i]\n  \n  # allocate the data points for these_cats to cats\n  cats_rt = cat_dog_df$time[these_cats]\n  cats_height = cat_dog_df$height[these_cats]\n  \n  # identify dogs\n  dogs_index = !(1:6 %in% these_cats)\n  dogs_rt = cat_dog_df$time[dogs_index]\n  dogs_height = cat_dog_df$height[dogs_index]\n  \n  corr_diff_perms[i] = cor.test(cats_rt,cats_height)$estimate - cor.test(dogs_rt,dogs_height)$estimate \n\n}\n\n# order the means from each permutation from most negative to most positive\nsort(corr_diff_perms)\n\n [1] -1.00000000 -0.50702194 -0.42049220 -0.38067051 -0.25959598 -0.25870207\n [7] -0.17636806 -0.02508371 -0.02337287 -0.01747225  0.01747225  0.02337287\n[13]  0.02508371  0.17636806  0.25870207  0.25959598  0.38067051  0.42049220\n[19]  0.50702194  1.00000000\n\n\nAs before, the difference between correlations was strongest in the comparison on the original data, and so the likelihood is 1 in 20 if you correctly expected dogs would have a more negative association between height and time than cats, and 1 in 10 if you expected there to be some difference in association of these variables between cats and dogs.\nNote - we don’t need data to be non-parametric for the above to be a useful solution. This use of permutation analysis is one of a few ways to try to identify whether there’s a significant difference between two correlations. Other methods include Fisher’s Z if you are comparing between two groups of participants like above. Steiger’s Z can be used to do within-subject comparison of correlations, which is distinct from above as you are comparing correlations between 3 variables. We don’t cover Fisher’s Z and Steiger’s Z on this website, so you may want to search online for further information about them."
  },
  {
    "objectID": "permutations/permutations.html#comparisons-between-more-than-2-conditions",
    "href": "permutations/permutations.html#comparisons-between-more-than-2-conditions",
    "title": "Permutations (R)",
    "section": "Comparisons between more than 2 conditions",
    "text": "Comparisons between more than 2 conditions\nAbove we highlighted that you can compare the original t-value against each permutation’s t-value to identify whether your outcome was significantly unlikely. You can use a similar logic to use an ANOVA’s f-value as an output, using the logic above, or another complex calculation. We’ll add foxes to the above example to allow for use to do a one-way ANOVA\n\nanimal_df <- data.frame(\n  animal = c(\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"dog\",\"fox\",\"fox\",\"fox\"),\n  time   = c(20,9,9,5,6,7,5,8,6)\n)\n\n# ANOVA on real data\nanova_animals<-aov(time~animal,animal_df)\nsummary(anova_animals)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nanimal       2  84.67   42.33   2.908  0.131\nResiduals    6  87.33   14.56               \n\n\nThe ANOVA was not significant, but perhaps this was a consequence of there being an unusually slow cat distorting the distribution? Let’s see if the results are still not significant when we use permutation analysis to generate F values to compare with the original F value of 2.91.\nFirst, we need to address the fact that there are now 3 categories of animal, and so we need to calculate new permutations to reflect these 9 animals.\n\n# see pages on probability for explanations of the following lines\n# check how many unique permutations we should find with later code\nunique_perms = factorial(9)/(factorial(3)*factorial(3)*factorial(3))\n\n# generate all permutations (allowing for repetitions, despite the \"repetitions = F\" input)\nlibrary(RcppAlgos)\nanimal_combos = RcppAlgos::permuteGeneral(c(\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"dog\",\"fox\",\"fox\",\"fox\"),repetition = F)\n\n# identify repetitions\ncombos_repeats = duplicated(paste(\n  animal_combos[,1],\n  animal_combos[,2],\n  animal_combos[,3],\n  animal_combos[,4],\n  animal_combos[,5],\n  animal_combos[,6],\n  animal_combos[,7],\n  animal_combos[,8],\n  animal_combos[,9]\n))\n# removing repetitions\nanimal_combos_clean = animal_combos[!combos_repeats,]\n\nWe have 1680 permutations, let’s apply them to the original data to get our distribution of outcomes\n\n#preparing vector to store the ANOVA outputs (note that the matrix has swapped to 1 row per permutation rather than 1 column)\nanova_perms = numeric(dim(animal_combos_clean)[1])\n\n# loop through all permutations \nfor(i in 1:length(anova_perms)){\n  # select the relevant permutation\n  \n  animal_df$perm_animal = animal_combos_clean[i,]\n  \n  # ANOVA on real data\n  anova_animals = aov(time~perm_animal,animal_df)\n  this_summary  = summary(anova_animals)\n  anova_perms[i]   = this_summary[[1]][[\"F value\"]][1]\n\n}\n\n# order the means from each permutation from most negative to most positive (commented out as it's not very visibly accessible at this size)\n# sort(anova_perms)\nhist(anova_perms)\nabline(v = 2.908, col = \"red\", lwd = 2)\n\n\n\n\nWe can see that the f-value from the original data was unusually high. The above figure also confirms how ANOVAs are 2-tailed tests, i.e. there’s no possible negative F value that’s directional. To calculate the p-value, we will compare the original F value of 2.908 with all F values generated by permutation, to see how many were equal or greater than 2.908:\n\nmean(anova_perms>2.908)\n\n[1] 0.03571429\n\n\nOnly .036 or 3.6% were equal to or higher than 2.908, so our p-value is .036 and is significant (less than .05)."
  },
  {
    "objectID": "permutations/permutations.html#monte-carlo-permutations",
    "href": "permutations/permutations.html#monte-carlo-permutations",
    "title": "Permutations (R)",
    "section": "Monte Carlo permutations",
    "text": "Monte Carlo permutations\nIn all the above we have only had a small number of participants, so it is still manageable to identify all permutations and then apply an original output against the outputs of all permutations. However, the number of possible permutations quickly goes up with the number of participants. Let’s focus on analyses in which there are comparisons between 2 groups:\n\npp_perm_df <- data.frame(\n  participants = c(6,8,10,12,14,16,18,20)\n)\npp_perm_df$pp_per_group = pp_perm_df$participants/2\npp_perm_df$permutations = NA\n\nfor(i in 1:length(pp_perm_df$participants)){\n  pp_perm_df$permutations[i] = dim(combn(pp_perm_df$participants[i], pp_perm_df$pp_per_group[i]))[2]\n}\n\nknitr::kable(pp_perm_df)\n\n\n\n \n  \n    participants \n    pp_per_group \n    permutations \n  \n \n\n  \n    6 \n    3 \n    20 \n  \n  \n    8 \n    4 \n    70 \n  \n  \n    10 \n    5 \n    252 \n  \n  \n    12 \n    6 \n    924 \n  \n  \n    14 \n    7 \n    3432 \n  \n  \n    16 \n    8 \n    12870 \n  \n  \n    18 \n    9 \n    48620 \n  \n  \n    20 \n    10 \n    184756 \n  \n\n\n\n\n\nInitially the above table was going to include calculations of how many permutations there are for 30 and 40 participants, but R was taking a long time to calculate them. This issue of computing power is relevant, as it rapidly becomes less computationally realistic to calculate every permutation the more participants you have.\nTo address this, we can use Monte Carlo permutations, which is a fancy way of saying that you generate a random sample of permutations (e.g. 10,000 permutations) to create a distribution from. As the permutations will be random, the analysis won’t be biased towards or away from significance. Let’s use Gapminder data to illustrate Monte Carlo permutations in action.\nLet’s say we wanted to investigate whether continent was a good predictor of life expectancy. To simplify things, let’s compare Europe and the Americas.\n\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007 & (continent == \"Europe\" | continent == \"Americas\")\n)\nrmarkdown::paged_table(gapminder_2007)\n\n\n\n  \n\n\n\nThe table above has 55 rows, and so it would be computationally intensive (possibly impossible for most/all of us) to calculate every possible permutation of life expectancy and continent. We will instead create 10,000 random permutations to compare against the original median difference in life expectancy between Europe and the Americas.\nLet’s start with the median difference in life expectancy between Europe and the Americas\n\nlife_exp_diff = median(gapminder_2007$lifeExp[gapminder_2007$continent == \"Europe\"]) - \n  median(gapminder_2007$lifeExp[gapminder_2007$continent == \"Americas\"])\nlife_exp_diff\n\n[1] 5.7095\n\n\nNext, we need to randomly create 10,000 permutations, taking into account that there should be 30 countries allocated to Europe and 25 allocated to the Americas.\n\n# setting seed so that we don't land on an unrepresentative permutation. Be careful about setting seeds, they will still allow random functions like sample to run, but you will get the same output every time.\nset.seed(seed = 1)\n\nlife_exp_perms = numeric(10000)\n\nfor(i in 1:10000){\n  europe_sample = sample(gapminder_2007$lifeExp, 30, replace = FALSE)\n  # select the opposite data for Americas\n  americas_index = !(gapminder_2007$lifeExp %in% europe_sample)\n  americas_sample = gapminder_2007$lifeExp[americas_index]\n  \n  # compare the medians\n  life_exp_perms[i] = median(europe_sample) - median(americas_sample)\n}\n\nhist(life_exp_perms, xlim = c(-6,6))\nabline(v = life_exp_diff, col = \"red\", lwd = 2)\n\n\n\n\nThe above histogram summarises the permutation analysis, in which the red line represents the actual difference in medians between Europe and America (a positive score reflecting longer life expectancy in Europe). To calculate the likelihood we would get this difference randomly (and thus its p-value), let’s start with a one-directional hypothesis that life expectancy is higher in Europe, and so the likelihood of getting a value of 5.7095 randomly. This is reflected in how many random permutations were greater than or equal to 5.7095:\n\noptions(scipen = 999)\nmean(life_exp_perms > life_exp_diff)\n\n[1] 0\n\n\nIn this analysis there were 0 permutations that showed an equal greater bias in life expectancy in favour of Europe. This suggests the original bias in favour of Europe was extremely unlikely to have occurred by chance (p < .001), and so you have significant evidence that life expectancy in Europe is higher than the Americas.\nFor completeness, let’s now look at a 2-tailed version of this analysis by comparing the absolute values of the permutations to the actual life expectancy difference:\n\nmean(abs(life_exp_perms) > life_exp_diff)\n\n[1] 0\n\n\nYou could visualise the 2-tailed test as follows:\n\nhist(abs(life_exp_perms), xlim = c(0,6))\nabline(v = life_exp_diff, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "permutations/permutations.html#within-subject-analyses-have-important-restrictions-in-valid-permutations",
    "href": "permutations/permutations.html#within-subject-analyses-have-important-restrictions-in-valid-permutations",
    "title": "Permutations (R)",
    "section": "Within-subject analyses have important restrictions in valid permutations",
    "text": "Within-subject analyses have important restrictions in valid permutations\nThe permutations analyses above have been mostly between-subject, whereas within-subject permutations are more restricted. It largely depends on the analysis, so we will try to illustrate some key principles using a paired samples t-test. Let’s imagine that you want to measure how many hours sleep a group of participants had in 2 nights, one in which they had a caffeinated drink at dinner, one in which they had no caffeine at dinner time. We might have data as follows:\n\nsleep_df = data.frame(\n  caffeine    = c(5,6,7,6,7,6),\n  no_caffeine = c(6,6,8,6,8,7)\n)\nknitr::kable(sleep_df)\n\n\n\n \n  \n    caffeine \n    no_caffeine \n  \n \n\n  \n    5 \n    6 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    6 \n    7 \n  \n\n\n\n\n\nThe effect of caffeine on sleep can be captured by a paired-samples t-test:\n\nt.test(sleep_df$caffeine, sleep_df$no_caffeine, paired = T)\n\n\n    Paired t-test\n\ndata:  sleep_df$caffeine and sleep_df$no_caffeine\nt = -3.1623, df = 5, p-value = 0.02503\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.2085929 -0.1247404\nsample estimates:\nmean difference \n     -0.6666667 \n\n\nSo far, so significant, with the there being more sleep in the no caffeine 6.8333333 than caffeine 6.1666667 condition. Whilst it might not be seen as necessary to double check this conclusion with permutation analysis, it can be a useful conceptual replication of the analysis (and is less sensitive to any problems with the assumptions of a normal distribution).\nAs this is within-subject, we are keeping the permutations within each subject. This means that we can swap the amount of sleep for caffeine vs. no-caffeine for each participant, but not between participants. As there are 2 levels for each participant, and 6 participants, we know that there are \\(2^6 = 64\\) permutations possible. Let’s first try to capture all the permutations and then look at a couple of them to visualise what a permutation means.\nTo capture all the permutations, we will identify for each participant whether the first column of their data (currently “caffeine”) will be allocated to “caffeine” or “no caffeine” for the permutation. By definition, the second column will then be allocated to the opposite condition.\n\n# each row is a permutation, each column is a participant. The outcome is whether the original \"caffeine\" response will be treated as \"caffeine\" in the permutation\ncaffeine_perms  = RcppAlgos::permuteGeneral(c(\"caffeine\",\"no caffeine\"),6,repetition = T)\n\nThe first permutation doesn’t change the original data:\n\ncaffeine_perms[1,]\n\n[1] \"caffeine\" \"caffeine\" \"caffeine\" \"caffeine\" \"caffeine\" \"caffeine\"\n\nknitr::kable(sleep_df)\n\n\n\n \n  \n    caffeine \n    no_caffeine \n  \n \n\n  \n    5 \n    6 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    6 \n    7 \n  \n\n\n\n\n\nBut the second permutation swaps the caffeine and no caffeine data for one participant (participant 6):\n\ncaffeine_perms[2,]\n\n[1] \"caffeine\"    \"caffeine\"    \"caffeine\"    \"caffeine\"    \"caffeine\"   \n[6] \"no caffeine\"\n\nsleep_df_perm = sleep_df\nsleep_df_perm$caffeine[6]    = sleep_df$no_caffeine[6]\nsleep_df_perm$no_caffeine[6] = sleep_df$caffeine[6]\nknitr::kable(sleep_df_perm)\n\n\n\n \n  \n    caffeine \n    no_caffeine \n  \n \n\n  \n    5 \n    6 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    6 \n    6 \n  \n  \n    7 \n    8 \n  \n  \n    7 \n    6 \n  \n\n\n\n\n\nNote how the last row’s values swapped in the second permutation compared to the first.\nNow let’s actually calculate the t-values for all the permutations to see whether the original t-value was unlikely to happen by chance:\n\n#preparing vector to store the paired t-test outputs (note that the matrix is 1 row per permutation rather than 1 per column)\npaired_perms = numeric(dim(caffeine_perms)[1])\n\n# loop through all permutations \nfor(i in 1:length(paired_perms)){\n  \n  this_perm = caffeine_perms[i,]\n  \n  sleep_df_perm = sleep_df\n  for(j in 1:6){\n    if(this_perm[j] == \"no caffeine\"){\n      sleep_df_perm$caffeine[j]    = sleep_df$no_caffeine[j]\n      sleep_df_perm$no_caffeine[j] = sleep_df$caffeine[j]\n    }\n  }\n  paired_perms[i] = t.test(sleep_df_perm$caffeine, sleep_df_perm$no_caffeine, paired = T)$statistic\n}\n\n# order the means from each permutation from most negative to most positive (commented out as it's not very visibly accessible at this size)\nsort(paired_perms)\n\n [1] -3.162278 -3.162278 -3.162278 -3.162278 -1.000000 -1.000000 -1.000000\n [8] -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000\n[15] -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000  0.000000\n[22]  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n[29]  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n[36]  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n[43]  0.000000  0.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n[50]  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n[57]  1.000000  1.000000  1.000000  1.000000  3.162278  3.162278  3.162278\n[64]  3.162278\n\n\nSo now let’s calculate the one-tailed likelihood of getting a t-value of -3.16 or less (as -3.16 was the t-value from the original t-test)\n\nmean(paired_perms <= t.test(sleep_df$caffeine, sleep_df$no_caffeine, paired = T)$statistic)\n\n[1] 0.0625\n\n\nNot as significant as the original t-test (p = .025). Let’s see what the p-value is for a 2-tailed hypothesis:\n\n# note that we now are looking at greater than as all values are absolute\nmean(abs(paired_perms) >= abs(t.test(sleep_df$caffeine, sleep_df$no_caffeine, paired = T)$statistic))\n\n[1] 0.125\n\n\nUnsurprisingly, the p-value doubled (as distributions are symmetric when you are able to complete all possible permutations, but almost never when using Monte Carlo simulations).\n\n\n\n\n\n\nOptional - why not swap between participants?\n\n\n\nIf you were to swap between participants you would be creating a distribution that includes between-participant variance, and so your permutation analysis would be on more noisy data because variance between participants isn’t relevant."
  },
  {
    "objectID": "permutations/permutationsQuestions.html",
    "href": "permutations/permutationsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nPermutation analysis can only be used on non-parametric data\n\nviewof permutatations_1_response = Inputs.radio(['True','False']);\ncorrect_permutatations_1 = 'False';\npermutatations_1_result = {\n  if(permutatations_1_response == correct_permutatations_1){\n    return 'Correct! Whilst it is useful for analysis of non-parametric data, it is not restricted to use in these contexts. It would be valid to double check the conclusion of your parametric analysis, or for alternative analyses such compare how significant the difference is between two correlations.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nPermutation analysis can only be done when comparing 2 groups?\n\nviewof permutatations_2_response = Inputs.radio(['True','False']);\ncorrect_permutatations_2 = 'False';\npermutatations_2_result = {\n  if(permutatations_2_response == correct_permutatations_2){\n    return 'Correct! It can be used on any analysis that generates a single output value (even if that single output value is a combination of other values). F-values from ANOVAs are an example of how you can compare between more than 2 groups. You can also compare between conditions within participants, but you would need to be mindful that you only permute swaps within participants for within-subject factors.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "advancedR/fancyFigures.html",
    "href": "advancedR/fancyFigures.html",
    "title": "Fancy Figures",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "correlations/partialCorrelations.html",
    "href": "correlations/partialCorrelations.html",
    "title": "Partial Correlations (R,Python)",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started\n\n\n\nAs you may have heard, correlation does not equal causation. One possible reason for this is that there’s a third variable that explains an association. Let’s imagine that we are trying to understand whether life expectancy goes up over time and why. First of all, let’s check if lifeExpectancy is going up over time using the gapminder data:\n\nRPython\n\n\n\nlibrary(gapminder)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\ngapminder %>% \n  group_by(year) %>% \n  summarise(\n    lifeExp = mean(lifeExp),\n    gdpPercap = mean(gdpPercap)\n  ) -> gapminder_by_year\n\ncor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$lifeExp\nt = 18.808, df = 10, p-value = 3.91e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9498070 0.9962336\nsample estimates:\n     cor \n0.986158 \n\n\n\nggplot(data = gapminder_by_year, aes(x = year, y = lifeExp)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = 'y ~ x')\n\n\n\n\nFigure 1: a scatterplot showing the association between year and life expectancy using Gapminder data.\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nfrom gapminder import gapminder\n\ngapminder_by_year = gapminder.groupby('year').agg({'lifeExp': 'mean', 'gdpPercap': 'mean'}).reset_index()\nprint(gapminder_by_year)\n\ncorrelation, p_value = pearsonr(gapminder_by_year['year'], gapminder_by_year['lifeExp'])\nprint(\"Correlation coefficient:\", correlation)\nprint(\"p-value:\", p_value)\n\nsns.scatterplot(data=gapminder_by_year, x='year', y='lifeExp')\nsns.regplot(data=gapminder_by_year, x='year', y='lifeExp')\nplt.xlabel('Year')\nplt.ylabel('Life Expectancy')\nplt.show()\n\n\n\n\nSo now that we’ve confirmed that there is a positive association between the year and life expectancy, the next question is why? What changes from year to year that could explain increased life expectancy? Let’s investigate whether gdp per capita generally goes up each year, and whether it’s associated with life expectancy. If both of these things are true, then perhaps the increase in gdp per year is an explanation of the association between year and life expectancy.\n\nIs year and gdp per capita associated?\n\nRPython\n\n\n\ncor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$gdpPercap\nt = 17.039, df = 10, p-value = 1.022e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9393538 0.9954265\nsample estimates:\n      cor \n0.9832101 \n\n\n\nggplot(data = gapminder_by_year, aes(x = year, y = gdpPercap)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = 'y~x')\n\n\n\n\nFigure 2: a scatterplot showing the association between year and GDP per Capita using Gapminder data.\n\n\n\n\n\n\n\ncorrelation, p_value = pearsonr(gapminder_by_year['year'], gapminder_by_year['gdpPercap'])\nprint(\"Correlation coefficient:\", correlation)\nprint(\"p-value:\", p_value)\n\nsns.scatterplot(data=gapminder_by_year, x='year', y='gdpPercap')\nsns.regplot(data=gapminder_by_year, x='year', y='gdpPercap')\nplt.xlabel('Year')\nplt.ylabel('gdpPercap')\nplt.show()\n\n\n\n\nWhilst there are some outliers in earlier years, we seem to have found that gdp per capita has gone up.\n\n\nIs gdp per capita and life expectancy associated?\n\nRPython\n\n\n\ncor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$gdpPercap and gapminder_by_year$lifeExp\nt = 11.137, df = 10, p-value = 5.875e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8663785 0.9895601\nsample estimates:\n      cor \n0.9619721 \n\n\n\nggplot(data = gapminder_by_year, aes(x = gdpPercap, y = lifeExp)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = 'y ~ x')\n\n\n\n\nFigure 3: a scatterplot showing the association between GDP per Capita and life expectancy using Gapminder data.\n\n\n\n\n\n\n\n# Perform correlation test between gdpPercap and lifeExp\ncorrelation, p_value = stats.pearsonr(gapminder_by_year['gdpPercap'], gapminder_by_year['lifeExp'])\n\n# Print the correlation coefficient and p-value\nprint(\"Correlation coefficient:\", correlation)\nprint(\"p-value:\", p_value)\n\n# Create scatter plot with regression line\nsns.scatterplot(data=gapminder_by_year, x='gdpPercap', y='lifeExp')\nsns.regplot(data=gapminder_by_year, x='gdpPercap', y='lifeExp', scatter=False)\nplt.xlabel('GDP per capita')\nplt.ylabel('Life Expectancy')\nplt.show()\n\n\n\n\nSo the above analysis suggests that all three variables are incredibly related. (you are unlikely to see such strong associations in real psychology experiments). But let’s now check in whether there’s still an association between the year and life expectancy once you control for GDP per capita. This partial correlation can be visualised as follows:\n\n\n\n\n\n\n\n\nG\n\n  \n\nX:Year\n\n X:Year   \n\nY: Life Expectancy\n\n Y: Life Expectancy   \n\nX:Year–Y: Life Expectancy\n\n   \n\nZ: GDP\n\n Z: GDP   \n\nX:Year–Z: GDP\n\n   \n\nZ: GDP–Y: Life Expectancy\n\n  \n\n\nFigure 4: The association between Year and Life expectancy could be explained by GDP.\n\n\n\n\nTo calculate the partial correlation r value you control for the association between the two variables\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{originalCorrelation - varianceExplainedByCovariate}{varianceNotExplainedByCovariate}\n\\]\n\n\\(x\\) = The Year\n\\(y\\) = Life expectancy\n\\(z\\) = GDP per capita\n\\(\\sqrt{1- r^2_{xz}}\\) = variance not explained by correlation between Year(\\(x\\)) and GDP (\\(z\\))\n\\(\\sqrt{1- r^2_{yz}}\\) = variance not explained by correlation between Life Expectancy (\\(y\\)) and GDP (\\(z\\))\n\nOne way to think of the formula above is that: - the top-half represents how much variance is explained by overlap between the two main variables, subtracted by the variance of each variable with the confound - the bottom-half represents how much variance there is left to explain once you’ve removed associations between each main variable and the covariate.\nLet’s see what the r value is after this partial correlation:\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{.986158 - .9832101 * .9619721 }{\\sqrt{(1-.9832101^2)(1-.9619721^2)}} = \\frac{.040354}{.04984321} = .8092841\n\\]\nLet’s check if the manually calculated partial r-value is the same as what R gives us:\n\nRPython\n\n\n\nlibrary(ppcor)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor(gapminder_by_year)\n\n$estimate\n               year    lifeExp  gdpPercap\nyear      1.0000000  0.8092844  0.7629391\nlifeExp   0.8092844  1.0000000 -0.2521280\ngdpPercap 0.7629391 -0.2521280  1.0000000\n\n$p.value\n                 year    lifeExp   gdpPercap\nyear      0.000000000 0.00254769 0.006309324\nlifeExp   0.002547690 0.00000000 0.454498736\ngdpPercap 0.006309324 0.45449874 0.000000000\n\n$statistic\n              year    lifeExp  gdpPercap\nyear      0.000000  4.1331001  3.5404830\nlifeExp   4.133100  0.0000000 -0.7816358\ngdpPercap 3.540483 -0.7816358  0.0000000\n\n$n\n[1] 12\n\n$gp\n[1] 1\n\n$method\n[1] \"pearson\"\n\n\n\n\n\nimport pingouin as pg\n\n# Calculate partial correlation with p-values, estimates, and statistics\npartial_corr1 = pg.partial_corr(data=gapminder_by_year, x=\"gdpPercap\", y=\"lifeExp\", covar=\"year\", method='pearson')\npartial_corr2 = pg.partial_corr(data=gapminder_by_year, x=\"year\", y=\"gdpPercap\", covar=\"lifeExp\", method='pearson')\npartial_corr3 = pg.partial_corr(data=gapminder_by_year, x=\"lifeExp\", y=\"year\", covar=\"gdpPercap\", method='pearson')\n\ncoef1 = partial_corr1['r'].values[0]\ncoef2 = partial_corr2['r'].values[0]\ncoef3 = partial_corr3['r'].values[0]\n\np_value1 = partial_corr1['p-val'].values[0]\np_value2 = partial_corr2['p-val'].values[0]\np_value3 = partial_corr3['p-val'].values[0]\n\n# Create a matrix\nmatrix1 = np.zeros((3, 3))\n\n# Assign the interaction values\nmatrix1[0, 1] = coef3\nmatrix1[1, 0] = coef3\nmatrix1[0, 2] = coef2\nmatrix1[2, 0] = coef2\nmatrix1[1, 2] = coef1\nmatrix1[2, 1] = coef1\n\n# Set the diagonal to 1\nnp.fill_diagonal(matrix1, 1)\n\n# Create a DataFrame from the matrix\nmatrix_coef = pd.DataFrame(matrix1, columns=['year', 'lifeExp', 'gdpPercap'], index=['year', 'lifeExp', 'gdpPercap'])\n\n# Create a matrix\nmatrix2 = np.zeros((3, 3))\n\n# Assign the interaction values\nmatrix2[0, 1] = p_value3\nmatrix2[1, 0] = p_value3\nmatrix2[0, 2] = p_value2\nmatrix2[2, 0] = p_value2\nmatrix2[1, 2] = p_value1\nmatrix2[2, 1] = p_value1\n\n# Set the diagonal to 1\nnp.fill_diagonal(matrix2, 1)\n\n# Create a DataFrame from the matrix\nmatrix_p_val = pd.DataFrame(matrix2, columns=['year', 'lifeExp', 'gdpPercap'], index=['year', 'lifeExp', 'gdpPercap'])\n\n# Print the resulting matrix\nprint(matrix_p_val)\n\n# Print the resulting matrix\nprint(matrix_coef)\n\n\n\n\nYes, the pearson correlation between year and life expectancy is .8092844, so the difference of .0000003 is a rounding error from the manual calculations above. Just to confirm that this is a rounding error, here’s what you get if you complete the same steps but with the estimate part of the correlation objects instead:\n\nRPython\n\n\n\nx.y.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\nx.z.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\ny.z.cor <- cor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n(x.y.cor$estimate - x.z.cor$estimate * y.z.cor$estimate)/sqrt((1-x.z.cor$estimate^2) * (1 - y.z.cor$estimate^2))\n\n      cor \n0.8092844 \n\n\n\n\n\nimport scipy.stats as stats\nimport numpy as np\n\n# Calculate correlation coefficients\nx_y_cor = stats.pearsonr(gapminder_by_year['year'], gapminder_by_year['lifeExp'])[0]\nx_z_cor = stats.pearsonr(gapminder_by_year['year'], gapminder_by_year['gdpPercap'])[0]\ny_z_cor = stats.pearsonr(gapminder_by_year['gdpPercap'], gapminder_by_year['lifeExp'])[0]\n\n# Calculate the desired value\nresult = (x_y_cor - x_z_cor * y_z_cor) / np.sqrt((1 - x_z_cor**2) * (1 - y_z_cor**2))\n\n# Print the result\nprint(result)\n\n\n\n\nConfirming that the difference in the manual calculation is a rounding error. Note that the numbers you get from R may be rounded numbers, and so your calculations may reflect the rounding.\nTo conclude, as there is still an association between Year and Life Expectancy once controlling for GDP, this partial correlation is consistent with GDP not being the only explanation for why Life Expectancy goes up each year.\n\nQuestion 1\nWhat does a variable need to correlate with to be a viable candidate as a covariate?\n\nviewof partial_correlations_1_response = Inputs.radio(['Both other variables','At least one other variable','No other variables']);\ncorrect_partial_correlations_1 = 'Both other variables';\npartial_correlations_1_result = {\n  if(partial_correlations_1_response == correct_partial_correlations_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "correlations/partialCorrelationsQuestions.html",
    "href": "correlations/partialCorrelationsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhat does a variable need to correlate with to be a viable candidate as a covariate?\n\nviewof partial_correlations_1_response = Inputs.radio(['Both other variables','At least one other variable','No other variables']);\ncorrect_partial_correlations_1 = 'Both other variables';\npartial_correlations_1_result = {\n  if(partial_correlations_1_response == correct_partial_correlations_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "correlations/correlationsQuestions.html",
    "href": "correlations/correlationsQuestions.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "Question 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "correlations/cooksd.html",
    "href": "correlations/cooksd.html",
    "title": "Cook’s Distance (R, incomplete)",
    "section": "",
    "text": "Cook’s Distance (AKA Cook’s D) summarises whether there are data points that are disproportionately influencing a correlation or simple regression.\n\nlibrary(ggplot2)\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007\n)\n\ngapminder_2007$corr.outlier <- cooks.distance(model = lm(lifeExp ~ gdpPercap, gapminder_2007)) > 4/142\n\nggplot(\n  data=gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp\n  )\n) +\n  geom_point(aes(color = corr.outlier)) + \n  geom_smooth(method = lm, formula = \"y ~ x\")\n\n\n\n\nis cook’s distance the difference in residuals between a model with and without a data point?\n0.02069620738 - score for first item\n\nthis_model <- lm(lifeExp ~ gdpPercap, gapminder_2007)\nthis_model$residuals[2:142]\n\n          2           3           4           5           6           7 \n 13.0746657   8.7702301 -19.8911299   7.6121709  -0.2705981  -2.7540718 \n          8           9          10          11          12          13 \n -2.9147296   3.6099346  -1.5913589  -3.7559419   3.5531359  10.5420588 \n         14          15          16          17          18          19 \n-16.8463317   7.0482188   6.6342522  -8.0460633 -10.2596628  -0.9345570 \n         20          21          22          23          24          25 \n-10.4367387  -2.0528745 -15.2744773 -10.0003672  10.5952492  10.2357286 \n         26          27          28          29          30          31 \n  8.8592184   4.9580414 -13.2804878  -6.5580766  13.0711521 -12.2218631 \n         32          33          34          35          36          37 \n  6.8679441  13.0062081   2.3724697  -3.7107349  -6.1014702   8.8303780 \n         38          39          40          41          42          43 \n 11.0491599   8.2163890   8.6626204 -15.7304355  -1.9342885  -7.0587859 \n         44          45          46          47          48          49 \n -1.4100171   1.6778622 -11.2449522  -0.5972526  -0.6564938  -0.3895150 \n         50          51          52          53          54          55 \n  2.3716877   7.3891404  -4.1592473 -13.5466984   0.5847459   8.3715872 \n         56          57          58          59          60          61 \n -2.6677900   2.2982367  -0.8606659   3.5699630   8.8284799   4.0039531 \n         62          63          64          65          66          67 \n -2.8693162  -6.5967158   4.9175988   2.7776063   8.3369672   2.8681884 \n         68          69          70          71          72          73 \n 10.0898469  -6.3879361   6.7163535   4.1814531 -12.1185481   5.7622523 \n         74          75          76          77          78          79 \n-17.9735247 -14.1517469   6.7041055  -0.7883088 -11.7464578   6.7419750 \n         80          81          82          83          84          85 \n -5.7629144   3.4495006   6.2542769   8.9980281   5.2649277   9.0813768 \n         86          87          88          89          90          91 \n  9.1643859 -18.0084483   1.9018953  -9.7249409   3.5240074  -3.2488695 \n         92          93          94          95          96          97 \n  4.5921209  11.5816637  -3.0934674 -13.9898238 -10.8168007   1.8559417 \n         98          99         100         101         102         103 \n  4.2570118   9.7215829   9.5276921   7.1348833  10.0895856   6.1919036 \n        104         105         106         107         108         109 \n  5.4649532   6.8653696  11.9894530   6.0239012 -13.8735532   4.9439324 \n        110         111         112         113         114         115 \n -0.5856827   2.4052755   8.2010146 -17.5472042  -9.6301791   3.1967583 \n        116         117         118         119         120         121 \n  1.9425134 -11.9967262 -16.1326655   3.0124664  10.3008666  -2.6677248 \n        122         123         124         125         126         127 \n-22.8283427  -0.2548516  -1.7612700  11.9112315   0.5369554  -7.7542648 \n        128         129         130         131         132         133 \n  6.2983510  -1.7082204  -1.2204860   9.8382065   6.8222933  -8.6967059 \n        134         135         136         137         138         139 \n -1.2955812  -8.6896144  10.0574246   6.9079504  13.1277383  11.9287963 \n        140         141         142 \n  1.6791936 -17.9915824 -16.3779179 \n\nnext_model <- lm(lifeExp ~ gdpPercap, gapminder_2007[2:142,])\nnext_model$residuals\n\n          1           2           3           4           5           6 \n 12.9144165   8.6121573 -20.0600429   7.5039315  -0.2142270  -2.6848461 \n          7           8           9          10          11          12 \n -2.8936226   3.4151322  -1.5406336  -3.9503640   3.3768110  10.3932817 \n         13          14          15          16          17          18 \n-16.9561638   6.9117518   6.5100611  -8.2421900 -10.4617713  -1.1269078 \n         19          20          21          22          23          24 \n-10.6265939  -1.9821837 -15.4744883 -10.1927920  10.4899914  10.0680460 \n         25          26          27          28          29          30 \n  8.7070989   4.7601597 -13.4837557  -6.7358426  12.9390881 -12.4154988 \n         31          32          33          34          35          36 \n  6.7736896  12.8688465   2.3406517  -3.6479556  -6.2910185   8.6708003 \n         37          38          39          40          41          42 \n 10.8960271   8.0534349   8.5007849 -15.8434279  -2.1347909  -7.2589125 \n         43          44          45          46          47          48 \n -1.3629823   1.7040921 -11.3499451  -0.7969084  -0.6173392  -0.5848012 \n         49          50          51          52          53          54 \n  2.3756340   7.2231829  -4.3574596 -13.7476731   0.3885021   8.1931811 \n         55          56          57          58          59          60 \n -2.5712117   2.2297480  -0.7910275   3.3832250   8.6500154   3.8867924 \n         61          62          63          64          65          66 \n -3.0407086  -6.4929087   4.9062278   2.7893917   8.1872368   2.9034336 \n         67          68          69          70          71          72 \n  9.9188225  -6.5821913   6.5230851   4.1535485 -11.9643377   5.6363910 \n         73          74          75          76          77          78 \n-18.1669735 -14.3539737   6.5903790  -0.9857449 -11.9460635   6.6312444 \n         79          80          81          82          83          84 \n -5.9603671   3.2578290   6.1321851   8.8836939   5.0830816   8.9463396 \n         85          86          87          88          89          90 \n  8.9880461 -18.2075649   1.7036932  -9.8937488   3.3269254  -3.1745401 \n         91          92          93          94          95          96 \n  4.5781786  11.3971841  -3.2941347 -14.1798928 -10.6470064   1.8201931 \n         97          98          99         100         101         102 \n  4.0714424   9.5907665   9.3540329   6.9858220   9.9084593   6.1035073 \n        103         104         105         106         107         108 \n  5.4154728   6.8069127  11.8423773   5.9006805 -14.0723703   4.7507048 \n        109         110         111         112         113         114 \n -0.6264584   2.2129147   8.0700260 -17.7460255  -9.4772138   3.1333575 \n        115         116         117         118         119         120 \n  1.9330045 -12.1950640 -16.2675829   3.0261624  10.1256664  -2.8533211 \n        121         122         123         124         125         126 \n-22.9994125  -0.2028558  -1.6815552  11.7376613   0.5498701  -7.9512242 \n        127         128         129         130         131         132 \n  6.1496659  -1.9068863  -1.2889781   9.6867434   6.6812085  -8.8940538 \n        133         134         135         136         137         138 \n -1.2485755  -8.5685096   9.9327065   6.7893462  12.9409195  11.7464149 \n        139         140         141 \n  1.4911525 -18.1872973 -16.5797251 \n\nnew_resid = (next_model$coefficients[2] * gapminder_2007$gdpPercap[1] + next_model$coefficients[1] - gapminder_2007$lifeExp[1])\n\n((-new_resid - this_model$residuals[1])^2)/(2*mean(next_model$residuals^2))\n\n   gdpPercap \n0.0002554773 \n\n((-new_resid - this_model$residuals[1])^2)/(2*mean(next_model$residuals^2)) + (sum((this_model$residuals[2:142] - next_model$residuals)^2)/(2*mean(next_model$residuals^2)))\n\n gdpPercap \n0.02136603 \n\n#y_this_pred = gapminder_2007$gdpPercap * \n\n\n  \nHmat = hatvalues(this_model)\n\nLeverage = Hmat/(1 - Hmat)\n\nmse = (this_model$residuals)^2/var(this_model$residuals)\n\nCooksD <- (1/2)*(mse)*Leverage\n  \n\n# 0.02069620738\n(sum((weighted.residuals(this_model[2:142]) - next_model$residuals)^2)/(2*sum(this_model$residuals^2)/141))\n\nWarning in weighted.residuals(this_model[2:142]) - next_model$residuals: longer\nobject length is not a multiple of shorter object length\n\n\n[1] 133.2546\n\n(sum((this_model$residuals[2:142] - next_model$residuals)^2)/(2*sum(this_model$residuals^2)/141))\n\n[1] 0.0205948\n\n + \n  ((-new_resid - this_model$residuals[1])^2)/(2*sum(next_model$residuals^2)/140)\n\n   gdpPercap \n0.0002536654 \n\nplot(gapminder_2007$gdpPercap,this_model$fitted.values)\n\n\n\nmean(next_model$fitted.values - this_model$fitted.values[2:142])\n\n[1] 0.1160184\n\ncooks.distance\n\nfunction (model, ...) \nUseMethod(\"cooks.distance\")\n<bytecode: 0x150175920>\n<environment: namespace:stats>\n\n\n0.02069620738\n\\[\nD_i = \\frac{e^2_i}{(k+1) * MS_E} * \\frac{h_i}{(1-h_i)^2}\n\\]\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i-\\bar{x})^2}{SS_x}\n\\]\n\nexample_data <- data.frame(\n  x = c(1,2,10,7),\n  y = c(2,3,5,5)\n)\n\norig_model <- lm(y ~ x, example_data)\ncooks.distance(orig_model)\n\n        1         2         3         4 \n0.6143274 0.1020408 2.0031217 0.3218240 \n\npoint_1_removed <- lm(y ~ x, example_data[c(2,3,4),])\npoint_2_removed <- lm(y ~ x, example_data[c(1,3,4),])\npoint_3_removed <- lm(y ~ x, example_data[c(1,2,4),])\npoint_4_removed <- lm(y ~ x, example_data[c(1,2,3),])\n\nsum((orig_model$residuals[c(2,3,4)] - point_1_removed$residuals)^2)/(2*sum(orig_model$residuals^2)/3)\n\n[1] 0.4180839\n\nsum((orig_model$residuals[c(1,3,4)] - point_2_removed$residuals)^2)/(2*sum(orig_model$residuals^2)/3)\n\n[1] 0.08928571\n\nsum((orig_model$residuals[c(1,2,4)] - point_3_removed$residuals)^2)/(2*sum(orig_model$residuals^2)/3)\n\n[1] 0.8624552\n\nsum((orig_model$residuals[c(1,2,3)] - point_4_removed$residuals)^2)/(2*sum(orig_model$residuals^2)/3)\n\n[1] 0.3262938\n\n\ncooks.distance(model, \\dots)\n\\method{cooks.distance}{lm}(model, infl = lm.influence(model, do.coef = FALSE),\nres = weighted.residuals(model),\nsd = sqrt(deviance(model)/df.residual(model)),\nhat = infl$hat, \\dots)\n\nthis_model$residuals\n\n          1           2           3           4           5           6 \n-16.3585885  13.0746657   8.7702301 -19.8911299   7.6121709  -0.2705981 \n          7           8           9          10          11          12 \n -2.7540718  -2.9147296   3.6099346  -1.5913589  -3.7559419   3.5531359 \n         13          14          15          16          17          18 \n 10.5420588 -16.8463317   7.0482188   6.6342522  -8.0460633 -10.2596628 \n         19          20          21          22          23          24 \n -0.9345570 -10.4367387  -2.0528745 -15.2744773 -10.0003672  10.5952492 \n         25          26          27          28          29          30 \n 10.2357286   8.8592184   4.9580414 -13.2804878  -6.5580766  13.0711521 \n         31          32          33          34          35          36 \n-12.2218631   6.8679441  13.0062081   2.3724697  -3.7107349  -6.1014702 \n         37          38          39          40          41          42 \n  8.8303780  11.0491599   8.2163890   8.6626204 -15.7304355  -1.9342885 \n         43          44          45          46          47          48 \n -7.0587859  -1.4100171   1.6778622 -11.2449522  -0.5972526  -0.6564938 \n         49          50          51          52          53          54 \n -0.3895150   2.3716877   7.3891404  -4.1592473 -13.5466984   0.5847459 \n         55          56          57          58          59          60 \n  8.3715872  -2.6677900   2.2982367  -0.8606659   3.5699630   8.8284799 \n         61          62          63          64          65          66 \n  4.0039531  -2.8693162  -6.5967158   4.9175988   2.7776063   8.3369672 \n         67          68          69          70          71          72 \n  2.8681884  10.0898469  -6.3879361   6.7163535   4.1814531 -12.1185481 \n         73          74          75          76          77          78 \n  5.7622523 -17.9735247 -14.1517469   6.7041055  -0.7883088 -11.7464578 \n         79          80          81          82          83          84 \n  6.7419750  -5.7629144   3.4495006   6.2542769   8.9980281   5.2649277 \n         85          86          87          88          89          90 \n  9.0813768   9.1643859 -18.0084483   1.9018953  -9.7249409   3.5240074 \n         91          92          93          94          95          96 \n -3.2488695   4.5921209  11.5816637  -3.0934674 -13.9898238 -10.8168007 \n         97          98          99         100         101         102 \n  1.8559417   4.2570118   9.7215829   9.5276921   7.1348833  10.0895856 \n        103         104         105         106         107         108 \n  6.1919036   5.4649532   6.8653696  11.9894530   6.0239012 -13.8735532 \n        109         110         111         112         113         114 \n  4.9439324  -0.5856827   2.4052755   8.2010146 -17.5472042  -9.6301791 \n        115         116         117         118         119         120 \n  3.1967583   1.9425134 -11.9967262 -16.1326655   3.0124664  10.3008666 \n        121         122         123         124         125         126 \n -2.6677248 -22.8283427  -0.2548516  -1.7612700  11.9112315   0.5369554 \n        127         128         129         130         131         132 \n -7.7542648   6.2983510  -1.7082204  -1.2204860   9.8382065   6.8222933 \n        133         134         135         136         137         138 \n -8.6967059  -1.2955812  -8.6896144  10.0574246   6.9079504  13.1277383 \n        139         140         141         142 \n 11.9287963   1.6791936 -17.9915824 -16.3779179 \n\nweighted.residuals(this_model) #this_res = \n\n          1           2           3           4           5           6 \n-16.3585885  13.0746657   8.7702301 -19.8911299   7.6121709  -0.2705981 \n          7           8           9          10          11          12 \n -2.7540718  -2.9147296   3.6099346  -1.5913589  -3.7559419   3.5531359 \n         13          14          15          16          17          18 \n 10.5420588 -16.8463317   7.0482188   6.6342522  -8.0460633 -10.2596628 \n         19          20          21          22          23          24 \n -0.9345570 -10.4367387  -2.0528745 -15.2744773 -10.0003672  10.5952492 \n         25          26          27          28          29          30 \n 10.2357286   8.8592184   4.9580414 -13.2804878  -6.5580766  13.0711521 \n         31          32          33          34          35          36 \n-12.2218631   6.8679441  13.0062081   2.3724697  -3.7107349  -6.1014702 \n         37          38          39          40          41          42 \n  8.8303780  11.0491599   8.2163890   8.6626204 -15.7304355  -1.9342885 \n         43          44          45          46          47          48 \n -7.0587859  -1.4100171   1.6778622 -11.2449522  -0.5972526  -0.6564938 \n         49          50          51          52          53          54 \n -0.3895150   2.3716877   7.3891404  -4.1592473 -13.5466984   0.5847459 \n         55          56          57          58          59          60 \n  8.3715872  -2.6677900   2.2982367  -0.8606659   3.5699630   8.8284799 \n         61          62          63          64          65          66 \n  4.0039531  -2.8693162  -6.5967158   4.9175988   2.7776063   8.3369672 \n         67          68          69          70          71          72 \n  2.8681884  10.0898469  -6.3879361   6.7163535   4.1814531 -12.1185481 \n         73          74          75          76          77          78 \n  5.7622523 -17.9735247 -14.1517469   6.7041055  -0.7883088 -11.7464578 \n         79          80          81          82          83          84 \n  6.7419750  -5.7629144   3.4495006   6.2542769   8.9980281   5.2649277 \n         85          86          87          88          89          90 \n  9.0813768   9.1643859 -18.0084483   1.9018953  -9.7249409   3.5240074 \n         91          92          93          94          95          96 \n -3.2488695   4.5921209  11.5816637  -3.0934674 -13.9898238 -10.8168007 \n         97          98          99         100         101         102 \n  1.8559417   4.2570118   9.7215829   9.5276921   7.1348833  10.0895856 \n        103         104         105         106         107         108 \n  6.1919036   5.4649532   6.8653696  11.9894530   6.0239012 -13.8735532 \n        109         110         111         112         113         114 \n  4.9439324  -0.5856827   2.4052755   8.2010146 -17.5472042  -9.6301791 \n        115         116         117         118         119         120 \n  3.1967583   1.9425134 -11.9967262 -16.1326655   3.0124664  10.3008666 \n        121         122         123         124         125         126 \n -2.6677248 -22.8283427  -0.2548516  -1.7612700  11.9112315   0.5369554 \n        127         128         129         130         131         132 \n -7.7542648   6.2983510  -1.7082204  -1.2204860   9.8382065   6.8222933 \n        133         134         135         136         137         138 \n -8.6967059  -1.2955812  -8.6896144  10.0574246   6.9079504  13.1277383 \n        139         140         141         142 \n 11.9287963   1.6791936 -17.9915824 -16.3779179 \n\n\n\\[\nD_i = \\frac{\\sum_{j}{(\\hat{y_j}}-\\hat{y}_{j(i)})^2}{(k+1)*MS_E}\n\\]\n\n# this_infl = lm.influence(this_model, do.coef=FALSE)\n# this_res = weighted.residuals(this_model)\n# this_sd = sqrt(deviance(this_model)/df.residual(this_model))\n# this_hat = this_infl$hat\n# \n# p <- this_model$rank\n# p\n# \n# res_2 <- ((this_res/(this_sd * (1 - this_hat)))^2 * this_hat)/p\n# \n# this_infl$hat\n# this_model$residuals[2:142] - next_model$residuals\n# \n# deviance(this_model)\n# sum(this_model$residuals^2)\n# \n# \n# \n# df.residual(this_model)\n# \n# res <- ((res/(sd * (1 - hat)))^2 * hat)/p\n# res[is.infinite(res)] <- NaN\n#     res\n\n\n# should be 0.00928455311\nsum((this_model$residuals[c(1,3:142)] - lm(lifeExp ~ gdpPercap, gapminder_2007[c(1,3:142),])$residuals)^2)/(2*sqrt(sum(this_model$residuals^2)/140))\n\n[1] 0.08192126\n\n\n\n# glm_example = glm()\n# cooks.distance()\n\n\\[\nD_i = \\frac{\\sum^2_{j=1}{(\\hat{y_j}}-\\hat{y}_{j(i)})^2}{(k+1)*MS_E}\n\\]"
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations (R,Python)",
    "section": "",
    "text": "Please make sure you’ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. We’ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data. Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations (R,Python)",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\n\\(\\sum\\) is saying to add together everything (i.e. the sum of everything within the brackets for this formula)\n\\(x_i\\) refers to each individual’s x-score\n\\(y_i\\) refers to each individual’s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\n\nThe variance for life expectancy can be visualised as the sum of the square of the following:\n\nRPython\n\n\n\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    linewidth  = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\"\n  )\n\n# ggsave(\"life_exp_resid.png\", life_exp_resid)\nlife_exp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add horizontal line for the mean of 'lifeExp'\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\nScatterplot with residuals of ‘Life expectancy’\n\n\n\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\nRPython\n\n\n\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nggsave(\"gdp_resid.png\", gdp_resid)\n\nSaving 7 x 5 in image\n\ngdp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\nScatterplot with residuals of ‘GDP per capita’\n\n\n\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations (R,Python)",
    "section": "Total variance",
    "text": "Total variance\nA correlation captures how much of the total variance is explained by the overlapping variance between the x and y axes. So we first need to capture the total variance. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nOr, to use the figures above:\n\n\n\n\n\n\n\n\n\n$Total$ $Var$ =\nsqrt( \\^2) $$ \\frac{}{N-1} $$\n$*$\nsqrt( \\^ 2) $$ \\frac{}{N-1} $$\n\n\n\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other."
  },
  {
    "objectID": "correlations/correlations.html#shared-variance-between-x-and-y-aka-covariance",
    "href": "correlations/correlations.html#shared-variance-between-x-and-y-aka-covariance",
    "title": "Correlations (R,Python)",
    "section": "Shared variance between \\(x\\) and \\(y\\) (AKA covariance)",
    "text": "Shared variance between \\(x\\) and \\(y\\) (AKA covariance)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\nRPython\n\n\n\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\nSaving 7 x 5 in image\n\nshared_resid\n\n\n\n\n\n\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n# save the plot\nplt.savefig('shared_resid.png')\n\n\n\n\nScatterplot with shared residuals’\n\n\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearson’s R correlation we need to compare the amount that \\(x\\) and \\(y\\) vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\nRPython\n\n\n\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n[1] 0.6786624\n\n\n\n\n\n# Import math Library\nimport math\n\nvarxy = sum(\n    (gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())*\n    (gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean()))\n\n\ntotalvar = math.sqrt(\n   sum((gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())**2)*\n   sum((gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean())**2))\nvarxy/totalvar\n\n\n\n\n0.6786623986777583\nIf the above calculation is correct, we’ll get exactly the same value when using the cor.test function:\n\nRPython\n\n\n\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\n\n\n\nimport scipy.stats\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n(0.6786623986777585, 1.6891897969647515e-20)\n\n\n\nAgain, a reminder of how shared variance could be visualised:\n\n\n\n\n\nA question you might have at this point, is whether the above figure of shared variance seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\nRPython\n\n\n\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\ninverted_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], 125-gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=125-gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=125-gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=125-gapminder_2007[\"lifeExp\"], ymax=125-gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('inverted_resid.png')\n\n\n\n\nScatterplot with inverted shared residuals’\n\n\n\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there’s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like it’s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman’s Rho (AKA Spearman’s Rank) instead:\n\nRPython\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\ndef spssSkewKurtosis(x):\n    import pandas as pd\n    import math\n    \n    w=len(x)\n    m1=x.mean()\n    m2=sum((x-m1)**2)\n    m3=sum((x-m1)**3)\n    m4=sum((x-m1)**4)\n    s1=(x).std()\n    skew=w*m3/(w-1)/(w-2)/s1**3\n    sdskew=math.sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n    kurtosis=(w*(w+1)*m4 - 3*m2**2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1**4)\n    sdkurtosis=math.sqrt( 4*(w**2-1) * sdskew**2 / ((w-3)*(w+5)) )\n    \n    ## z-scores added by reading-psych\n    zskew = skew/sdskew\n    zkurtosis = kurtosis/sdkurtosis\n    \n    mat = pd.DataFrame([[skew, sdskew, zskew],[kurtosis, sdkurtosis, zkurtosis]], columns=['estimate','se','zScore'], index=['skew','kurtosis'])\n    \n    return mat\n  \nspssSkewKurtosis(gapminder_2007[\"gdpPercap\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp\"])\n\n\n\n\nSkewness and kurtosis for ‘gdpPercap’\n\n\n\n\n\nSkewness and kurtosis for ‘lifeExp’\n\n\n\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearman’s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations (R,Python)",
    "section": "Spearman’s Rank (AKA Spearman’s Rho)",
    "text": "Spearman’s Rank (AKA Spearman’s Rho)\nSpearman’s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e. unusually extreme) data points. Let’s now turn the gapminder data we’ve been working with above into ranks and then run a Pearson’s correlation on it to confirm this:\n\nRPython\n\n\n\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\n\n\ngapminder_2007[\"gdpPercap_rank\"] = gapminder_2007[\"gdpPercap\"].rank()\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\n\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\nRPython\n\n\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\n\n\n\nspssSkewKurtosis(gapminder_2007[\"gdpPercap_rank\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp_rank\"])\n\n\n\n\nSkewness and kurtosis for ‘gdpPercap_rank’\n\n\n\n\n\nSkewness and kurtosis for ‘lifeExp_rank’\n\n\n\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e. be at a higher risk of false positives. This is evidence that using a Spearman’s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\nRPython\n\n\n\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\n\n\n\n# Pearson correlation on **ranked** data:\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n# Spearman correlation applied to original data (letting R do the ranking)\nscipy.stats.spearmanr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n(0.8565899189213543, 4.6229745362984015e-42)\n\nSpearmanrResult(correlation=0.8565899189213544, pvalue=4.62297453629821e-42)\n\n\n\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\nRPython\n\n\n\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\nrank_resid\n\n\n\n\n\n\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap_rank\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp_rank\"],xmin=gapminder_2007[\"gdpPercap_rank\"], xmax=gapminder_2007[\"gdpPercap_rank\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp_rank\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap_rank\"],ymin=gapminder_2007[\"lifeExp_rank\"], ymax=gapminder_2007[\"lifeExp_rank\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita (RANK)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy (RANK)\")\n\nred_patch = mpatches.Patch(color='red', label='GDP Residuals')\ngreen_patch = mpatches.Patch(color='green', label='Life Expectancy Residuals')\n\nplt.legend(handles=[red_patch, green_patch],bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# show the plot\nplt.show()\n\n\n\n\nScatterplot for ‘Life Expectancy (RANK)’ and ‘GDP per capita (RANK)’ with residuals’\n\n\n\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\nshared_resid\n\n\n\n\n\n\n\n\n\n\nR values are standardised values\n\n\n\nAs both Pearson’s R and Spearman’s Rank are calculations of the proportion of total variance that can be explained by covariance between variabels, they will always be a value between -1 (all variance is explained for a negative association) to 1 (all variance is explained for a positive association). R values are thus standardised, unlike variance and covariance values which have no limit in their values.\n\n\n\nConsolidation questions\n\nQuestion 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/questionMaker.html",
    "href": "extras/questionMaker.html",
    "title": "Question Maker",
    "section": "",
    "text": "If you have any questions you would like to contribute to any of the consolidation questions, this page will help generate the code needed to include them. Alternatively, you can simply suggest a question at:\nhttps://github.com/Reading-Psych/jast/discussions/categories/suggest-content\nIf you would like to generate the code, please complete the following inputs:\nGive your question a unique name. It cannot have a space in it, so use underscores (e.g. “question_1” rather than “question 1”)\n\nviewof question_name = Inputs.text();\n\n\n\n\n\n\nWrite the question itself here:\n\nviewof question_text = Inputs.text();\n\n\n\n\n\n\nWhat type of question is it?\n\nviewof question_type = Inputs.radio([\n  \"Multiple choice\", \n  \"Numeric\",\n  \"Text\"\n]);\n\n\n\n\n\n\nIf you selected multiple choice, write the options you would like the user to choose from. Please put a | (known as a “pipe”) character between each option\n\nviewof question_responses = Inputs.text();\n\n\n\n\n\n\nWhat is the correct answer? Be precise in how it’s written.\n\nviewof question_correct   = Inputs.text();\n\n\n\n\n\n\n\noutput_string = {\n  var accuracy_code = \"correct_\" + question_name + \" = '\" + question_correct + \"';\\n\" +\n    question_name + \"_result = {\\n\" +\n    \"  if(\" + question_name + \"_response == correct_\" + question_name + \"){\\n\" +\n    \"    return 'Correct!';\\n\" +\n    \"  } else {\\n\" +\n    \"    return 'Incorrect or incomplete.';\\n\" +\n    \"  };\\n\" +\n    \"}\\n\" +\n    \"```\\n\" +\n    \"**${\" + question_name + \"_result}**\";\n  switch(question_type){\n    case \"Multiple choice\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.radio(['\" + question_responses.split(\"|\").join(\"','\") + \"']);\\n\" +\n        accuracy_code;\n      break;\n    case \"Numeric\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.number();\\n\" +\n        accuracy_code;\n      break;\n    case \"Text\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"//| echo: false\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.text();\\n\" +\n        accuracy_code;\n      break;\n  }  \n}\n\n\n\n\n\n\n\nQuestion code:"
  },
  {
    "objectID": "extras/allQuestions.html",
    "href": "extras/allQuestions.html",
    "title": "All questions",
    "section": "",
    "text": "Which is bigger?\n\nviewof scientific_notation_1_response = Inputs.radio(['3.1e3','310']);\ncorrect_scientific_notation_1 = '3.1e';\nscientific_notation_1_result = {\n  if(scientific_notation_1_response == correct_scientific_notation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich is bigger?\n\nviewof scientific_notation_2_response = Inputs.radio(['2.5 * 10^-3',' .00025']);\ncorrect_scientific_notation_2 = '2.5 * 10^-3';\nscientific_notation_2_result = {\n  if(scientific_notation_2_response == correct_scientific_notation_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that you are investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_1_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_1 = '100';\nstats_basics_1_result = {\n  if(stats_basics_1_response == correct_stats_basics_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that you are NOT investigating a real effect, which sample size is more likely to give you a significant result?\n\nviewof stats_basics_2_response = Inputs.radio(['100','200','neither']);\ncorrect_stats_basics_2 = 'neither';\nstats_basics_2_result = {\n  if(stats_basics_2_response == correct_stats_basics_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha values are…\n\nviewof stats_basics_3_response = Inputs.radio(['higher when the p-value is higher','lower when the p-value is lower','neither']);\ncorrect_stats_basics_3 = 'neither';\nstats_basics_3_result = {\n  if(stats_basics_3_response == correct_stats_basics_3){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the likelihood of flipping a (non-biased) coin heads and then tails?\n\nviewof probability_1_response = Inputs.number();\ncorrect_probability_1 = .025;\nprobability_1_result = {\n  if(probability_1_response == correct_probability_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the likelihood of rolling 3 sixes in a row?\n\nviewof probability_2_response = Inputs.radio(['(1/6)*3','(1/6)^3','(1/6)+3']);\ncorrect_probability_2 = '(1/6)^3';\nprobability_2_result = {\n  if(probability_2_response == correct_probability_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#describing-data",
    "href": "extras/allQuestions.html#describing-data",
    "title": "All questions",
    "section": "Describing Data",
    "text": "Describing Data\n\nCentral Tendency\n\nQuestion 1\nWhich of the following is most influenced by outliers?\n\nviewof central_tendency_1_response = Inputs.radio(['Mean','Median','Mode']);\ncorrect_central_tendency_1 = 'Mean';\ncentral_tendency_1_result = {\n  if(central_tendency_1_response == correct_central_tendency_1){\n    return 'Correct! Mode and median are unlikely to be influenced by a single value, whereas an extreme value can drag the mean up or down.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDispersion\n\nQuestion 1\nTrue or False: Using degrees of freedom (N-1) rather than N controls for bias\n\nviewof dispersion_1_response = Inputs.radio(['True','False']);\ncorrect_dispersion_1 = 'True';\ndispersion_1_result = {\n  if(dispersion_1_response == correct_dispersion_1){\n    return 'Correct! Note that bias does not apply to means, but applies to estimates of distribution like variance and SD.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following can be negative?\n\nviewof dispersion_2_response = Inputs.radio(['SD','Variance','Both']);\ncorrect_dispersion_2 = 'SD';\ndispersion_2_result = {\n  if(dispersion_2_response == correct_dispersion_2){\n    return 'Correct! Variance cannot be negative because it is SD^2, and squared values are always positive.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#distributions",
    "href": "extras/allQuestions.html#distributions",
    "title": "All questions",
    "section": "Distributions",
    "text": "Distributions\n\nBinomial\n\nQuestion 1\nDo binomial distributions only work if there is an equal likelihood of either outcome?\n\nviewof binomial_1_response = Inputs.radio(['Yes','No']);\ncorrect_binomial_1 = 'No';\nbinomial_1_result = {\n  if(binomial_1_response == correct_binomial_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat is the likelihood of flipping 2 heads in a row if your coin is .6 biased towards heads\n\nviewof binomial_2_response = Inputs.number();\ncorrect_binomial_2 = '.36';\nbinomial_2_result = {\n  if(binomial_2_response == correct_binomial_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal\n\nQuestion 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof normal_question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nnormal_question_1_result = { \n  if(normal_question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamie’s score into?\n\nnormal_question_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof normal_question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nnormal_question_2_result = { \n  if(normal_question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(normal_question_2_correct == normal_question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\nIf you want to practice with different numbers in these questions then please reload the page.\n\n\n\n\nSkewness\n\nQuestion 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof skewness_question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var skewness_question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(skewness_question_1_response == \"Yes\"){\n      skewness_question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(skewness_question_1_response == \"No\") {\n      skewness_question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return skewness_question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is \n\n\n\nTransforming Data\n\nQuestion 1\nWhich types of transformations might make a distribution normal?\n\nviewof transformation_1_response = Inputs.radio(['linear','non-linear']);\ncorrect_transformation_1 = 'non-linear';\ntransformation_1_result = {\n  if(transformation_1_response == correct_transformation_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete. The pattern of a distribution does not change after linear transformations.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich of the following transformations is least likely to result in a normal distribution?\n\nviewof transformation_2_response = Inputs.radio(['log','square','square-root']);\ncorrect_transformation_2 = 'square';\ntransformation_2_result = {\n  if(transformation_2_response == correct_transformation_2){\n    return 'Correct! Squaring your distribution will exagerate even relative differences between your data points, and thus likely to skew your distribution.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#correlations",
    "href": "extras/allQuestions.html#correlations",
    "title": "All questions",
    "section": "Correlations",
    "text": "Correlations\n\nCorrelations\n\nQuestion 1\nWhich test would be less influenced by skewed data?\n\nviewof correlations_1_response = Inputs.radio([\"Pearson's R\",\"Spearman's Rho/Rank\"]);\ncorrect_correlations_1 = \"Spearman's Rho/Rank\";\ncorrelations_1_result = {\n  if(correlations_1_response == correct_correlations_1){\n    return 'Correct! Ranks are less influenced by outliers which can skew the data than raw data.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nCan r values be greater than 1?\n\nviewof correlations_2_response = Inputs.radio(['Yes, if the association is super strong','No, never ever']);\ncorrect_correlations_2 = 'No';\ncorrelations_2_result = {\n  if(correlations_2_response == correct_correlations_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Correlations\n\nQuestion 1\nWhat does a variable need to correlate with to be a viable candidate as a covariate?\n\nviewof partial_correlations_1_response = Inputs.radio(['Both other variables','At least one other variable','No other variables']);\ncorrect_partial_correlations_1 = 'Both other variables';\npartial_correlations_1_result = {\n  if(partial_correlations_1_response == correct_partial_correlations_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#regressions",
    "href": "extras/allQuestions.html#regressions",
    "title": "All questions",
    "section": "Regressions",
    "text": "Regressions\n\nSimple Regressions\n\nQuestion 1\nIs an r-value a standardised or unstandardised estimate of the association between a predictor and an outcome?\n\nviewof simple_regression_1_response = Inputs.radio(['Standardised','Unstandardised']);\ncorrect_simple_regression_1 = 'Standardised';\nsimple_regression_1_result = {\n  if(simple_regression_1_response == correct_simple_regression_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Regressions"
  },
  {
    "objectID": "extras/allQuestions.html#categorical",
    "href": "extras/allQuestions.html#categorical",
    "title": "All questions",
    "section": "Categorical",
    "text": "Categorical\n\nContingency\n\nQuestion 1\nWill Cramer’s V give the same value as Phi?\n\nviewof contingency_1_response = Inputs.radio(['Yes','No']);\ncorrect_contingency_1 = 'Yes';\ncontingency_1_result = {\n  if(contingency_1_response == correct_contingency_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nChi Square tests tell you…\n\nviewof contingency_2_response = Inputs.radio(['Effect size','Significance']);\ncorrect_contingency_2 = 'Significance';\ncontingency_2_result = {\n  if(contingency_2_response == correct_contingency_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#item-analysis",
    "href": "extras/allQuestions.html#item-analysis",
    "title": "All questions",
    "section": "Item Analysis",
    "text": "Item Analysis\n\nCronbach Alpha\n\nQuestion 1\nCronbach’s Alpha is useful to check whether items in a measure are…\n\nviewof cronbach_alpha_1_response = Inputs.radio(['valid','reliable']);\ncorrect_cronbach_alpha_1 = 'reliable';\ncronbach_alpha_1_result = {\n  if(cronbach_alpha_1_response == correct_cronbach_alpha_1){\n    return 'Correct! Specifically, whether they reliably measure the same construct. However, weird things can happen if multiple similar constructs are captured in the measure, so it can be helpful to conduct Principle Component Analysis first.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nDo you need to reverse code relevant items before conducting Cronbach’s Alpha?\n\nviewof cronbach_alpha_2_response = Inputs.radio(['Yes','No']);\ncorrect_cronbach_alpha_2 = 'Yes';\ncronbach_alpha_2_result = {\n  if(cronbach_alpha_2_response == correct_cronbach_alpha_2){\n    return 'Correct! Otherwise the item will reduce the alpha value even if the item is reliably associated with other items (just going in the opposite direction).';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#multiple-testing",
    "href": "extras/allQuestions.html#multiple-testing",
    "title": "All questions",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nFamily-Wise Error Rate\n\nQuestion 1\nAn alpha value of .05 suggests that 5% of published studies are false-positives?\n\nviewof fwer1_response = Inputs.radio(['True','False']);\ncorrect_fwer1 = 'False';\nfwer_1color = {\n  if(fwer1_response == correct_fwer1){\n    return 'blue';\n  } else {\n    return 'red';\n  }   \n}\nfwer1_result = {\n  if(fwer1_response == correct_fwer1){\n    return 'Correct! It suggests that 5% of studies that investigate effects that do not exist in the population will find them in the sample. However, if no studies are investigating effects that are real in the population then 100% of published studies would be false positives, even though 95% of studies conducted would be correct negatives.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhich correction is more useful for keeping the alpha and FWER rates the same\n\nviewof fwer2_response = Inputs.radio(['Bonferroni','Šidák']);\ncorrect_fwer2 = 'Šidák';\nfwer2_result = {\n  if(fwer2_response == correct_fwer2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse Discovery Rate\n\nQuestion 1\nWhich of the following reflects false discovery rate:\n\nviewof fdr_1_response = Inputs.radio(['False Positives/Negatives in the population','False Positves/Positives in the population','False positives/(False Positives + True Positives)']);\ncorrect_fdr_1 = 'False positives/(False Positives + True Positives)';\nfdr_1_result = {\n  if(fdr_1_response == correct_fdr_1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter ranking all the p-values which of the following is true about the Benjamini-Hochberg procedure:\n\nviewof fdr_2_response = Inputs.radio(['You only accept p-values from the smallest to the largest until the first one that is above the corrected alpha value',\"You accept all p-values that are smaller than the highest p-value below it's respective alpha threshold.\"]);\ncorrect_fdr_2 = \"You accept all p-values that are smaller than the highest p-value below it's respective alpha threshold.\";\nfdr_2_result = {\n  if(fdr_2_response == correct_fdr_2){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFamily-wise Error Rate vs. False Discovery Rate\n\nQuestion 1\nWhat association is there between the proportion of positive effects and FWER?\n\nviewof fwer_vs_fdr1_response = Inputs.radio(['Positive','Neutral','Negative']);\ncorrect_fwer_vs_fdr1 = 'Neutral';\nfwer_vs_fdr1_result = {\n  if(fwer_vs_fdr1_response == correct_fwer_vs_fdr1){\n    return 'Correct!';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nWhat association is there between the proportion of positive effects and FDR?\n\nviewof fwer_vs_fdr2_response = Inputs.radio(['Positive','Neutral','Negative']);\ncorrect_fwer_vs_fdr2 = 'Negative';\nfwer_vs_fdr2_result = {\n  if(fwer_vs_fdr2_response == correct_fwer_vs_fdr2){\n    return 'Correct! There should be a lower rate of false positives compared to false positives and true positives in a study that is investigating mostly positive effects compared to a study that is investigating mostly negative effects.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "extras/allQuestions.html#permutations",
    "href": "extras/allQuestions.html#permutations",
    "title": "All questions",
    "section": "Permutations",
    "text": "Permutations\n\nQuestion 1\nPermutation analysis can only be used on non-parametric data\n\nviewof permutatations_1_response = Inputs.radio(['True','False']);\ncorrect_permutatations_1 = 'False';\npermutatations_1_result = {\n  if(permutatations_1_response == correct_permutatations_1){\n    return 'Correct! Whilst it is useful for analysis of non-parametric data, it is not restricted to use in these contexts. It would be valid to double check the conclusion of your parametric analysis, or for alternative analyses such compare how significant the difference is between two correlations.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nPermutation analysis can only be done when comparing 2 groups?\n\nviewof permutatations_2_response = Inputs.radio(['True','False']);\ncorrect_permutatations_2 = 'False';\npermutatations_2_result = {\n  if(permutatations_2_response == correct_permutatations_2){\n    return 'Correct! It can be used on any analysis that generates a single output value (even if that single output value is a combination of other values). F-values from ANOVAs are an example of how you can compare between more than 2 groups. You can also compare between conditions within participants, but you would need to be mindful that you only permute swaps within participants for within-subject factors.';\n  } else {\n    return 'Incorrect or incomplete.';\n  };\n}"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#section",
    "href": "GeneralLinearModels/TTests.html#section",
    "title": "T-Tests (R, Python incomplete)",
    "section": "",
    "text": "F-value approach (AKA General Linear Model (GLM) approach)\n\n\n\n\n\n\n\n\n\n\n\ninternal callout\n\n\n\n\n\nbeep bop\n\n\n\nTo calculate the F-Value, we want to create a model that captures explained variance and unexplained variance. This model will aim to explain any variance around the population mean (mu or \\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\n\\(y\\) is the data point value you are trying to predict.\n\\(\\bar{y}\\) is mean of all y data points. Note that for this formula you will always have the same predicted outcome (the mean).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the Mu (\\(\\mu\\)) and will also suggested that there is significant reason to reject the \\(\\mu\\) as the real mean of your population.\n\n\n\n\n\n\nFirst, we need to capture how much variance there is around the \\(\\mu\\), which we’ll do using sum of squares:\n\\[\nSS_{total} = \\sum(y_i-\\mu)^2\n\\]\n\n\\(y_i\\) is an individual data point\n\n\n\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - 55)^2)\n\n\n[1] 41025.16\n\n\n\n\n\n\nCode\n# import numpy \nimport numpy as np\n\n# calculate the squared dfference\nnp.sum((gapminder_2007['lifeExp'] - 55) ** 2)\n\n\n41025.157014\n\n\n\nSo we have to explain 41025.16 in variance around the \\(\\mu\\).\nOur model’s explanation of variance around the \\(\\mu\\) is the sample mean (\\(\\bar{y}\\)). For each data point the variance explained (from the \\(mu\\)) is the difference between \\(\\mu\\) and \\(\\bar{y}\\), and then that leaves the unexplained variance (from the \\(mu\\)) as the difference between the data point and the sample mean (\\(\\bar{y}\\)). If we visualise this for a single data point it should look something like:\n\nFor the above data point the variance (around the \\(\\mu\\)) explained is \\(\\bar{y} - \\mu = 67.007 - 55 = 22.007\\). However, we can see that the model isn’t perfect for this data point, and so there is a residual left over, i.e. a difference between the predicted value of 67.007 and the actual value of 58.04, meaning the unexplained variance is \\(67.007 - 58.04 = 8.967\\).\nYour explained variance by this model is thus a repetition of comparisons of the \\(\\mu\\) and \\(\\bar{y}\\) as many times as there are data points (and squaring this difference to make the values positive):\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n\n[1] 20473.3\n\n\n\n\n\n\nCode\nlen(gapminder_2007['lifeExp']) *( 55 - gapminder_2007['lifeExp'].mean())**2\n\n\n20473.303823352093\n\n\n\nAs described above, unexplained variance is the residuals around the sample mean, as this is variance that is not explained by the model. We can summarise what we get when we calculate the sum of squares of these residuals around the sample mean as follows:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n\n[1] 20551.85\n\n\n\n\n\n\nCode\nnp.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n\n20551.853190647882\n\n\n\nTo capture the effectiveness of the model we can calculate the F-value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nRPython\n\n\n\n\nCode\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n\n[1] 140.4611\n\n\n\n\n\n\nCode\n# Calculate the sum of squared differences between each value in 'lifeExp' and the mean\nss_between = len(gapminder_2007['lifeExp']) * (55 - gapminder_2007['lifeExp'].mean()) ** 2\n\n# Calculate the sum of squared differences within groups\nss_within = np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n# Calculate the degrees of freedom for between groups and within groups\ndf_between = 1\ndf_within = len(gapminder_2007['lifeExp']) - 1\n\n# Calculate the F-statistic\nf_value = (ss_between / df_between) / (ss_within / df_within)\n\nprint(f_value)\n\n\n140.46109673488004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANVOA vs. T-test formula\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} =\n\\]\n\\[\n\\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\n\\(T\\) is the t-value\n\\(F\\) is the f-value\n\\(SS_{exp}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexp}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{exp}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels).\n\nTo confirm, the formula for a one-sample t-test is just:\n\\[\nT = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\]\nF-values are squares of t-values, so let’s see if this is true here also:\n\nRPython\n\n\n\n\nCode\nsqrt(f_value)\n\n\n[1] 11.85163\n\n\nCode\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Calculate the square root of the F-value\nnp.sqrt(f_value)\n\n\n11.851628442323022\n\n\nCode\n# Perform a t-test\nt_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 11.851628442323024\np-value: 6.463174215427706e-23"
  },
  {
    "objectID": "GeneralLinearModels/mixedAnova.html#x-2-anova",
    "href": "GeneralLinearModels/mixedAnova.html#x-2-anova",
    "title": "Mixed ANOVA (incomplete)",
    "section": "2 x 2 ANOVA",
    "text": "2 x 2 ANOVA\nLet’s create data to allow us to compare between 2 years, and between Europe and Americas\n\nRPython\n\n\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 <- subset(\n  gapminder,   # the data set\n  year == 2002 & continent == \"Africa\" |\n  year == 2007 & continent == \"Africa\" |\n  year == 2002 & continent == \"Europe\" |\n  year == 2007 & continent == \"Europe\"\n)\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7759  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 = gapminder.loc[(gapminder['year'] == 2002) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2002) & (gapminder['continent'] == \"Europe\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Europe\") ]\n\nfrom statsmodels.formula.api import ols\n\nfit = ols('lifeExp ~ C(year) + C(continent)', data=gapminder_2_by_2).fit() \n\nfit.summary()\n\nlm_4_continents_aov_table = sm.stats.anova_lm(fit, typ=2)\nlm_4_continents_aov_table\n\n\n\n\nmanual calculation of f-value for 2 x 2\n\nRPython\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\noverallMeanLifeExp = mean(gapminder_2_by_2$lifeExp)\ntotalVar = sum((gapminder_2_by_2$lifeExp - mean(gapminder_2_by_2$lifeExp))^2)\ngapminder_2_by_2 %>%\n  group_by(continent, year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> year_continent_means\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nyear_continent_means\n\n# A tibble: 4 × 5\n# Groups:   continent [2]\n  continent  year mean_lifeExp countries betweenSS\n  <fct>     <int>        <dbl>     <int>     <dbl>\n1 Africa     2002         53.3        52     4396.\n2 Africa     2007         54.8        52     3094.\n3 Europe     2002         76.7        30     6033.\n4 Europe     2007         77.6        30     6866.\n\nsum(year_continent_means$betweenSS)\n\n[1] 20389.47\n\ntotalVar\n\n[1] 30311.89\n\nsum(year_continent_means$betweenSS)/totalVar\n\n[1] 0.6726556\n\n(sum(year_continent_means$betweenSS))/totalVar\n\n[1] 0.6726556\n\ndf_total <- length(gapminder_2_by_2$country) - 1\ndf_res <- length(gapminder_2_by_2$country) - \n  1 - #data points\n  3   # predictors\n\nss_res = totalVar - sum(year_continent_means$betweenSS)\n\n1 - (ss_res/df_res)/(totalVar/df_total)\n\n[1] 0.6665179\n\n##\n# break down of types of variance\n##\ncontinent_df <- gapminder_2_by_2 %>%\n  group_by(continent) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(continent_df$betweenSS)\n\n[1] 20318.97\n\nyear_df <- gapminder_2_by_2 %>%\n  group_by(year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(year_df$betweenSS)\n\n[1] 67.79278\n\n##\n# interaction\n##\nsum(year_continent_means$betweenSS) - sum(continent_df$betweenSS) - sum(year_df$betweenSS)\n\n[1] 2.70036\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n20319 +\n68 +\n3\n\n[1] 20390\n\n\n\n\n\nfrom siuba import group_by, summarize, _\n\noverallMeanLifeExp = gapminder_2_by_2['lifeExp'].mean()\n\ntotalVar = sum((gapminder_2_by_2['lifeExp'] - gapminder_2_by_2['lifeExp'].mean())**2)\n\n\nyear_continent_means=(gapminder_2_by_2\n  >> group_by(_.continent, _.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_continent_means['betweenSS'] = year_continent_means['countries'] * ((overallMeanLifeExp - year_continent_means['mean_lifeExp'])**2)\n\nprint(tabulate(year_continent_means, headers=year_continent_means.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nsum(year_continent_means['betweenSS'])\n\ntotalVar\n\nsum(year_continent_means['betweenSS'])/totalVar\n\ndf_total = len(gapminder_2_by_2['country']) - 1\ndf_res = len(gapminder_2_by_2['country']) - 1 - 3\nss_res = totalVar - sum(year_continent_means['betweenSS'])\n1 - (ss_res/df_res)/(totalVar/df_total)\n\ncontinent_df=(gapminder_2_by_2\n  >> group_by(_.continent)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\ncontinent_df['betweenSS'] = continent_df['countries'] * ((overallMeanLifeExp - continent_df['mean_lifeExp'])**2)\nsum(continent_df['betweenSS'])\n\nyear_df=(gapminder_2_by_2\n  >> group_by(_.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_df['betweenSS'] = year_df['countries'] * ((overallMeanLifeExp - year_df['mean_lifeExp'])**2)\nsum(year_df['betweenSS'])\n\nsum(year_continent_means['betweenSS']) - sum(continent_df['betweenSS']) - sum(year_df['betweenSS'])\n\nprint(tabulate(gapminder[:10], headers=gapminder.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n20319 +68 +3"
  },
  {
    "objectID": "GeneralLinearModels/repeatedAnova.html",
    "href": "GeneralLinearModels/repeatedAnova.html",
    "title": "Repeated Measures ANOVAs",
    "section": "",
    "text": "Course Overview\n\n\n\n\n\n\n\n\n\n\n\nRed means that the page does not exist yet\nOrange means that the page is started"
  },
  {
    "objectID": "GeneralLinearModels/ancova.html#tbc",
    "href": "GeneralLinearModels/ancova.html#tbc",
    "title": "ANCOVA (incomplete)",
    "section": "TBC",
    "text": "TBC"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#t-tests-use-the-same-underlying-mathematics-as-anovas.-there-is-an-argument-that-its-better-to-just-use-the-anova-mathematics-as-this-will-allow-you-more-flexibility-in-future-to-do-more-complex-analyses",
    "href": "GeneralLinearModels/TTests.html#t-tests-use-the-same-underlying-mathematics-as-anovas.-there-is-an-argument-that-its-better-to-just-use-the-anova-mathematics-as-this-will-allow-you-more-flexibility-in-future-to-do-more-complex-analyses",
    "title": "T-Tests (R, Python incomplete)",
    "section": "T-tests use the same underlying mathematics as ANOVAs. There is an argument that it’s better to just use the ANOVA mathematics as this will allow you more flexibility in future to do more complex analyses",
    "text": "T-tests use the same underlying mathematics as ANOVAs. There is an argument that it’s better to just use the ANOVA mathematics as this will allow you more flexibility in future to do more complex analyses\nIn the tests below we will frequently come across the following concepts:\nTotal sum of squares represents all the variance around the mean. We create models to try to explain why data points are different to the mean.\nExplained sum of squares represents the variance around the mean that is explained by your model.\nUnexplained variance represents whatever variance around the mean that isn’t explained by your model. Sometimes the word residual or error is used to capture the difference between the predicted value of a data point (i.e. what would be explained by the model) and the actual data point (the difference being unexplained).\nWe summarise this ratio of explained vs. unexplained variance using an F value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}\n\\]\n\n\\(SS_{explained}\\) is explained sum of squares\n\\(SS_{unexplained}\\) is the unexplained sum of squares\n\\(df_{explained}\\) is the degrees of freedom (df) for the model explaining the variance. This is generally the number of levels in your model minus 1. For example, if you were testing if life expectancy is explained by which continent a country is in, then the number of levels is the number of continents in your analysis (e.g. 5 continents), and the df would be 5-1=4. The df is used to address uncertainty because we are using a sample to estimate a population, but you don’t need to understand this to continue on this page.\n\\(df_{unexplained}\\) is the degrees of freedom (df) that is not addressed by the model. Generally this is the number of subjects minus 1. Using the above example about continents and life expectancy, each subject could be one of 142 countries in your analysis, and so the df would be 142-1=141.\n\nThe key points in the above formula are:\n\nThe better your model is the more Sum of Squares will be explained, which will increase F\nThe more data points (e.g. subjects) you have, the higher your \\(df_{unexplained}\\) will be, which will also increase F\n\nOn this page we will be focusing on simpler analyses that only compare between 2 conditions, which means \\(df_{explained}\\) will be (2-1=) 1, which simplifies the above formula:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{SS_{explained}/1}{SS_{unexplained}/df_{unexplained}} = \\frac{SS_{explained}}{SS_{unexplained}/df_{unexplained}}\n\\]\nThis allows us to complete simpler analyses to calculate \\(t\\) values, which are the square root of \\(F\\):\n\\[\nt = \\sqrt{F} = \\sqrt{\\frac{SS_{explained}}{SS_{unexplained}/df_{unexplained}}}\n\\]\n\\(t\\) values are used for \\(t\\)-tests, which we’ll address below. :::"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#section-1",
    "href": "GeneralLinearModels/TTests.html#section-1",
    "title": "T-Tests (R, Python incomplete)",
    "section": "",
    "text": "F-value approach (AKA General Linear Model (GLM) approach)\n\n\n\n\n\n\n\n\n\n\n\ninternal callout\n\n\n\n\n\nbeep bop\n\n\n\nTo calculate the F-Value, we want to create a model that captures explained variance and unexplained variance. This model will aim to explain any variance around the population mean (mu or \\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\n\\(y\\) is the data point value you are trying to predict.\n\\(\\bar{y}\\) is mean of all y data points. Note that for this formula you will always have the same predicted outcome (the mean).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the Mu (\\(\\mu\\)) and will also suggested that there is significant reason to reject the \\(\\mu\\) as the real mean of your population.\n\n\n\n\n\n\nFirst, we need to capture how much variance there is around the \\(\\mu\\), which we’ll do using sum of squares:\n\\[\nSS_{total} = \\sum(y_i-\\mu)^2\n\\]\n\n\\(y_i\\) is an individual data point\n\n\n\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - 55)^2)\n\n\n[1] 41025.16\n\n\n\n\n\n\nCode\n# import numpy \nimport numpy as np\n\n# calculate the squared dfference\nnp.sum((gapminder_2007['lifeExp'] - 55) ** 2)\n\n\n41025.157014\n\n\n\nSo we have to explain 41025.16 in variance around the \\(\\mu\\).\nOur model’s explanation of variance around the \\(\\mu\\) is the sample mean (\\(\\bar{y}\\)). For each data point the variance explained (from the \\(mu\\)) is the difference between \\(\\mu\\) and \\(\\bar{y}\\), and then that leaves the unexplained variance (from the \\(mu\\)) as the difference between the data point and the sample mean (\\(\\bar{y}\\)). If we visualise this for a single data point it should look something like:\n\nFor the above data point the variance (around the \\(\\mu\\)) explained is \\(\\bar{y} - \\mu = 67.007 - 55 = 22.007\\). However, we can see that the model isn’t perfect for this data point, and so there is a residual left over, i.e. a difference between the predicted value of 67.007 and the actual value of 58.04, meaning the unexplained variance is \\(67.007 - 58.04 = 8.967\\).\nYour explained variance by this model is thus a repetition of comparisons of the \\(\\mu\\) and \\(\\bar{y}\\) as many times as there are data points (and squaring this difference to make the values positive):\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n\n[1] 20473.3\n\n\n\n\n\n\nCode\nlen(gapminder_2007['lifeExp']) *( 55 - gapminder_2007['lifeExp'].mean())**2\n\n\n20473.303823352093\n\n\n\nAs described above, unexplained variance is the residuals around the sample mean, as this is variance that is not explained by the model. We can summarise what we get when we calculate the sum of squares of these residuals around the sample mean as follows:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n\n[1] 20551.85\n\n\n\n\n\n\nCode\nnp.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n\n20551.853190647882\n\n\n\nTo capture the effectiveness of the model we can calculate the F-value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nRPython\n\n\n\n\nCode\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n\n[1] 140.4611\n\n\n\n\n\n\nCode\n# Calculate the sum of squared differences between each value in 'lifeExp' and the mean\nss_between = len(gapminder_2007['lifeExp']) * (55 - gapminder_2007['lifeExp'].mean()) ** 2\n\n# Calculate the sum of squared differences within groups\nss_within = np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n# Calculate the degrees of freedom for between groups and within groups\ndf_between = 1\ndf_within = len(gapminder_2007['lifeExp']) - 1\n\n# Calculate the F-statistic\nf_value = (ss_between / df_between) / (ss_within / df_within)\n\nprint(f_value)\n\n\n140.46109673488004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANVOA vs. T-test formula\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} =\n\\]\n\\[\n\\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\n\\(T\\) is the t-value\n\\(F\\) is the f-value\n\\(SS_{exp}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexp}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{exp}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels).\n\nTo confirm, the formula for a one-sample t-test is just:\n\\[\nT = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\]\nF-values are squares of t-values, so let’s see if this is true here also:\n\nRPython\n\n\n\n\nCode\nsqrt(f_value)\n\n\n[1] 11.85163\n\n\nCode\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Calculate the square root of the F-value\nnp.sqrt(f_value)\n\n\n11.851628442323022\n\n\nCode\n# Perform a t-test\nt_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 11.851628442323024\np-value: 6.463174215427706e-23"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#t-values-calculating-p-values-from-t-distributions",
    "href": "GeneralLinearModels/TTests.html#t-values-calculating-p-values-from-t-distributions",
    "title": "T-Tests (R, Python incomplete)",
    "section": "T-values Calculating p-values from t-distributions",
    "text": "T-values Calculating p-values from t-distributions\n\n\n\n\n\n\nT-values can be used to calculate p-values\n\n\n\n\n\nSimilar to the normal distribution, we can calculate how likely it is that you would get a t-value or higher (or lower) by drawing a t-distribution. A t-distribution is like a normal distribution (it’s a bell curve), but also takes into account the number of participants (or data points) using degrees of freedom (N-1). Here are the t-distributions you would get for 101, 21 and 6 participants:\n\nRPython\n\n\n\n\nCode\ncurve(dt(x, df=100), from=-4, to=4, col='green')\ncurve(dt(x, df=20), from=-4, to=4, col='red', add=TRUE)\ncurve(dt(x, df=5), from=-4, to=4, col='blue', add = TRUE) \n\n#add legend\nlegend(-4, .3, legend=c(\"df=5\", \"df=20\", \"df=100\"),\n       col=c(\"blue\", \"red\", \"green\"), lty=1, cex=1.2)\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t\n\n# Define the x values\nx = np.linspace(-4, 4, 1000)\n\n# Create density curves for different degrees of freedom\nplt.plot(x, t.pdf(x, df=100), color='green', label='df=100')\nplt.plot(x, t.pdf(x, df=20), color='red', label='df=20')\nplt.plot(x, t.pdf(x, df=5), color='blue', label='df=5')\n\n# Add a legend\nplt.legend(loc='upper left')\n\n# Set axis labels and plot title\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('T-Distribution Density Curves')\n\n# Show the plot\nplt.show()\n\n\n\n\n\nT-Distribution Density Curves’\n\n\n\n\n\nWe can see that the slopes get steeper the higher the \\(df\\) is (i.e. the more participants you have). This means that the results become more significant even with the same t-value. Not convinced? Let’s use a t-value of 2 to illustrate this. In the following figure you can see that the area under the curve for a t-value of 2 or more gets visually smaller the more participants/the higher the \\(df\\) value is:\n\nRPython\n\n\n\n\nCode\nggplot(data.frame(x = c(-4, 4)), aes(x)) +\n  stat_function(fun = dt,   args =list(df =5), xlim = c(2,4), geom = \"area\") + \n  stat_function(fun = dt, args =list(df =5), color = \"blue\") +\n  stat_function(fun = dt, args =list(df =20), color = \"red\") +\n  stat_function(fun = dt, args =list(df =100), color = \"green\") \n\n\n\n\n\n\n\n\n\nCode\nfrom plotnine import ggplot, aes, stat_function, xlim, geom_area, geom_line\nfrom scipy.stats import t\n\nimport pandas as pd\n\n# Define the data frame with x values\ndf = pd.DataFrame({'x': [-4, 4]})\n\n# Create the  plot\n(\n    ggplot(df, aes(x='x'))\n    + stat_function(fun=lambda x: t.pdf(x, df=5), geom=\"area\", xlim=(2, 4), fill=\"blue\")\n    + stat_function(fun=lambda x: t.pdf(x, df=5), color=\"blue\")\n    + stat_function(fun=lambda x: t.pdf(x, df=20), color=\"red\")\n    + stat_function(fun=lambda x: t.pdf(x, df=100), color=\"green\")\n)\n\n\n\n\n\nFigure with the area under the curve for a t-value of 2’\n\n\n\n\n\nRemember, area under the curve IS the p-value, so the area under the curve for a t-value of 2 or above for each degrees of freedom is:\n\nRPython\n\n\n\n\nCode\n# area under the curve for 2 and above\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=5, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.05096974\n\n\nCode\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=20, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.02963277\n\n\nCode\npt(\n  # t-value\n  q=2, \n  # degrees of freedom\n  df=100, \n  # is the test of the t-value and below (TRUE) or the t-value and above (FALSE)\n  lower.tail = FALSE\n)\n\n\n[1] 0.02410609\n\n\n\n\n\n\nCode\nfrom scipy.stats import t\n\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 5\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.050969739414929105\n\n\nCode\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 20\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.02963276772328527\n\n\nCode\n# t-value\nt_value = 2\n\n# Degrees of freedom\ndf = 100\n\n# Calculate the area under the curve for t >= 2\narea_above_2 = 1 - t.cdf(t_value, df=df)\narea_above_2\n\n\n0.024106089365566796\n\n\n\nNote that if you wanted a 2-tailed test (i.e. you didn’t have an expected direction of your finding) you would double the area under the curve/p-value.\nSimilar principles apply for an F-distribution, described next."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#t-tests-and-anovas",
    "href": "GeneralLinearModels/TTests.html#t-tests-and-anovas",
    "title": "T-Tests (R, Python incomplete)",
    "section": "T-tests and ANOVAs",
    "text": "T-tests and ANOVAs\nT-tests use the same underlying mathematics as ANOVAs. There is an argument that it’s better to just use the ANOVA mathematics as this will allow you more flexibility in future to do more complex analyses. Expand this for insights into the ANOVA concepts and how they overlap with t-tests.\n\n\n\n\n\n\nExpand this for key terminology for ANOVAs.\n\n\n\n\n\nIn the tests below we will frequently come across the following concepts:\nTotal sum of squares represents all the variance around the mean. We create models to try to explain why data points are different to the mean.\nExplained sum of squares represents the variance around the mean that is explained by your model.\nUnexplained variance represents whatever variance around the mean that isn’t explained by your model. Sometimes the word residual or error is used to capture the difference between the predicted value of a data point (i.e. what would be explained by the model) and the actual data point (the difference being unexplained).\nWe summarise this ratio of explained vs. unexplained variance using an F value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}\n\\]\n\n\\(SS_{explained}\\) is explained sum of squares\n\\(SS_{unexplained}\\) is the unexplained sum of squares\n\\(df_{explained}\\) is the degrees of freedom (df) for the model explaining the variance. This is generally the number of levels in your model minus 1. For example, if you were testing if life expectancy is explained by which continent a country is in, then the number of levels is the number of continents in your analysis (e.g. 5 continents), and the df would be 5-1=4. The df is used to address uncertainty because we are using a sample to estimate a population, but you don’t need to understand this to continue on this page.\n\\(df_{unexplained}\\) is the degrees of freedom (df) that is not addressed by the model. Generally this is the number of subjects minus 1. Using the above example about continents and life expectancy, each subject could be one of 142 countries in your analysis, and so the df would be 142-1=141.\n\nThe key points in the above formula are:\n\nThe better your model is the more Sum of Squares will be explained, which will increase F\nThe more data points (e.g. subjects) you have, the higher your \\(df_{unexplained}\\) will be, which will also increase F\n\n\n\n\n\n\n\n\n\n\nHow F-values for a test with 2 levels can be simplified to calculate a t-value\n\n\n\n\n\nOn this page we will be focusing on simpler analyses that only compare between 2 conditions, which means \\(df_{explained}\\) will be (2-1=) 1, which simplifies the above formula:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{SS_{explained}/1}{SS_{unexplained}/df_{unexplained}} = \\frac{SS_{explained}}{SS_{unexplained}/df_{unexplained}}\n\\]\nThis allows us to complete simpler analyses to calculate \\(t\\) values, which are the square root of \\(F\\):\n\\[\nt = \\sqrt{F} = \\sqrt{\\frac{SS_{explained}}{SS_{unexplained}/df_{unexplained}}}\n\\]\n\n\n\n\\(t\\) values are used for \\(t\\)-tests, which we’ll address below."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#section-2",
    "href": "GeneralLinearModels/TTests.html#section-2",
    "title": "T-Tests (R, Python incomplete)",
    "section": "",
    "text": "F-value approach (AKA General Linear Model (GLM) approach)\n\n\n\n\n\n\n\n\n\n\n\ninternal callout\n\n\n\n\n\nbeep bop\n\n\n\nTo calculate the F-Value, we want to create a model that captures explained variance and unexplained variance. This model will aim to explain any variance around the population mean (mu or \\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\n\\(y\\) is the data point value you are trying to predict.\n\\(\\bar{y}\\) is mean of all y data points. Note that for this formula you will always have the same predicted outcome (the mean).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the Mu (\\(\\mu\\)) and will also suggested that there is significant reason to reject the \\(\\mu\\) as the real mean of your population.\n\n\n\n\n\n\nFirst, we need to capture how much variance there is around the \\(\\mu\\), which we’ll do using sum of squares:\n\\[\nSS_{total} = \\sum(y_i-\\mu)^2\n\\]\n\n\\(y_i\\) is an individual data point\n\n\n\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - 55)^2)\n\n\n[1] 41025.16\n\n\n\n\n\n\nCode\n# import numpy \nimport numpy as np\n\n# calculate the squared dfference\nnp.sum((gapminder_2007['lifeExp'] - 55) ** 2)\n\n\n41025.157014\n\n\n\nSo we have to explain 41025.16 in variance around the \\(\\mu\\).\nOur model’s explanation of variance around the \\(\\mu\\) is the sample mean (\\(\\bar{y}\\)). For each data point the variance explained (from the \\(mu\\)) is the difference between \\(\\mu\\) and \\(\\bar{y}\\), and then that leaves the unexplained variance (from the \\(mu\\)) as the difference between the data point and the sample mean (\\(\\bar{y}\\)). If we visualise this for a single data point it should look something like:\n\nFor the above data point the variance (around the \\(\\mu\\)) explained is \\(\\bar{y} - \\mu = 67.007 - 55 = 22.007\\). However, we can see that the model isn’t perfect for this data point, and so there is a residual left over, i.e. a difference between the predicted value of 67.007 and the actual value of 58.04, meaning the unexplained variance is \\(67.007 - 58.04 = 8.967\\).\nYour explained variance by this model is thus a repetition of comparisons of the \\(\\mu\\) and \\(\\bar{y}\\) as many times as there are data points (and squaring this difference to make the values positive):\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n\n[1] 20473.3\n\n\n\n\n\n\nCode\nlen(gapminder_2007['lifeExp']) *( 55 - gapminder_2007['lifeExp'].mean())**2\n\n\n20473.303823352093\n\n\n\nAs described above, unexplained variance is the residuals around the sample mean, as this is variance that is not explained by the model. We can summarise what we get when we calculate the sum of squares of these residuals around the sample mean as follows:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nRPython\n\n\n\n\nCode\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n\n[1] 20551.85\n\n\n\n\n\n\nCode\nnp.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n\n20551.853190647882\n\n\n\nTo capture the effectiveness of the model we can calculate the F-value:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nRPython\n\n\n\n\nCode\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n\n[1] 140.4611\n\n\n\n\n\n\nCode\n# Calculate the sum of squared differences between each value in 'lifeExp' and the mean\nss_between = len(gapminder_2007['lifeExp']) * (55 - gapminder_2007['lifeExp'].mean()) ** 2\n\n# Calculate the sum of squared differences within groups\nss_within = np.sum((gapminder_2007['lifeExp'] - gapminder_2007['lifeExp'].mean()) ** 2)\n\n# Calculate the degrees of freedom for between groups and within groups\ndf_between = 1\ndf_within = len(gapminder_2007['lifeExp']) - 1\n\n# Calculate the F-statistic\nf_value = (ss_between / df_between) / (ss_within / df_within)\n\nprint(f_value)\n\n\n140.46109673488004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANVOA vs. T-test formula\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{exp}/df_{exp}}{SS_{unexp}/df_{unexp}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} =\n\\]\n\\[\n\\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\n\\(T\\) is the t-value\n\\(F\\) is the f-value\n\\(SS_{exp}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexp}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{exp}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels).\n\nTo confirm, the formula for a one-sample t-test is just:\n\\[\nT = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\]\nF-values are squares of t-values, so let’s see if this is true here also:\n\nRPython\n\n\n\n\nCode\nsqrt(f_value)\n\n\n[1] 11.85163\n\n\nCode\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Calculate the square root of the F-value\nnp.sqrt(f_value)\n\n\n11.851628442323022\n\n\nCode\n# Perform a t-test\nt_statistic, p_value = stats.ttest_1samp(gapminder_2007['lifeExp'], popmean=55)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n\nt-statistic: 11.851628442323024\np-value: 6.463174215427706e-23"
  }
]