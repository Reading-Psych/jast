[
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency",
    "section": "",
    "text": "Central tendancy describes typical values of a variable, such as it’s mean and median.\n\nMean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\nLoading required package: gapminder\n\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n  \n\n\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# import the tabulate\nfrom tabulate import tabulate\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n#display table\nprint(tabulate(gapminder_2007[:6], headers=gapminder_2007.head() , tablefmt=\"fancy_grid\",showindex=False ))\n\n# total of all years\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n\n\n\n\n\nTable\n\n\n67.00742253521126\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n\n\n\n67.00742253521126\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\ngapminder_2007['lifeExp'].median()\n\n\n\n\n67.00742253521126\n71.93549999999999\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\ngapminder_2007['lifeExp'].mode()\n\n\n\n\n\n\n\nMode of ‘lifeExp’\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\nlength(gapminder_2007$lifeExp)\n\n[1] 142\n\nlength(unique(gapminder_2007$lifeExp))\n\n[1] 142\n\n\n\n\n\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n\n\n\n142\n142\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n[1] 2 4\n\n\n\n\n\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n\n\n\n\n\nExample of Mode\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most."
  },
  {
    "objectID": "describingData/dispersion.html",
    "href": "describingData/dispersion.html",
    "title": "Dispersion",
    "section": "",
    "text": "To understand distribution (later), it’s helpful to clarify some more basic concepts around how data is dispersed or spread."
  },
  {
    "objectID": "describingData/dispersion.html#range",
    "href": "describingData/dispersion.html#range",
    "title": "Dispersion",
    "section": "Range",
    "text": "Range\nRange simply captures the min and the maximum values. Lets look at the min and max for the life expectancy data from 2007:\n\nRPython\n\n\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmin(gapminder_2007$lifeExp)\n\n[1] 39.613\n\nmax(gapminder_2007$lifeExp)\n\n[1] 82.603\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].min()\ngapminder_2007['lifeExp'].max()\n\n\n\n\n39.613\n\n82.603\nSo the range for life expectancy in 2007 was between 39.613 and 82.603."
  },
  {
    "objectID": "describingData/dispersion.html#variance",
    "href": "describingData/dispersion.html#variance",
    "title": "Dispersion",
    "section": "Variance",
    "text": "Variance\n\nPopulation Variance\nVariance is how much the data varies around a mean. To capture this, we compare each individual’s score with the mean, so lets do this with our gapminder data’s life expectancy:\n\nRPython\n\n\n\nlife_expectancy_variance_table <- data.frame(\n  life_expectancy = gapminder_2007$lifeExp,\n  diff_from_mean  = gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp)\n)\n\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nimport pandas as pd\nfrom tabulate import tabulate\n\nlife_expectancy_variance_table = {\n  'life_expectancy' : gapminder_2007['lifeExp'],\n  'diff_from_mean': gapminder_2007['lifeExp']- gapminder_2007['lifeExp'].mean(),\n}\n\n# convert it to a data frame\nlife_expectancy_variance_table = pd.DataFrame(life_expectancy_variance_table)\n\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\nSo we know for each country how different their life expectacy is to the mean life expectancy. But ideally we would like a single value to summarise variance. Lets see what would happen if we tried to summarise these differences from the mean by calculating the mean difference from the mean:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_from_mean) \n\n[1] 5.153937e-15\n\n\n\n\n\nlife_expectancy_variance_table['diff_from_mean'].mean()\n\n\n\n\n5.254013186958487e-15\nWe get a number that is effectively zero (go here for an explanation about e-numbers), because all the values above the mean balance out those below the mean. So to address this, we can square the differences to force all the numbers to be positive:\n\nRPython\n\n\n\nlife_expectancy_variance_table$diff_squared = life_expectancy_variance_table$diff_from_mean^2\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'] = life_expectancy_variance_table['diff_from_mean'].pow(2)\n# print the table\nprint(tabulate(life_expectancy_variance_table[:10], headers=life_expectancy_variance_table.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\nIf we calculate the average of this, then we get a summary of the variance that is more informative:\n\nRPython\n\n\n\nmean(life_expectancy_variance_table$diff_squared)\n\n[1] 144.7314\n\n\n\n\n\nlife_expectancy_variance_table['diff_squared'].mean()\n\n\n\n\n144.73136049752028\nHowever, as mean is what you get when you add all the items together and then divide it by the number of items, this can also be done in 2 steps in R (this will help us understand the formula later):\n\nRPython\n\n\n\nsum_of_squares = sum(life_expectancy_variance_table$diff_squared)\nthis_variance  = sum_of_squares/length(life_expectancy_variance_table$diff_squared)\nthis_variance\n\n[1] 144.7314\n\n\n\n\n\nsum_of_squares = life_expectancy_variance_table['diff_squared'].sum()\nthis_variance = sum_of_squares/life_expectancy_variance_table['diff_squared'].count()\nthis_variance\n\n\n\n\n144.73136049752028\nWe can represent the above in the following formula for the population’s (remember, this is when you have everyone from the group you are measuring) variance:\n\\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N}\n\\]\nLet’s break down each of the above symbols: σ^2 is population variance Σ is sum xi refers to the value for each participant x̄ refers to the mean for all participants N refers to the number of participants\n(note that the above is written as if we’re looking at the variance of a group of participants, but the principles still work if looking at non-participant data)\n\n\nSample variance\nTo calculate the variance for a sample of participants, rather than every participant in the group you’re measuring, you need a slightly different formula:\n\\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N - 1}\n\\] Note that Sample variance is represented by S^2 rather than σ^2\nSo why do we divide by N-1 rather than N? This is because the sample variance is an estimate rather than the actual population variance. When estimating the population variance you take into account the actual number of people (N) in the sample, whereas when you are estimating what happens generally in the population based on your sample, you take into account the degrees of freedom (N-1). In broad terms this reduces the risk of you under-estimating the variance of the population. You don’t necessarily need to understand degrees of freedom beyond the idea that you are controlling for the fact that you are analysing a sample rather than the population they represent, so don’t worry if the next section isn’t completely clear."
  },
  {
    "objectID": "describingData/dispersion.html#degrees-of-freedom",
    "href": "describingData/dispersion.html#degrees-of-freedom",
    "title": "Dispersion",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom calculations can be useful to address statistics that are vulnerable to bias within a sample (i.e. the sample being distorted compared to the population). Interestingly, mean is not, but variance is. Lets see this by looking at differences between the population and sample for mean and variance, looking at the height of three people, and combining them into every combination of 2 people possible:\n\nRPython\n\n\n\nthree_heights = c(150,160,170)\npopulation_height_mean = mean(three_heights)\npopulation_height_variance = sum((three_heights - population_height_mean)^2)/3\n#sample participants in pairs\nsample_heights = data.frame(\n  pp1 = c(150,150,NA),\n  pp2 = c(160,NA,160),\n  pp3 = c(NA,170,170),\n  pair = c(\n    \"1 and 2\",\n    \"1 and 3\",\n    \"2 and 3\"\n  )\n)\n\nsample_heights$mean = c(\n  mean(c(three_heights[1], three_heights[2])),\n  mean(c(three_heights[1], three_heights[3])),\n  mean(c(three_heights[2], three_heights[3]))\n)\n\nsample_heights$pop_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/3,\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/3,\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/3\n)\n\nsample_heights$sample_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/(3-1),\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/(3-1),\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/(3-1)\n)\n\n\nrmarkdown::paged_table(sample_heights)\n\n\n\n  \n\n\nmean_sample_mean <- mean(sample_heights$mean)\nmean_sample_variance <- mean(sample_heights$sample_var)\nmean_population_variance <- mean(sample_heights$pop_var)\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nthree_heights = [150,160,170]\nthree_heights = pd.DataFrame(three_heights)\npopulation_height_mean = three_heights.mean()\npopulation_height_variance = (three_heights - population_height_mean).pow(2).sum()/3\n\n#sample participants in pairs\nsample_heights = {\n  'pp1': [150,150, np.nan],\n  'pp2': [160, np.nan, 160],\n  'pp3': [np.nan, 170, 170],\n  'pair': [\"1 and 2\", \"1 and 3\", \"2 and 3\"]\n}\n\nsample_heights = pd.DataFrame(sample_heights)\nmean = [three_heights.iloc[0:2].mean(),three_heights.iloc[[0,2],].mean(),three_heights.iloc[1:3].mean()]\nmean = pd.DataFrame(mean)\nsample_heights['mean']=mean\n\npop_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/3,((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/3,((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/3]\npop_var = pd.DataFrame(pop_var)\nsample_heights['pop_var']= pop_var\n\nsample_var=[((three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[0:2].mean()).pow(2))/(3-1),((three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2) + (three_heights.iloc[0] - three_heights.iloc[[0,2],].mean()).pow(2))/(3-1),((three_heights.iloc[1] - three_heights.iloc[1:3].mean()).pow(2) + (three_heights.iloc[2] - three_heights.iloc[1:3].mean()).pow(2))/(3-1)]\nsample_var=pd.DataFrame(sample_var)\nsample_heights['sample_var']=sample_var\n\n\nsample_heights = pd.DataFrame(sample_heights)\n#print(markdownTable(sample_heights.to_dict(orient='records')).getMarkdown())\nprint(tabulate(sample_heights, headers=sample_heights.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nmean_sample_mean = sample_heights['mean'].mean()\nmean_sample_variance = sample_heights['sample_var'].mean()\nmean_population_variance = sample_heights['pop_var'].mean()\n\n\n\n\n\n\n\nTable\n\n\nWhen comparing the population mean to the mean sample mean (i.e., what is the typical mean for any sample), they’re identical (i.e. NOT biased):\n\nRPython\n\n\n\npopulation_height_mean\n\n[1] 160\n\nmean_sample_mean\n\n[1] 160\n\n\n\n\n\npopulation_height_mean\nmean_sample_mean\n\n\n\n\n0    160.0\ndtype: float64\n\n160.0\nWhereas when comparing the actual population variance (population_height_variance) to the mean (to identify what is a typical) estimate of variance using the population formula that should not be used for samples (mean_population_variance) finds the estimate of variance is typically smaller than the actual variance in the population:\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_population_variance\n\n[1] 33.33333\n\n\n\n\n\npopulation_height_variance\nmean_population_variance\n\n\n\n\n0    66.666667\ndtype: float64\n\n33.333333333333336\nAs this bias (almost) always underestimates the population variance, degrees of freedom is a useful correction to address this within calculations of sample variance. Lets compare the actual population height variance (population_height_variance) to the mean estimate using degrees of freedom that should be used for samples (mean_sample_variance).\n\nRPython\n\n\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_sample_variance\n\n[1] 50\n\n\n\n\n\npopulation_height_variance\nmean_sample_variance\n\n\n\n\n0    66.666667\ndtype: float64\n\n50.0\nSo, not perfect, but this is less under-representative of the variance.\nOne thing to bear in mind is that calculation of some statistics does not require use of the degrees of freedom to correct for bias (as seen above, mean was not susceptible to bias).\nIf you would like to understand how degrees of freedom are determined, and what the thinking is behind this term, read on for a brief description of this (otherwise, feel free to skip to the next section).\nDegrees of freedom refers to how many values could change in your variable once you know what the outcome of the relevant statistic is. For example, if you’re interested in the variance of the height of the three people, then you only have 2 degrees of freedom, because once you know the height of 2 of the participants AND the variance of the height, then there the remaining participant only has a 2 possible heights (so their height isn’t free to change)."
  },
  {
    "objectID": "describingData/dispersion.html#standard-deviation-sd",
    "href": "describingData/dispersion.html#standard-deviation-sd",
    "title": "Dispersion",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is the square root of the variance. This takes into account that that the variance includes the square of the difference between the individual values and the mean:\nPopulation Variance \\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}\n\\] Population SD \\[\n\\sigma = \\sqrt\\frac{\\sum((x_i- \\bar{x}{})\\color{Black}{^2}\\color{Black})} {N}\n\\]\nSample Variance \\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}\n\\] Sample SD \\[\nS = \\sqrt\\frac{\\sum((x_i- \\bar{x}{} )\\color{Black}{^2}\\color{Black})} {N - 1}\n\\]"
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n  \n\n\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n!correct_vector == \"correct\" \n\n[1] FALSE  TRUE FALSE\n\n# which is the same as\n!(correct_vector == \"correct\")\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "rBasics/fundamentals.html",
    "href": "rBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "R allows you to complete calculations, so lets start with that. Type into your markdown or script\nOften, you can use R as a calculator, for example, if you want to know what two people’s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nIt’s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g. how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. “my object” would be an invalid object name.\nObject names are case-sensitive, so if your object is called “MyObject” you cannot refer to it as “myobject”.\n“.” and “_” are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou can’t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing “t.te” and pressing the tab will give you “t.test”\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you don’t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe “c” above is short for “combine” as you’re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the “typeof” function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (“heights”) is a “double” vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers can’t always be integers. Strings are identified as “character” objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the “mpg” dataframe. To look at it, just type in it’s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below won’t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see what’s in the “manufacturer” column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about “intelligent copy and paste” to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming you’ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code we’ve copied so that you can apply it to your own scripts. There’s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\nggplot(             # R will keep looking at your code until all the open brackets have been closed)\n  mpg,              #\n  aes(              #\n    displ,          #\n    hwy,            #\n    colour = class  #\n  )\n) +                 # R will look to the next line if you end a line with +\ngeom_point()        #\n\n\n\n\nHere’s how I would comment it out:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    displ,          # x-axis\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\n\n\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    cty,            # x-axis - updated\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\n\n\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "rBasics/filetypes.html",
    "href": "rBasics/filetypes.html",
    "title": "Types of Scripts",
    "section": "",
    "text": "Welcome to using R. This subsection will explain the basics about R. First, lets discuss the different ways to write R code, as there are (at least) 5:\n\ntype it directly into the console (generally not recommended)\nsave it as a script (better) note that script can refer to any of the below, but in this case is being used to describe a script that doesn’t generate a notebook\nsave it as an R Markdown (better still) - this allows you to make beautiful documents\nsave it as an R Notebook (arguably better than R Markdown) - this allows you to make beautiful documents, and is quicker\n\n\nThe Console\nAt the bottom left of RStudio you should have a console that looks something like what’s highlighted in red below:\n\n\n\nconsole\n\n\nYou can type straight into the console, to get a result. You can scroll through your previous commands by pressing the up arrow in the console. Each time code is run in the console it updates the environment in the top right of R-Studio:\n\n\n\nenvironment\n\n\n\n\nScripts\nThe word “script” can be interpreted specifically, to refer to a type of R file that includes a lot of code, or generally to refer to any file that includes both R code and code that allows you make a nice looking report. In this subsection, we will be focusing on “script” as a particular type of file. To create a script, click on File -> New File -> R Script\n\n\n\nnewScript\n\n\nYou will then be shown a blank script, in which you can write a series of functions, and then run them. To run the lines of code, select a line, and then press CTRL-ENTER, or highlight a chunk of code and then press CTRL-ENTER. In either case, the code will be sent to the console and run there.\nAn advantage of a script over just using the console is that you can analyse your data in both structured and complex ways which is difficult if you are typing code directly into the console.\n\n\nR Markdown\nAs highlighted above, R Markdown is a type of “script” in the general sense of the word, but allows you to create beautiful .html notebooks (.html files are what internet pages are based on). You are in fact reading an example of what can be produced by R Markdown (and R Notebooks). To make an R Markdown file, click on File -> New File -> R Markdown. You will be asked for a title, author and what output you would like. I would suggest “first markdown”, your name and “html” as the respective answers. You should then see something like:\n\n\n\nmarkdown\n\n\nThe following points apply to both R Markdown and R Notebooks\nIf you look above, you may notice that there are 2 types of code: Markdown (to write a nice looking report) and R (in grey chunks). I think these are well explained here: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf so I’ll just explain that R Markdowns run all the code in the chunks each time they generate the output file (e.g. html file). This is important to know, because R Notebooks do not run all the code in all the chunks when you generate them (see below for more on this).\n\n\nR Notebooks\nR Notebooks can be created by clicking on File -> New File -> R Notebook. They look quite similar to R Markdowns, but automatically generate the .html output each time you save the notebook. The output file will be a .nb.html file in the same folder as your notebook.\nVery importantly - the .nb.html file will be built based on what happened the last time you run each R chunk. If you never ran the R Chunk, then the nb.html file will not use the output from that chunk. This makes R Notebooks quicker than R Markdowns, because you don’t have to generate the output from scratch each time, as it will just use whatever was generated the last time the chunk was run. However, this means that there’s a risk your .html output will not be what you expect if you failed to run all of your chunks before saving your file. To address this risk (when you’ve finished editing your file), you can select the “run all” option.\n\n\n\nrunAllChunks"
  },
  {
    "objectID": "installing.html",
    "href": "installing.html",
    "title": "Installing R(Studio)",
    "section": "",
    "text": "Installing R\nDownload and install the following depending on your operating system:\n\nWindows: https://cran.r-project.org/bin/windows/base/\nMac: and select the R-x.x.x.pkg notarized and signed option\n\n\n\nInstalling RStudio\n\nhttps://www.rstudio.com/products/rstudio/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "href": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "title": "R-squared vs. Adjusted R-squared",
    "section": "",
    "text": "When completing a regression, there’s always a risk of “overfitting” the data, i.e. creating a model that includes predictors that have no meaningful association with the outcome variable. One reason that overfitting the data is a problem is that it is almost impossible for a predictor to have no association with an outcome variable. For that to happen you would need any data points that suggested a positive association between the outcome and the predictor to be equally balanced out by data points that suggest a negative association:\n\nRPython\n\n\n\nlibrary(ggplot2)\nno_association_df <- data.frame(\n  predictor = c(1,1,1,2,2,2,3,3,3),\n  outcome   = c(1,2,3,1,2,3,1,2,3)\n)\n\nggplot(no_association_df, aes(x = predictor, y = outcome)) + geom_point() + geom_smooth(method=lm, formula = 'y ~ x')\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#sample participants in pairs\nno_association_df = {\n  'predictor': [1,1,1,2,2,2,3,3,3],\n  'outcome': [1,2,3,1,2,3,1,2,3]\n}\n\nno_association_df = pd.DataFrame(no_association_df)\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\ncoef = np.polyfit(no_association_df[\"predictor\"],no_association_df[\"outcome\"],1)\npoly1d_fn = np.poly1d(coef) \n# poly1d_fn is now a function which takes in x and returns an estimate for y\n\nplt.plot(no_association_df[\"predictor\"],no_association_df[\"outcome\"], 'ko', no_association_df[\"predictor\"], poly1d_fn(no_association_df[\"predictor\"]), '-b') #'--k'=black dashed line, 'yo' = yellow circle marker\n\n# add title on the x-axis\nplt.xlabel(\"predictor\")\n\n# add title on the y-axis\nplt.ylabel(\"outcome\")\n\nplt.show()\n\n\n\n\n\n\n\nUnrealistic Data Distribution\n\n\nFig. X. An example of how unrealistically balanced your data needs to be to find no association. As this (almost) never happens in reality, samples are biased towards finding associations between predictor and outcome variables even when there aren’t any in the population. For example, let’s generate some random data, and see what R-Values we find. Remember, random data really shouldn’t have any association between predictor and outcome variables.\n\nRPython\n\n\n\nrandom_df = data.frame(\n  random_iv_1 = runif(100),\n  random_iv_2 = runif(100),\n  random_iv_3 = runif(100),\n  random_dv = runif(100)\n)\nrmarkdown::paged_table(random_df)\n\n\n\n  \n\n\n\n\n\n\nimport random\nfrom tabulate import tabulate\n\nrandom_df = {\n    'random_iv_1': np.random.random_sample(size = 100),\n    'random_iv_2': np.random.random_sample(size = 100),\n    'random_iv_3': np.random.random_sample(size = 100),\n    'random_dv': np.random.random_sample(size = 100)\n}\n \n# convert it to a data frame\nrandom_df = pd.DataFrame(random_df)\n\n# print the table\nprint(tabulate(random_df[:10], headers=random_df.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\n\nRPython\n\n\n\nrandom_lm <- lm(random_dv ~ random_iv_1, random_df)\nrandom_summary <- summary(random_lm)\nrandom_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1, data = random_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50997 -0.21262 -0.00434  0.24704  0.52558 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.55350    0.05684   9.738 4.49e-16 ***\nrandom_iv_1 -0.11303    0.09842  -1.148    0.254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2874 on 98 degrees of freedom\nMultiple R-squared:  0.01328,   Adjusted R-squared:  0.003211 \nF-statistic: 1.319 on 1 and 98 DF,  p-value: 0.2536\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_lm = smf.ols(formula='random_dv ~ random_iv_1', data=random_df).fit()\nrandom_lm.summary()\n\n\n\n\n\n\n\nTable\n\n\nNote that the above output is generated each time this page is rendered (generated), and so by chance may happen to look like the random predictor is significant. If so, there’s a 95% chance that this predictor will not be significant next time the page is rendered.\nLooking at the output above, we can see that 1.3279% of the variance of random_dv was explained by random_iv_1 before correction. Considering that these were randomly generated numbers, that’s 1.3279% too much. However, the Adjusted R-squared is only 0.0032. Note that Adjusted R-squared can be a negative number, and a negative number suggests that based on the sample, the predictor(s) has(/have) no association with the outcome variable in the population.\nA formula for the adjusted r-squared is:\n\\[\n\\bar{R^2} = 1-\\frac{SS_{res}/df_{res}}{SS_{tot}/df_{tot}}\n\\] \\(\\bar{R^2}\\) is the Adjusted R-Squared \\(SS_{total}\\) is the Sum of Squares of the total (i.e. how much total variance there is around the mean to explain) \\(SS_{res}\\) is the Sum of Squares of the residuals (i.e. how much isn’t explained by the model) \\(df_{total}\\) is the Degrees of Freedom of the total. This is the number of data points - 1, so is N - 1 \\(df_{res}\\) is the Degrees of Freedom of the residuals. The degrees of freedom for the residuals takes into account the number of data points and the number of predictors, and so is N - 1 - 1\nLet’s use the above formula to manually calculate the Adjusted R Squared\n\nRPython\n\n\n\nss_res <- sum(random_lm$residuals^2)\nss_total <- sum(\n  (\n    random_df$random_dv - mean(random_df$random_dv)\n  )^2\n)\n\n\nrandom_r_square = ss_total - ss_res\ndf_total <- length(random_lm$residuals) - 1\ndf_res <- length(random_lm$residuals) -\n  1 - # remove 1 from the number of data points\n  1 # remove another 1 to reflect there being 1 predictor\nadjusted_random_r_square = 1 - (ss_res/df_res)/(ss_total/df_total)\n\nadjusted_random_r_square\n\n[1] 0.00321055\n\n\n\n\n\nss_res = sum(random_lm.resid**2)\nss_total = sum((random_df[\"random_dv\"] - random_df[\"random_dv\"].mean())**2)\n\nrandom_r_square = ss_total-ss_res\ndf_total = len(random_lm.resid)-1\ndf_res = len(random_lm.resid)-1 - 1\n\nadjusted_random_r_square = 1-(ss_res/df_res)/(ss_total/df_total)\nadjusted_random_r_square\n\n\n\n\n-0.009973456268681513\nThe number above should match the Adjusted R-Squared from the multiple regression above. Let’s explore what happens when we have multiple predictors:\n\nRPython\n\n\n\nrandom_lm_multiple <- lm(random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, random_df)\nrandom_multiple_summary <- summary(random_lm_multiple)\nrandom_multiple_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, \n    data = random_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5360 -0.2005 -0.0203  0.2274  0.5424 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.55721    0.08721   6.389 5.98e-09 ***\nrandom_iv_1 -0.09873    0.09821  -1.005    0.317    \nrandom_iv_2 -0.14274    0.09274  -1.539    0.127    \nrandom_iv_3  0.12236    0.10055   1.217    0.227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2855 on 96 degrees of freedom\nMultiple R-squared:  0.04587,   Adjusted R-squared:  0.01606 \nF-statistic: 1.538 on 3 and 96 DF,  p-value: 0.2095\n\n\n\n\n\nimport statsmodels.formula.api as smf\nrandom_multiple_summary = smf.ols(formula='random_dv ~ random_iv_1 + random_iv_2 + random_iv_3', data=random_df).fit()\nrandom_multiple_summary.summary()\n\n\n\n\n\n\n\nTable\n\n\nTwo things to look for from the above: - The model with 3 predictors has higher (Multiple) R-Squared than the model with only 1 predictor. This reflects problems with over-fitting the model: the more predictors you include in your sample, the more variance in the outcome that will be explained by the predictors, even if those associations between the predictors are arbitrary (i.e. don’t reflect anything about the general population). - Adjusted R-squared values are less susceptible to this bias of overfitting the data (but is not completely invulnerable to it). All statistical tests are vulnerable to false positives and including Adjusted R-squared values.\nRemember, the adjusted r-square is necessary for us to make claims about the general population. If we just wanted to make a claim about our sample, we would just use the r-squared, as we don’t need to correct our estimate."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html",
    "href": "GeneralLinearModels/generalLinearModels.html",
    "title": "General Linear Models and Sum of Squares",
    "section": "",
    "text": "General linear models allow you to analyse data in which the dependent variable is continuous. For example, if you are analysing the height of a group of individuals, you might use one of the following analyses:\n\nt-test, comparisons between two conditions e.g. are males taller than females?\nregression, one or more predictors of a single outcome e.g. does foot size, weight etc. predict height? (Note that correlations are equivalent to a regression with a single predictor)\nANOVA, comparisons between 3 or more conditions or between multiple categorical factors, e.g. are there differences in height between sexes and nationalities?\n\nLinear refers to the dependent variable being continuous.\nGeneral refers to the fact that the independent variables can both be continuous (e.g. regression) or categorical (e.g. t-test or ANOVA).\nIn general linear models all analyses involve creating a model, and capturing what is and isn’t explained by the model (i.e. the error of the model). All analyses in general linear models can be formulated as:\n\\[\nData = Model + Error\n\\]\nData: The dependent variable in your analysis Model: A model which predicts a phenomenon. This could be multiple independent variables. Error: What data isn’t explained by the model."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "href": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "title": "General Linear Models and Sum of Squares",
    "section": "Dummy vs. effect coding for categorical variables in a model",
    "text": "Dummy vs. effect coding for categorical variables in a model\nGeneral Linear Models need numerical values for the predictors. As categorical variables (e.g. Sex) don’t have a numeric value by default, we have to substitute the categories with numbers:\n\nEffect coding can be used when you have a binary categorical variable, and you allocate one level 1 and the other -1. For example, you could allocate all females the score 1, and all non-female participants -1. A disadvantage of this approach is that it works best when you have binary categorical variable, but doesn’t work as well when you have 3 or more levels. For example, coding female, male and non-binary sex doesn’t work well with effect coding.\nDummy coding involves allocating a 1 if someone is in a cateogory, and 0 if they are outside of the category. For example, you could allocate 1 to all your female participants, and 0 to all participants who aren’t female to a variable “sex_female”. An advantage of this approach is that you have flexibility to have more than 2 levels, such as having “sex_female”, “sex_male” and “sex_nonbinary” as variables that are all either 1 or 0."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "href": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "title": "General Linear Models and Sum of Squares",
    "section": "Mean as the simplest model of data",
    "text": "Mean as the simplest model of data\nIf you want to estimate what someone’s life expectancy would be in 2007, you could look at the mean life expectancy using the gapminder data. In terms of how this corresponds to the above model:\n\\[\nData = Model + Error\n\\]\n\\[\nestimatedLifeExpectancy = mean(lifeExpectancy) + Error\n\\]\n\nRPython\n\n\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\ngapminder_2007['lifeExp'].mean()\n\n\n\n\n67.00742253521126\n\\[\nestimatedLifeExpectancy = 67.01 + Error\n\\]\nWhich could be visualised as:\n\nRPython\n\n\n\nlibrary(ggplot2)\nggplot(\n  gapminder_2007, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_jitter() +\n  geom_hline(yintercept = mean(gapminder_2007$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], gapminder_2007[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\nplt.show()\n\n\n\n\n\n\n\nPlot of Mean rank(lifeExp) and residuals\n\n\nFig. 1.\nIn English, the above model and figure allow you to predict that anyone’s life expectancy will be 67 years. However, as you can also see, there’s a huge amount of error, i.e. variance in life expectancy that is not explained by the model. These errors can be squared and summed to give the sum of squares, a statistic of how much error there is around the model:\n\\[\nSS = \\sum(Y_i-\\bar{Y})^2\n\\]\nWhich can be visualised as follows:\n\nRPython\n\n\n\nggplot(\n  gapminder_2007, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"lifeExp_rank\"], (gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, color='black', s=10)\n\n# vertical lines\nplt.vlines(x=gapminder_2007[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_2007[\"lifeExp\"]-gapminder_2007[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\n\n# show plot\nplt.show()\n\n\n\n\n\n\n\nPlot of squared residuals\n\n\nFig. 2.\nYou can directly compare fig. 1. and fig. 2. to see how much error is associated with each data point compared to the model. Fig. 2. is positive because it is the distance of the data-point from the mean squared. If you added together all the squares (pink lines) in fig. 2. that would give you the sum of squares.\nAs you may have guessed, it is possible to have more precise models that have less error, and thus a smaller sum of squares. The sum of squares around the mean is also the total sum of squares, and the total variance. When we calculate the proportion of the variance that a model explains, we are comparing it to this variance around the mean.\nLet’s explore those possibilities now."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "href": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "title": "General Linear Models and Sum of Squares",
    "section": "T-Tests",
    "text": "T-Tests\nT-tests are restricted to comparisons between 2 conditions/groups, so we will restrict the Gapminder data to allow a comparison between 2 continents. To see if life expectancy was different if you are born in Europe compared to the Americas, let’s first check what the sum of squares is when you just use the mean as the model of life expectancy across these contents:\n\nRPython\n\n\n\ngapminder_americas_europe <- subset(\n  gapminder_2007,   # the data set\n  continent == \"Europe\" | continent == \"Americas\"\n)\n\nggplot(\n  gapminder_americas_europe, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_point() +\n  geom_hline(yintercept = mean(gapminder_americas_europe$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\ngapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == \"Europe\") | (gapminder_2007['continent'] == \"Americas\")]\n\ngapminder_americas_europe[\"lifeExp_rank\"] = gapminder_americas_europe[\"lifeExp\"].rank()\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], gapminder_americas_europe[\"lifeExp\"], color='black', s=10)\n# only one line may be specified; full height\nplt.axhline(y=gapminder_americas_europe[\"lifeExp\"].mean(), color='blue', ls='-')\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=gapminder_americas_europe[\"lifeExp\"], ymax=gapminder_americas_europe[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"lifeExp\")\n\nplt.show()\n\n\n\n\nFig. 3. The errors around the mean of life expectancy across Europe and American countries.\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\nOnce we square the errors in the pink lines above, we’ll get the squares:\n\nRPython\n\n\n\nggplot(\n  gapminder_americas_europe, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\nsum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\n\n[1] 953.4478\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_americas_europe[\"lifeExp_rank\"], (gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, color='black', s=10)\n# only one line may be specified; full height\n\nplt.vlines(x=gapminder_americas_europe[\"lifeExp_rank\"],ymin=0, ymax=(gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"rank(lifeExp)\")\n\n# add title on the y-axis\nplt.ylabel(\"(Life Expectancy - mean(Life Expectancy))^2\")\nplt.show()\n\nsum((gapminder_americas_europe[\"lifeExp\"]-gapminder_americas_europe[\"lifeExp\"].mean())**2)\n\n\n\n\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\n953.4477649818183\nAnd when you add all of these together:\n\\[\nSumOfSquares = \\sum(Y_i-\\bar{Y})^2 = 953.4478\n\\]\nSo if the model we create for a t-test would result in a smaller sum of squares then that suggests it’s a more precise model for estimating life expectancy than simply using the mean as a model. This is because this would mean there’s less error in this model. Let’s model this using a t-test. For this we will need to dummy code country:\n\nRPython\n\n\n\n# create a column to place 1 or -1 for each row dependent on the country\ncontEffect = NA\ncontEffect[gapminder_americas_europe$continent == \"Europe\"] = 1\ncontEffect[gapminder_americas_europe$continent == \"Americas\"] = -1\ngapminder_americas_europe = cbind(contEffect,gapminder_americas_europe)\nrmarkdown::paged_table(head(gapminder_americas_europe))\n\n\n\n  \n\n\n\n\n\n\ngapminder_americas_europe = gapminder_2007.loc[(gapminder_2007['continent'] == \"Europe\") | (gapminder_2007['continent'] == \"Americas\")]\ngapminder_americas_europe[\"contEffect\"]=0\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Europe\")]=1\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Americas\")]=-1\ncols = list(gapminder_americas_europe.columns)\ncols = list(gapminder_americas_europe.columns)\ncols = cols[len(cols)-1:len(cols):1]+cols[0:-1:1]\ngapminder_americas_europe = gapminder_americas_europe[cols]\nprint(tabulate(gapminder_americas_europe[:6], headers=gapminder_americas_europe.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n\n\n\n\n\n\nTable\n\n\nNow that we have dummy coded the continent, we can create a new model to try to predict an individual’s life expectancy based on which continent they are from\n\\[\nY = intercept + \\beta * dummyVariable + Error\n\\]\n\\[\nlifeExp = mean(lifeExp) + \\beta * contEffect + Error\n\\]\n\nY being the predicted life expectancy.\n\\(\\bar{Y}\\) being the mean life expectancy regardless of continent. For a t-test this is also the \\(intercept\\).\n\\(\\beta\\) being how much to adjust the prediction based on which continent the person is from\n\\(contEffect\\) being 1 (Europe) or -1 (Americas) to reflect which continent the participant is from\n\\(Error\\) being any error in the prediction not captured by the model\n\nTo get the \\(intercept\\) and \\(\\beta\\) for the above formula let’s use the lm function in R:\n\nRPython\n\n\n\ncontinent_ttest <- lm(lifeExp ~ contEffect, gapminder_americas_europe)\n\ncontinent_ttest$coefficients[1] \n\n(Intercept) \n   75.62836 \n\ncontinent_ttest$coefficients[2]\n\ncontEffect \n   2.02024 \n\ngapminder_americas_europe$t_fit = continent_ttest$coefficients[1] + # intercept\n  continent_ttest$coefficients[2]                       * # gradient\n  gapminder_americas_europe$contEffect\n\n\nggplot(gapminder_americas_europe, aes(x = contEffect, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = contEffect,\n      yend = t_fit,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = -1.9, \n    xend = -.1, \n    y = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n    yend = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = 0.1, \n      xend = 1.9, \n      y = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n      yend = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]\n    ),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = - 1.9,\n      xend = 1.9,\n      y = mean(gapminder_americas_europe$lifeExp),\n      yend = mean(gapminder_americas_europe$lifeExp)\n    ),\n    color = \"dark green\"\n  )\n\nWarning: Use of `gapminder_americas_europe$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\nUse of `gapminder_americas_europe$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\n\n\n\n\n\n\n\n\nfrom scipy import stats\n\n# convert 'contEffect' to type category\ngapminder_americas_europe['contEffect'] = gapminder_americas_europe['contEffect'].astype('category')\n\n# lm 'contEffect' ~ 'lifeExp'\ncontinent_ttest = stats.linregress(gapminder_americas_europe['contEffect'],gapminder_americas_europe['lifeExp'])\n\n# show results of lm\ncontinent_ttest\n\ngapminder_americas_europe[\"contEffect\"]=0\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Europe\")]=1\ngapminder_americas_europe[\"contEffect\"].loc[(gapminder_americas_europe['continent'] == \"Americas\")]=-1\n\ngapminder_americas_europe['t_fit'] = continent_ttest.intercept +continent_ttest.slope * gapminder_americas_europe['contEffect']\n\ngapminder_americas_europe['t_res_square'] = (gapminder_americas_europe['lifeExp'] - gapminder_americas_europe['t_fit'])**2\n\n# calculate 'lifeExp' mean for 'contEffect' ==-1\nm1 = gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1].mean()\n\n# calculate 'lifeExp' mean for 'contEffect' ==1\nm2 = gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1].mean()\n\n# repeat lifeExp' mean for 'contEffect' ==-1\nm11=np.repeat(m1, len(gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1]), axis=0)\n\n# repeat lifeExp' mean for 'contEffect' ==1\nm22=np.repeat(m2, len(gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1]), axis=0)\n\n# create x coordinates for 'contEffect' ==-1\nx1 = np.arange(-1.98, -.05, 0.08)\n\n# create x coordinates for 'contEffect' ==1\nx2 = np.arange(0.05, 1.98, 0.065)\n\n\nfig, ax = plt.subplots(figsize =(10, 7))\nax.set_ylim([60, 80])\nplt.axhline(y=gapminder_americas_europe[\"lifeExp\"].mean(), color='green', ls='-')\nplt.hlines(y=m1,xmin=-1.99, xmax=-.04, colors='blue', lw=0.5)\nplt.hlines(y=m2,xmin=1.99, xmax=.04, colors='blue', lw=0.5)\nplt.vlines(x= x1,ymin=gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == -1], ymax=m11, colors='red', lw=0.5)\nplt.vlines(x= x2,ymin=gapminder_americas_europe[\"lifeExp\"][gapminder_americas_europe['contEffect'] == 1], ymax=m22, colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"contEffect\")\n\n# add title on the y-axis\nplt.ylabel(\"lifeExp\")\n\nplt.show()\n\n\n\n\nLinregressResult(slope=2.020240000000001, intercept=75.62836, rvalue=0.4832076439158285, pvalue=0.00018637488941351192, stderr=0.502794193121279, intercept_stderr=0.5027941931212789)\n\n\n\nPlot with the errors around the mean of life expectancy across Europe and American countries\n\n\nFig. X. Countries in the americas are dummy coded as -1 and countries in Europe are dummy coded as 1. Note that jittering has been used to help visualise variation within continents, and so all countries in Americas had a \\(contEffect\\) score of -1, even if the jittering above makes it look like participants from Europe had slightly different \\(contEffect\\) values to each other.\nSo now that we’ve visualised the predictions and the error, lets summarise these errors with their sum of squares:\n\nRPython\n\n\n\n#temp_summary <- summary(lm(lifeExp ~ contEffect, data = gapminder_americas_europe))\nsummary(aov(lifeExp ~ contEffect, data = gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# between\noverall_mean <- mean(gapminder_americas_europe$lifeExp)\neurope_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1])\namerica_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1])\nss_between <- \n  sum(gapminder_americas_europe$contEffect == 1) * (europe_mean - overall_mean)^2 +\n  sum(gapminder_americas_europe$contEffect == -1) * (america_mean - overall_mean)^2\n  \ntop_half = ss_between\n\nss_within = (\n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] - europe_mean)^2) + \n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1] - america_mean)^2)\n)\n  \nbottom_half = (ss_within/(length(gapminder_americas_europe$lifeExp) - 2))\n\ntop_half/bottom_half\n\n[1] 16.14453\n\n# Compare with a t-test\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\n4.018^2\n\n[1] 16.14432\n\n# look at a t-distribution compared to an f-distribution\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nmodel = ols('lifeExp ~ contEffect', data = gapminder_americas_europe).fit()\naov_table = sm.stats.anova_lm(model, typ=2)\naov_table\n\n# between\noverall_mean = gapminder_americas_europe['lifeExp'].mean()\neurope_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1].mean()\n\namerica_mean = gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1].mean()\n\nss_between =(sum(gapminder_americas_europe['contEffect'] == 1) * (europe_mean - overall_mean)**2) + (sum(gapminder_americas_europe['contEffect'] == -1) * (america_mean - overall_mean)**2)\n\ntop_half = ss_between\n\nss_within = (sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1] - europe_mean)**2)+ sum((gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1] - america_mean)**2))\n\nbottom_half = (ss_within/(len(gapminder_americas_europe['lifeExp']) - 2))\n\ntop_half/bottom_half\n\n# Compare with a t-test\nfrom scipy.stats import ttest_ind\n\nt_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\nt_test\nt_test.statistic **2\n\n\n\n\n\n\n\nTable\n\n\nTtest_indResult(statistic=4.018025720342183, pvalue=0.00018637488941352037)\n\n16.144530689331322\nSo the new sum of squares is 730.8276, which is smaller than it was when we just used the mean regardless of continent (953.4478) which also summarises the total variance (around the mean). In fact, we can use these 2 numbers to calculate the \\(r^2\\) value (i.e. what proportion of the variance around the mean is explained by the model). The amount of variance explained by the model can be calculated by:\n\\[\ntotalSumOfSquares - modelSumOfSquares = totalError - modelError\n\\]\nThis allows us to calculate an r-value and thus a p-value:\n\nRPython\n\n\n\nthis_r2 = 1 - sum(gapminder_americas_europe$t_res_squared)/sum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\nthis_r = sqrt(this_r2)\nthis_r\n\n[1] 1\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nthis_r2\n\n[1] 1\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport math\n\nthis_r2 = 1- sum(gapminder_americas_europe['t_res_square'])/ (sum((gapminder_americas_europe['lifeExp'] - gapminder_americas_europe['lifeExp'].mean())**2))\nthis_r=math.sqrt(this_r2)\nthis_r\nt_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\nt_test\n\nmodel = ols('lifeExp ~ contEffect', data = gapminder_americas_europe).fit()\naov_table = sm.stats.anova_lm(model, typ=2)\naov_table\n\nthis_r2\n\n\n\n\nYou may notice above that the manually calculated \\(r^2\\) value is identical to the “Multiple R-Squared”, rather than the “Adjusted R-squared”. So what’s the difference between r-squared and adjusted r-squared?\n\nEffect sizes (eta-squared and partial eta-squared)\n\nRPython\n\n\n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\n\nCall:\nlm(formula = lifeExp ~ contEffect, data = gapminder_americas_europe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6921  -2.1364   0.4494   2.5671   7.0449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  75.6284     0.5028 150.416  < 2e-16 ***\ncontEffect    2.0202     0.5028   4.018 0.000186 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.713 on 53 degrees of freedom\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.219 \nF-statistic: 16.14 on 1 and 53 DF,  p-value: 0.0001864\n\n222.62 /(222.62 +13.79)\n\n[1] 0.9416691\n\n\n\n\n\nt_test\naov_table\n222.62 /(222.62 +13.79)\n\n\n\n\n0.941669134131382\n\nRPython\n\n\n\nmale_female_height <- data.frame(\n  sex = c(\"male\",\"male\",\"female\",\"female\",\"female\",\"female\",\"female\",\"female\"),\n  height = c(2.5,2.2,1.5,1.5,1.4,1.4,1.3,1.3),\n  sex_dummy = c(-1,-1,1,1,1,1,1,1)\n)\nmean(male_female_height$height[male_female_height$sex == \"male\"])\n\n[1] 2.35\n\nmean(male_female_height$height[male_female_height$sex == \"female\"])\n\n[1] 1.4\n\nsummary(lm(height ~ sex_dummy, data = male_female_height))\n\n\nCall:\nlm(formula = height ~ sex_dummy, data = male_female_height)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.15  -0.10   0.00   0.10   0.15 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.87500    0.04859  38.587 2.02e-08 ***\nsex_dummy   -0.47500    0.04859  -9.775 6.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.119 on 6 degrees of freedom\nMultiple R-squared:  0.9409,    Adjusted R-squared:  0.9311 \nF-statistic: 95.56 on 1 and 6 DF,  p-value: 6.592e-05\n\n\n\n\n\nmale_female_height = {\n    'sex': [\"male\",\"male\",\"female\",\"female\",\"female\",\"female\",\"female\",\"female\"],\n    'height': [2.5,2.2,1.5,1.5,1.4,1.4,1.3,1.3],\n    'sex_dummy': [-1,-1,1,1,1,1,1,1]\n}\nmale_female_height = pd.DataFrame(male_female_height)\nmale_female_height['height'][male_female_height['sex'] == \"male\"].mean()\nmale_female_height['height'][male_female_height['sex'] == \"female\"].mean()\n\nimport statsmodels.formula.api as smf\nmodel2 = smf.ols(formula='height ~ sex_dummy', data=male_female_height).fit()\nmodel2.summary()\n\n\n\n\n2.35\n\n1.4000000000000001\n\n\n\nTale\n\n\nTo show that we’ve achieved the same as a t-test, let’s run a between subjects t-test that assumes the variance is equal between the groups (which is an assumption of a general linear model), and see if the p-values are the same:\n\nRPython\n\n\n\n#953.4478/730.8276\ncontinent_ttest <- t.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  # general linear models assume the variance between conditions is equal\n  var.equal = T\n)\ncontinent_model <- summary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\ncontinent_ttest$p.value\n\n[1] 0.0001863749\n\ncontinent_model$coefficients[2,4] # p-value for the continent as a predictor\n\n[1] 0.0001863749\n\n\n\n\n\ncontinent_ttest = t_test = ttest_ind(gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == 1],gapminder_americas_europe['lifeExp'][gapminder_americas_europe['contEffect'] == -1])\n\ncontinent_ttest.pvalue\n\n\n\n\nThere are some advantages of conducting a t-test using the “lm” functionality:\n\nYou can capture residuals\nYou have more flexibility to make more complex models\n\nLet’s now see how we can proceed if we have a more complex design, i.e. 3 or more levels and/or more than 1 factor, using ANOVAs.\n\nRPython\n\n\n\n## Automatic calculation\ngapminder_3_continents <- subset(\n  gapminder_2007, \n  continent == \"Europe\" | continent == \"Americas\" | continent == \"Africa\"\n)\n\nsummary(aov(lifeExp ~ factor(continent), data = gapminder_3_continents))\n\n                   Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(continent)   2  12017    6008   114.4 <2e-16 ***\nResiduals         104   5461      53                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Manual calculation\nss_between = (\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Europe\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Americas\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Africa\") \n)\n\ntotalSS = sum((gapminder_3_continents$lifeExp - mean(gapminder_3_continents$lifeExp))^2)\n\nshortcut_ss_within = totalSS - ss_between\n\nss_within_long = sum(\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]))^2\n)\n\n(ss_between/2)/(ss_within_long/(length(gapminder_3_continents$lifeExp)-3))\n\n[1] 114.4212\n\n\n\n\n\ngapminder_3_continents['continent'] = gapminder_3_continents['continent'].astype('category')\nmodel3 = ols('lifeExp ~ continent', data = gapminder_3_continents).fit()\naov_table3 = sm.stats.anova_lm(model3, typ=2)\naov_table3\n\nss_between = (gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Europe\")+(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Americas\")+(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"].mean() - gapminder_3_continents['lifeExp'].mean())**2 * sum(gapminder_3_continents['continent']==\"Africa\")\n\ntotalSS = sum((gapminder_3_continents['lifeExp'] - gapminder_3_continents['lifeExp'].mean())**2)\n\nwithin1_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Europe\"].mean())**2\nwithin1= sum(within1_all)\n\nwithin2_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Americas\"].mean())**2\nwithin2= sum(within2_all)\n\nwithin3_all=(gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"]-gapminder_3_continents['lifeExp'][gapminder_3_continents['continent']==\"Africa\"].mean())**2\nwithin3= sum(within3_all)\n\nss_within_long = within1 + within2 + within3\n\n(ss_between/2)/(ss_within_long/(len(gapminder_3_continents['lifeExp'])-3))\n\n\n\n\n114.42116407168957\nTo visualise this\n\nRPython\n\n\n\nlm_3_continents <- summary(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\nlm_3_continents$coefficients\n\n                          Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)               54.80604   1.004904 54.53856 2.466786e-78\nfactor(continent)Americas 18.80208   1.763600 10.66119 2.243512e-18\nfactor(continent)Europe   22.84256   1.661388 13.74908 3.900175e-25\n\nmean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])\n\n[1] 77.6486\n\ngapminder_3_continents$continent_mean = lm_3_continents$coefficients[1,1] # intercept, which is mean for Africa as there are only \"Americas\" and \"Europe\" factors.\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] + lm_3_continents$coefficients[2,1]\n\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] + lm_3_continents$coefficients[3,1]\n\n\n\nggplot(gapminder_3_continents, aes(x = continent, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = continent,\n      yend = continent_mean,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = 0.55, \n    xend = 1.45, \n    y = lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 1.55, \n    xend = 2.45, \n    y = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 2.55, \n    xend = 3.45, \n    y = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) +\n  geom_segment(\n    aes(\n      x = 0.55,\n      xend = 3.45,\n      y = mean(gapminder_3_continents$lifeExp),\n      yend = mean(gapminder_3_continents$lifeExp),\n      color = \"Overall Mean\"\n    )\n  )\n\nWarning: Use of `gapminder_3_continents$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\nUse of `gapminder_3_continents$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nspec = dict(x=\"continent\", y=\"lifeExp\", data=gapminder_3_continents)\nfig, ax = plt.subplots(figsize =(7, 5))\nsns.stripplot(**spec, size=4, palette=\"deep\")\nsns.pointplot(**spec, join=False, ci=0, capsize=.7, scale=0, palette=\"deep\")\nplt.axhline(y=gapminder_3_continents[\"lifeExp\"].mean(), color='red', ls='-')\nplt.show()\n\n\n\n\nFig. XXX. Our model in the figure above considers the distance from each of the continent’s mean to the overall mean as part of the explained variance. For each data point, the squared distance from the the mean line to the overall mean line is part of the sum of squares at the top of the formula.\n\n\n2 x 2 ANOVA\nLet’s create data to allow us to compare between 2 years, and between Europe and Americas\n\nRPython\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 <- subset(\n  gapminder,   # the data set\n  year == 2002 & continent == \"Africa\" |\n  year == 2007 & continent == \"Africa\" |\n  year == 2002 & continent == \"Europe\" |\n  year == 2007 & continent == \"Europe\"\n)\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 = gapminder.loc[(gapminder['year'] == 2002) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Africa\") | (gapminder['year'] == 2002) & (gapminder['continent'] == \"Europe\") | (gapminder['year'] == 2007) & (gapminder['continent'] == \"Europe\") ]\n\nfrom statsmodels.formula.api import ols\n\nfit = ols('lifeExp ~ C(year) + C(continent)', data=gapminder_2_by_2).fit() \n\nfit.summary()\n\nlm_4_continents_aov_table = sm.stats.anova_lm(fit, typ=2)\nlm_4_continents_aov_table\n\n\n\n\nmanual calculation of f-value for 2 x 2\n\nRPython\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\noverallMeanLifeExp = mean(gapminder_2_by_2$lifeExp)\ntotalVar = sum((gapminder_2_by_2$lifeExp - mean(gapminder_2_by_2$lifeExp))^2)\ngapminder_2_by_2 %>%\n  group_by(continent, year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> year_continent_means\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nyear_continent_means\n\n# A tibble: 4 × 5\n# Groups:   continent [2]\n  continent  year mean_lifeExp countries betweenSS\n  <fct>     <int>        <dbl>     <int>     <dbl>\n1 Africa     2002         53.3        52     4396.\n2 Africa     2007         54.8        52     3094.\n3 Europe     2002         76.7        30     6033.\n4 Europe     2007         77.6        30     6866.\n\nsum(year_continent_means$betweenSS)\n\n[1] 20389.47\n\ntotalVar\n\n[1] 30311.89\n\nsum(year_continent_means$betweenSS)/totalVar\n\n[1] 0.6726556\n\n(sum(year_continent_means$betweenSS))/totalVar\n\n[1] 0.6726556\n\ndf_total <- length(gapminder_2_by_2$country) - 1\ndf_res <- length(gapminder_2_by_2$country) - \n  1 - #data points\n  3   # predictors\n\nss_res = totalVar - sum(year_continent_means$betweenSS)\n\n1 - (ss_res/df_res)/(totalVar/df_total)\n\n[1] 0.6665179\n\n##\n# break down of types of variance\n##\ncontinent_df <- gapminder_2_by_2 %>%\n  group_by(continent) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(continent_df$betweenSS)\n\n[1] 20318.97\n\nyear_df <- gapminder_2_by_2 %>%\n  group_by(year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(year_df$betweenSS)\n\n[1] 67.79278\n\n##\n# interaction\n##\nsum(year_continent_means$betweenSS) - sum(continent_df$betweenSS) - sum(year_df$betweenSS)\n\n[1] 2.70036\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\n20319 +\n68 +\n3\n\n[1] 20390\n\n\n\n\n\nfrom siuba import group_by, summarize, _\n\noverallMeanLifeExp = gapminder_2_by_2['lifeExp'].mean()\n\ntotalVar = sum((gapminder_2_by_2['lifeExp'] - gapminder_2_by_2['lifeExp'].mean())**2)\n\n\nyear_continent_means=(gapminder_2_by_2\n  >> group_by(_.continent, _.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_continent_means['betweenSS'] = year_continent_means['countries'] * ((overallMeanLifeExp - year_continent_means['mean_lifeExp'])**2)\n\nprint(tabulate(year_continent_means, headers=year_continent_means.head(), tablefmt=\"fancy_grid\",showindex=False))\n\nsum(year_continent_means['betweenSS'])\n\ntotalVar\n\nsum(year_continent_means['betweenSS'])/totalVar\n\ndf_total = len(gapminder_2_by_2['country']) - 1\ndf_res = len(gapminder_2_by_2['country']) - 1 - 3\nss_res = totalVar - sum(year_continent_means['betweenSS'])\n1 - (ss_res/df_res)/(totalVar/df_total)\n\ncontinent_df=(gapminder_2_by_2\n  >> group_by(_.continent)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\ncontinent_df['betweenSS'] = continent_df['countries'] * ((overallMeanLifeExp - continent_df['mean_lifeExp'])**2)\nsum(continent_df['betweenSS'])\n\nyear_df=(gapminder_2_by_2\n  >> group_by(_.year)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nyear_df['betweenSS'] = year_df['countries'] * ((overallMeanLifeExp - year_df['mean_lifeExp'])**2)\nsum(year_df['betweenSS'])\n\nsum(year_continent_means['betweenSS']) - sum(continent_df['betweenSS']) - sum(year_df['betweenSS'])\n\nprint(tabulate(gapminder[:10], headers=gapminder.head(), tablefmt=\"fancy_grid\",showindex=False))\n\n20319 +68 +3\n\n\n\n\n\n\n3 way ANOVA\n\nRPython\n\n\n\ngapminder_2_by_2$pop_split = \"high\"\ngapminder_2_by_2$pop_split[gapminder_2_by_2$pop < median(gapminder_2_by_2$pop)] = \"low\"\n\ngapminder_2_by_2 %>%\n  group_by(continent, year, pop_split) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> three_way_summary\n\n`summarise()` has grouped output by 'continent', 'year'. You can override using\nthe `.groups` argument.\n\nsum(three_way_summary$betweenSS)\n\n[1] 20403.63\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n                                                  Df Sum Sq Mean Sq F value\nfactor(year)                                       1     68      68   1.067\nfactor(continent)                                  1  20319   20319 319.911\nfactor(pop_split)                                  1     13      13   0.212\nfactor(year):factor(continent)                     1      3       3   0.046\nfactor(year):factor(pop_split)                     1      0       0   0.000\nfactor(continent):factor(pop_split)                1      0       0   0.007\nfactor(year):factor(continent):factor(pop_split)   1      0       0   0.000\nResiduals                                        156   9908      64        \n                                                 Pr(>F)    \nfactor(year)                                      0.303    \nfactor(continent)                                <2e-16 ***\nfactor(pop_split)                                 0.646    \nfactor(year):factor(continent)                    0.830    \nfactor(year):factor(pop_split)                    0.989    \nfactor(continent):factor(pop_split)               0.931    \nfactor(year):factor(continent):factor(pop_split)  0.988    \nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent) * factor(pop_split), \n    data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5189  -4.8868  -0.3127   3.1208  22.0864 \n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   52.96628\nfactor(year)2007                                               1.53805\nfactor(continent)Europe                                       23.52019\nfactor(pop_split)low                                           0.69131\nfactor(year)2007:factor(continent)Europe                      -0.59779\nfactor(year)2007:factor(pop_split)low                         -0.06377\nfactor(continent)Europe:factor(pop_split)low                  -0.26305\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low  0.07923\n                                                              Std. Error\n(Intercept)                                                      1.59392\nfactor(year)2007                                                 2.21201\nfactor(continent)Europe                                          2.60286\nfactor(pop_split)low                                             2.21201\nfactor(year)2007:factor(continent)Europe                         3.65535\nfactor(year)2007:factor(pop_split)low                            3.12825\nfactor(continent)Europe:factor(pop_split)low                     3.65535\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    5.16944\n                                                              t value Pr(>|t|)\n(Intercept)                                                    33.230  < 2e-16\nfactor(year)2007                                                0.695    0.488\nfactor(continent)Europe                                         9.036 5.91e-16\nfactor(pop_split)low                                            0.313    0.755\nfactor(year)2007:factor(continent)Europe                       -0.164    0.870\nfactor(year)2007:factor(pop_split)low                          -0.020    0.984\nfactor(continent)Europe:factor(pop_split)low                   -0.072    0.943\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low   0.015    0.988\n                                                                 \n(Intercept)                                                   ***\nfactor(year)2007                                                 \nfactor(continent)Europe                                       ***\nfactor(pop_split)low                                             \nfactor(year)2007:factor(continent)Europe                         \nfactor(year)2007:factor(pop_split)low                            \nfactor(continent)Europe:factor(pop_split)low                     \nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.97 on 156 degrees of freedom\nMultiple R-squared:  0.6731,    Adjusted R-squared:  0.6585 \nF-statistic: 45.89 on 7 and 156 DF,  p-value: < 2.2e-16\n\n20319 +\n68 +\n3 + \n  13\n\n[1] 20403\n\n20403.63\n\n[1] 20403.63\n\n\n\n\n\ngapminder_2_by_2['pop_split']=0\n\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']<np.median(gapminder_2_by_2['pop'])]=\"low\"\ngapminder_2_by_2['pop_split'].loc[gapminder_2_by_2['pop']>=np.median(gapminder_2_by_2['pop'])]=\"high\"\n\nthree_way_summary = (gapminder_2_by_2\n  >> group_by(_.continent, _.year, _.pop_split)\n  >> summarize(mean_lifeExp = _.lifeExp.mean(),\n              countries = _.lifeExp.count())\n              )\nthree_way_summary['betweenSS'] = three_way_summary['countries'] * ((overallMeanLifeExp - three_way_summary['mean_lifeExp'])**2)\nsum(three_way_summary['betweenSS'])\n\n\n\nfit2 = ols('lifeExp ~ C(year) + C(continent) + C(pop_split)', data=gapminder_2_by_2).fit() \n\nfit2.summary()"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html",
    "href": "GeneralLinearModels/TTests.html",
    "title": "T-Tests(incomplete)",
    "section": "",
    "text": "In all general linear models you are trying to compare how much of the variance is explained by a model compared to what’s not being explained by a model. In short\n\\[\n\\frac{var_{explained}}{var_{unexplained}} = \\frac{SS_{explained}}{SS_{unexplained}}\n\\]\nFor each type of t-test, the way we calculate this is slightly different:"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "href": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "One-Sample t-tests",
    "text": "One-Sample t-tests\n\nGLM approach\nOne sample t-tests try to explain whether variance of data is better explained around one specific value (sample mean) compared to another (previously assumed value). For example, imagine that you wanted to test whether life expectancy is higher than 55 across the world:\n\nYour \\(\\mu\\) would be 55. This can be thought of as the assumed population mean that we want to use our sample to test.\nYour \\(\\bar{x}\\) would be the sample mean.\n\nLet’s visualise these values using gapminder data from 2007:\n\nlibrary(ggplot2)\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007\n)\nggplot(gapminder_2007, aes(x=year,y=lifeExp)) + \n  geom_jitter() + \n  xlab(\"\") + \n  theme(axis.text.x = element_blank()) +\n  theme(axis.ticks.x = element_blank()) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = 55,\n      yend = 55,\n      color = \"Mu\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = mean(lifeExp),\n      yend = mean(lifeExp),\n      color = \"Sample Mean\"\n    )\n  )\n\n\n\n\nWe want to create a model that explains any variance around the population mean (\\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\nY is the data point value you are trying to predict. Note that for this formula you will always have the same prediction.\n\\(\\bar{y}\\) is mean of y. You are only interested in whether predicting y based on y’s mean captures a significant amount of the variance of the y-values around the \\(\\mu\\).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the “population” mean (and will also suggested that there is significant reason to reject the population mean). The total variance using sum of squares is thus:\n\\[\nSS_{total} = \\sum(x_i-\\mu)^2\n\\]\nWhich for the above data would give us:\n\nsum((gapminder_2007$lifeExp - 55)^2)\n\n[1] 41025.16\n\n\nSo your explained variance by this model is any difference between the Mu (\\(\\mu\\)) and the sample mean (\\(\\bar{x}\\)). To summarise this using sum of squares, for each data point you subtract the two from each other and square them, as this difference is what we can explain of variance away from the MU:\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n[1] 20473.3\n\n\nUnexplained variance would be the residuals around the sample mean, as this is variance that is not explained by the model. Conveniently, we can calculate the sum of squared around the sample mean quite elegantly:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n[1] 20551.85\n\n\nSo the F-value should be:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n[1] 140.4611\n\n\nF-values are squares of t-values, so let’s see if this is true here also:\n\nsqrt(f_value)\n\n[1] 11.85163\n\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\nT is the t-value\nF is the f-value\n\\(SS_{explained}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexplained}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{explained}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels)."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Paired samples t-tests",
    "text": "Paired samples t-tests\nPaired samples t-tests can be approached like 1-sample t-tests, but you first of all need to collapse the data to have a single variable to compare to a \\(\\mu\\) of zero. Let’s do this for gapminder data, comparing life expectancies between 2002 and 2007:\n\ngapminder_2002_2007_life_exp <- gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]\nt.test(gapminder_2002_2007_life_exp, mu = 0)\n\n\n    One Sample t-test\n\ndata:  gapminder_2002_2007_life_exp\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean of x \n   1.3125 \n\n\nThe above suggests that life expectancy was significanctly different. Let’s see if we get the exact same value when we use a paired t-test in R:\n\nt.test(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002], paired=T)\n\n\n    Paired t-test\n\ndata:  gapminder$lifeExp[gapminder$year == 2007] and gapminder$lifeExp[gapminder$year == 2002]\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean difference \n         1.3125 \n\n\nLooks identical. Let’s compare formulas to see why this is:\n\\[\nt_{paired} = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sigma_{pooled}/\\sqrt{N}} = \\frac{\\bar{x_3}}{\\sigma_{pooled}/\\sqrt{N}}\n\\]\nWhere\n\n\\(\\bar{x_1}\\) is the mean of condition 1\n\\(\\bar{x_2}\\) is the mean of condition 2\n\\(\\bar{x_3}\\) is the mean of the result you get when you subtract condition 2 from condition 1 for each participant, i.e. \\(mean(x_1-x_2)\\).\n\\[\n\\sigma_{pooled}  = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}} OR \\frac{\\sum(x_1 - x_2)^2}{N-1}\n\\] One way effectively gets the average of the standard deviations of condition and 1. The second way gets the standard deviation of the differences between conditions 1 and 2. Both give you the same outcome.\n\\(N\\) is the number of participants\n\nYou can rewrite the above formula to compare \\(\\bar{x_3}\\) to \\(\\mu\\), as we know \\(\\mu\\) is zero, which would make this formula (effectively) identical to the one above for one-sample t-tests:\n\\[\n\\frac{\\bar{x_3} - \\mu}{\\sigma_{pooled}/\\sqrt{N}}\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Independent Samples t-tests",
    "text": "Independent Samples t-tests\n\nGLM approach\nFor an independent samples t-test we can create a simple model based on the means of the two groups. You can either dummy or effect code the groups, so we’ll do both to look at how the output is slightly different each way. We’ll use the gapminder data to see if there are differences in life expectancies between the Americas and Europe in 2007 to illustrate these:\n\ngapminder_2007_Am_Eu <- subset(\n  gapminder,   # the data set\n  year == 2007 & continent == \"Americas\" | \n  year == 2007 & continent == \"Europe\"\n)\n\n\nDummy coding\nOne way to make a model for a t-test is to have a variable that is 1 for one level, and 0 for the other level (note that this gets more complicated if you are going an ANOVA with 3 or more levels). Let’s create a new variable for continent that is 1 if the country is in the Americas, and 0 if it’s not:\n\ngapminder_2007_Am_Eu$americas_dummy = ifelse(gapminder_2007_Am_Eu$continent == \"Americas\", 1,0)\nrmarkdown::paged_table(gapminder_2007_Am_Eu)\n\n\n\n  \n\n\n\nNow that we have added our dummy code, we can write a model for what we expect life expectancy to be for each country:\n\\[\nlifeExp = \\beta_{americas}*mean(lifeExp_{americas}) + \\beta_{europe}*mean(lifeExp_{europe}) + e\n\\]\n\n\nEffect coding"
  },
  {
    "objectID": "categorical/ contingency.html",
    "href": "categorical/ contingency.html",
    "title": "Contingency(incomplete)",
    "section": "",
    "text": "library(psych)\ncontingency_data <- matrix(\n  data = c(45,55,120,80),\n  nrow = 2,\n  ncol = 2\n)\ncontingency_data\n\n     [,1] [,2]\n[1,]   45  120\n[2,]   55   80\n\nmanual_phi = (\n  contingency_data[1,1] * contingency_data[2,2] - \n    contingency_data[1,2] * contingency_data[2,1]\n  )/sqrt(\n    sum(contingency_data[1,]) *\n    sum(contingency_data[2,]) *\n    sum(contingency_data[,1]) *\n    sum(contingency_data[,2])\n  )\nphi(contingency_data, digits = 7)\n\n[1] -0.1421338\n\nmanual_phi\n\n[1] -0.1421338\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics (incomplete)",
    "section": "",
    "text": "In statistics, a variable is any (measurable) attribute that describes any organism or object. It’s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object.\nWhat is a hypothesis? A(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e. randomly) if your main hypothesis were not true. We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\nIt is the p-value threshold that identifies if a result is “significant” or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is “significant” (i.e. so unlikely that this would have happened by chance that we conclude this didn’t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e. we say there is an effect when there isn’t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "href": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is power (and what is the beta value)",
    "text": "What is power (and what is the beta value)"
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Scientific notation is used to express very large or very small numbers in a concise format using exponents of 10 (\\(10^n\\)). So, rather than having to show all of the digits for \\(4,600,000,000\\) (4.6 Trillion) it can be expressed in scientific notation: \\(4.6 \\times 10^9\\). Likewise, very small numbers can be expressed using the same format, e.g. \\(0.0000005\\) = \\(5.0 \\times 10^{-7}\\).\nNote: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n3.1e3 == 3.1 * 10^3\n\n[1] TRUE\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n2.5e-3 == 2.5 * 10^(-3)\n\n[1] TRUE"
  },
  {
    "objectID": "templates/ojs.html",
    "href": "templates/ojs.html",
    "title": "ojs",
    "section": "",
    "text": "Note that you won’t be able to preview figures created using this\n\ndata = FileAttachment(\"palmerpenguins.csv\").csv({ typed: true })\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\nathletes = FileAttachment(\"athletes.csv\").csv({typed: true})\n\n\ndotplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndotplot = Plot.dot(athletes, {x: \"weight\", y: \"height\", stroke: \"sex\"}).plot()\n\n\n//dotplot.legend(\"color\")\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"bill_depth_mm\", stroke: \"species\"}).plot()\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"count\", stroke: \"sex\"}).plot()\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  )\n).plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "templates/tabsets.html",
    "href": "templates/tabsets.html",
    "title": "tabsets template",
    "section": "",
    "text": "You can use Python and R within Quarto files\n\nRPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n\n1 + 2\n\n3"
  },
  {
    "objectID": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "href": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "title": "Multi-collinearity (incomplete)",
    "section": "Measuring multi-collinearity using Variance Inflation Factor (VIF)",
    "text": "Measuring multi-collinearity using Variance Inflation Factor (VIF)\n\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\ngdp_pop_predict_lifeExp <- lm(\n  formula = lifeExp ~ pop + gdpPercap,\n  data = gapminder_2007\n    \n)\n\n\ngdp_pop_predict_lifeExp$coefficients\n\n (Intercept)          pop    gdpPercap \n5.920520e+01 7.000961e-09 6.416085e-04 \n\npred_lm <- lm(lifeExp ~ pop, gapminder_2007)\npred_lm$coefficients[2]\n\n         pop \n3.889069e-09 \n\n1-sqrt(pred_lm$coefficients[2])\n\n      pop \n0.9999376 \n\nvif(gdp_pop_predict_lifeExp)\n\n      pop gdpPercap \n 1.003109  1.003109"
  },
  {
    "objectID": "regressions/simpleRegressions.html",
    "href": "regressions/simpleRegressions.html",
    "title": "Simple regression (incomplete)",
    "section": "",
    "text": "Simple regression, also known as linear regression, builds on correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression can’t tell us exactly what is influencing what (i.e. whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\nGradient reflects how steep the line is\nIntercept is the point at which the regression line crosses the y-axis\n\nLet’s use coding magic to find out the intercept and the gradient (AKA slope):\n\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nThe above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more useful terms, for every extra $10,000 dollars per person, the life expectancy goes up by 6 years).\nFor the above equation we will always retrieve values from the graph, except residuals, which is the ‘error’ and so a more complete formula for the outcome can be represented by the following formula\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nResidual reflects what’s left over, and is not represented in the line of best fit formula because you can’t predict what’s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\nThese residuals can be thought of the error, i.e. what the model failed to predict. In more mathematical terms, the model would be:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "regressions/simpleRegressions.html#notes-for-anthony",
    "href": "regressions/simpleRegressions.html#notes-for-anthony",
    "title": "Simple regression (incomplete)",
    "section": "Notes for Anthony",
    "text": "Notes for Anthony\nR2 = SSR/SSTO = 1 - SSE/SSTO\n\nsse  = sum((gapminder_2007$lifeExp - gapminder_2007$fitted)^2)\nssto = sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\nrsqr = 1-(sse/ssto)\n\nIf your model is minimising the error, then what happens if you have 2 predictors:\n\\[\nY = a + b_1X_1 + b_2X_2 + e\n\\]\nSave the residuals after 1 correlation:\n\nres_gdp = gapminder_2007$lifeExp - gapminder_2007$fitted\ncor.test(gapminder_2007$pop, res_gdp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and res_gdp\nt = 1.3842, df = 140, p-value = 0.1685\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.04948127  0.27564447\nsample estimates:\n      cor \n0.1161931"
  },
  {
    "objectID": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "href": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "title": "Simple regression (incomplete)",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\nIn correlations we discussed how the strength of association is the proportion of variance of y explained by x. For simple regression, this is also the case:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply the above formula to see what R is for \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap))\n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) \n  )\n\n[1] 0.6786624\n\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)\n\n[1] 0.6786624\n\n# r^2\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)^2 \n\n[1] 0.4605827\n\n\nWe can confirm that squaring R gives r^2\n\nsummary(life_expectancy_model)\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.828  -6.316   1.922   6.898  13.128 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(>|t|)    \n(Intercept) 59.56565008  1.01040864   58.95 <0.0000000000000002 ***\ngdpPercap    0.00063713  0.00005827   10.93 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.899 on 140 degrees of freedom\nMultiple R-squared:  0.4606,    Adjusted R-squared:  0.4567 \nF-statistic: 119.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nNow lets see if the same logic works when there are 2 or more predictors that interact to predict each other:\n\\[\nr = \\frac{var_{xyz}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})(z_i-\\bar{z})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2 * \\sum(z_i-\\bar{z})^2}}\n\\]\n\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap)) *\n  (gapminder_2007$pop-mean(gapminder_2007$pop))  \n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n    sum((gapminder_2007$pop - mean(gapminder_2007$pop))^2) \n  )\n\n[1] -0.007330224\n\n\n\nsummary(lm(lifeExp ~  gdpPercap + pop, data = gapminder_2007))\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap + pop, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                   Estimate      Std. Error t value            Pr(>|t|)    \n(Intercept) 59.205198140717  1.040398672164  56.906 <0.0000000000000002 ***\ngdpPercap    0.000641608517  0.000058176209  11.029 <0.0000000000000002 ***\npop          0.000000007001  0.000000005068   1.381               0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.87 on 139 degrees of freedom\nMultiple R-squared:  0.4679,    Adjusted R-squared:  0.4602 \nF-statistic: 61.11 on 2 and 139 DF,  p-value: < 0.00000000000000022\n\n\n\nWhy use regression?\nRegression builds on correlation by providing a more detailed view of your data and with this provides an equation that can be used for any future predicting and optimizing of your data.\n\n\n[1] 4\n\n\nThe differences between regression and correlation"
  },
  {
    "objectID": "excelIntro/averageIfs.html",
    "href": "excelIntro/averageIfs.html",
    "title": "Averageifs",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the averageifs tab at the bottom of excel.\nSometimes it will be helpful to average only specific values in a column. One way to do this is using an \"averageifs\" formula.\nLet us imagine we want separate average response times for male and female participants. We can use a formula that lets us use one column to identify which rows we want to average in another column. So if we want to average the response time for females, we're only interested in the rows with female data:\n\nSo here’s how an \"averageifs\" formula could look:\n\nSo let's break this down. This averageifs formula has three inputs:\n\nThe cells that will be averaged - B2:B11\nA set of cells which will be used to determine which rows are selected: C2:C11\nA value that is compared with the previous set of cells to determine which rows are selected: \"Female\"\n\nIn practice, the above formula does this:\n\nSo you now have a formula for calculating the mean scores based on one criterion, in this case gender. Let's consolidate this by you answering the following questions\n\nviewof question_1_response = Inputs.number([0,500], {label: \"Mean female ms\", step:1});\ncorrect_female_ms = 413;\n\nquestion_1_result = { \n  if(question_1_response == correct_female_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_2_response = Inputs.number([0,500], {label: \"Mean male ms\", step:1});\ncorrect_male_ms = 408;\n\nquestion_2_result = { \n  if(question_2_response == correct_male_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \nNow that you have calculated the average score based on one criterion, we can calculate it based on two criterion! It's like the original formula, but we add another range of cells, and another value to compare the cells to.\nSo let us imagine we want the average response time for females with a mobile phone? We would use our original formula (see above), but before closing it we would add the cells referring to whether a mobile phone is present (B2:B11), and then compare these cells to the word \"yes\". Let's do that:\n\nJust to really consolidate what is going on, like before, we focussed only on female participants:\n\nBut are now also focusing on rows in which a mobile phone is present (as indicated by \"yes\")\n\nSo now you should be able to calculate the response times for all four groups of participants:\n\nviewof question_3_response = Inputs.number([0,700], {label: \"Mean female using phones\", step:1});\ncorrect_female_yes_ms = 385;\n\nquestion_3_result = { \n  if(question_3_response == correct_female_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_4_response = Inputs.number([0,700], {label: \"Mean female NOT using phones\", step:1});\ncorrect_female_no_ms = 457;\n\nquestion_4_result = { \n  if(question_4_response == correct_female_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_5_response = Inputs.number([0,700], {label: \"Mean male using phones\", step:1});\ncorrect_male_yes_ms = 612;\n\nquestion_5_result = { \n  if(question_5_response == correct_male_yes_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… \n\nviewof question_6_response = Inputs.number([0,700], {label: \"Mean male NOT using phones\", step:1});\ncorrect_male_no_ms = 103;\n\nquestion_6_result = { \n  if(question_6_response == correct_male_no_ms){\n    return \"Correct!\"\n  } else {\n    return \"Missing or incorrect. Perhaps you forgot to round up?\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is…"
  },
  {
    "objectID": "excelIntro/formulas.html",
    "href": "excelIntro/formulas.html",
    "title": "Formulas",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here.\nBy using formulas we can automate a lot of processing in excel.\nAll you need to do to make a cell a formula, is to put an “=” sign at the start of it.\nLet’s start by going to the worksheet Formulas (you might already be there, or need to scroll left in the tabs at the bottom of the page):\n\nYou’ll see that there’s a hypothetical situation in which someone has done two runs, and wants to calculate the total distance they’ve run. We can use the following formula to add the values in each of the cells together:\n\nYou can see that you can just add cell B2 to cell B3 to get the total added together.\n\nYour turn\nDo this yourself, and complete the following:\nThe total distance run was…\n\nviewof question_0_response = Inputs.number([0,100], {label: \"\", step:.01});\ncorrect_q0 = 33.37;\n\nquestion_0_result = { \n  if(question_0_response == correct_q0){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s use this logic to work out some other values on that sheet:\nThe total distance run by James was…\n\nviewof question_1_response = Inputs.number([0,100], {label: \"\", step:.0005});\ncorrect_q1 = 58.3535;\n\nquestion_1_result = { \n  if(question_1_response == correct_q1){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElaine ran…\n\nviewof question_2_response = Inputs.number([0,100], {label: \"\", step:.333});\ncorrect_q2 = 16.333;\n\nquestion_2_result = { \n  if(question_2_response == correct_q2){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe total distance run by Dianna was…\n\nviewof question_3_response = Inputs.number([0,100], {label: \"\", step:.002});\ncorrect_q3 = 32.184;\n\nquestion_3_result = { \n  if(question_3_response == correct_q3){\n    return \"Correct!\";\n  } else {\n    return \"Missing or incorrect.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeel free to play around with these formulas a bit more. A really good way to learn excel is to play around with what you’ve learned to build up confidence with it."
  },
  {
    "objectID": "excelIntro/sum.html",
    "href": "excelIntro/sum.html",
    "title": "Sum and SumIf",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. Go to the sum tab at the bottom of excel:\n\nIn this example, we would like to calculate the total response times, known as sum. To start with, let’s calculate the SUM for all participants in this spreadsheet:\n\nSo to calculate the SUM (i.e. total of the selected cells added together), you write a formula as follows:\n=sum(START:END)\nNow because everything is lined up to have all the mobile users first, and then the non-mobile users, so you can just use the sum function to select the rows where it is “yes” for mobile phone users. It’s not always practical to align your data column by column, so sumifs is a helpful function. Here’s a formula you could run to calculate the sum for females:\n\nThe excel formula’s structure:\n=sumifs(cells_you_want_to_sum, selection_crition_cells_1, selction_criterion_1)\nYou can have as many criterion as you like. Let’s look at females with mobile phones:\n\nYou may have noticed 2 things:\n\nYou just add an extra criteria column (in this case column D) and a new criteria to compare it to (“yes”) to allow you to have an extra criteria to select your rows on\nThis example compares column C to I7 rather than the word “Females”. Referring to a specific cell can help make your formulas more efficient, as you don’t have to repeatedly type the same word again and again. In fact, arguably it would be better to have anchored I$7 so that you could copy the formula down."
  },
  {
    "objectID": "excelIntro/if.html",
    "href": "excelIntro/if.html",
    "title": "If function",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "excelIntro/if.html#running-code",
    "href": "excelIntro/if.html#running-code",
    "title": "If function",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "excelIntro/count.html",
    "href": "excelIntro/count.html",
    "title": "Count and countifs function",
    "section": "",
    "text": "The spreadsheet this worksheet is based on can be downloaded from here. You can find the relevant tab below:\n\nThis function allows you to count the number of cells in a selection.\nLike all formulas, it begins with an “=” sign.\nLet’s use this to count the number of participants you have (even though in this case we already know it is 10). The general formula is:\n=count(START:END)\nSo to calculate the number of participants, we would write:\n\nAn important thing to know about count formulas is that they only count cells with numbers in them. So the following would get zero:\n\nIf you want to count the number of phone and non-phone users, you can use countifs. This allows you to count how many occurrences there are of a value you are looking for. You can do this for just one column looking for just one value, or multiple columns looking for multiple values.\nThe general formula if you are just looking for one value in one column:\n=countifs(column_1,value_1)\nIf you wanted to know the number of phone users, you could type:\n=countifs(D2:D11,\"yes\")\nHowever, as there is nothing else in column D, it would be more elegant to refer to the whole column instead:\n=countifs(D:D,\"yes\")\nNow if we wanted to calculate how many females had a phone, you could use add a second column and compare a second value to it:\n=countifs(D:D,\"yes\",C:C,\"Female\")\n\nYour turn\nUse countif formulas to calculate the following. Your forumas should get you the same numbers as listed below:\n\n\n\nSex\nPhones\nNo Phones\nTotal\n\n\n\n\nFemales\n3\n2\n5\n\n\nMales\n3\n2\n5"
  },
  {
    "objectID": "jast_setup.html",
    "href": "jast_setup.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "distributions/skewness.html",
    "href": "distributions/skewness.html",
    "title": "Skewness",
    "section": "",
    "text": "Parametric analyses are based on the assumption that the data you are analysing is normally distributed (see below):\nWhite in the above figure represents the median. Note that the mean and median overlap in a normal distribution.\nIf your data fits a normal distribution, then you can draw conclusions based on certain facts about this distribution, e.g. the fact that 97.7% of your population should have a score that is more negative than +2 standard deviations above the mean (because Z-scores represent standard deviations from the mean). As a result, if your data is skewed:\nWhite represents the median in the figure above. As you can see from the above skewed distribution, the median is below the mean, consistent with the data being skewed. Importantly, the assumptions that we can make about what proportion of the population are 1 standard deviation above and below the mean are no longer valid, as more than half the population are below the mean in this case. This would suggest that non-parametric analyses could be more appropriate if your data is skewed.\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is.\nThe next section is a breakdown of the formula for those interested in it (but this is not crucial)."
  },
  {
    "objectID": "distributions/skewness.html#consolidation-questions",
    "href": "distributions/skewness.html#consolidation-questions",
    "title": "Skewness",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_skew_no = Math.round(Math.random() * 400)/100;\n\n\n\n\n\n\nIs a skewness z-score of  indicative of a significant problem of skewness?\n\nviewof question_1_response = Inputs.radio([\"Yes\", \"No\"], {label: \"\", value: \"A\"});\nthis_result = { \n  var question_1_result = \"awaiting response\";\n\n  if(rand_skew_no > 1.96){\n    if(question_1_response == \"Yes\"){\n      question_1_result = \"Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    } else if(question_1_response == \"No\") {\n      question_1_result = \"Not Correct - Z scores above 1.96 suggest significant problems with skewness\";\n    }\n  } else {\n    if(question_1_response == \"Yes\"){\n      question_1_result = \"Not Correct - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    } else if(question_1_response == \"No\") {\n      question_1_result = \"Correct  - Z scores below 1.96 **do not** suggest significant problems with skewness\";\n    }\n  }\n  return question_1_result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is"
  },
  {
    "objectID": "distributions/ttests.html",
    "href": "distributions/ttests.html",
    "title": "t-tests (incomplete)",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for …\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "Kurtosis refers to how influenced a distribution is by its tails.\n\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\nIs Platykurtic vs. leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "distributions/transforming.html",
    "href": "distributions/transforming.html",
    "title": "Transforming Data (incomplete)",
    "section": "",
    "text": "A lot of analyses is dependent on data being normally distributed. One problem with your data might be that it is skewed. Lets focus on the gapminder data from 2007 to see if the \\(gdp\\) and \\(life\\) \\(expectancy\\) data is skewed, and how this could be addressed.\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nSo it looks like both the \\(gdp\\) and \\(life\\) \\(expectancy\\) are skewed (as their z-scores are greater than 1.96). Lets double check with a quick plot:\n\nplot(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n\n\nIt’s relatively easy to see the skewness of \\(gdp\\), but \\(life\\) \\(expectancy\\) is a bit more subtle. We can complete a logarithmic transformation to reduce the skewness, so lets do that to both variables and then replot the data:\n\ngapminder_2007$gdpPercap_log <- log(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_log <- log(gapminder_2007$lifeExp)\nplot(\n  gapminder_2007$gdpPercap_log,\n  gapminder_2007$lifeExp_log\n)\n\n\n\n\nLets check if the skewness has changed for the \\(gdp\\):\n\n# original gdp\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\n# transformed gdp (log)\nspssSkewKurtosis(gapminder_2007$gdpPercap_log)\n\n           estimate        se     zScore\nskew     -0.1540524 0.2034292 -0.7572778\nkurtosis -1.1256815 0.4041614 -2.7852277\n\n\nSo, transforming the \\(gdp\\) did reduce skewness but increased Kurtsosis, so beware that applying a transformation may cause other problems! Lets check whether the log transformation reduced skewness for \\(life\\) \\(expectancy\\):\n\n# original life expectancy\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n# transformed life expectancy\nspssSkewKurtosis(gapminder_2007$lifeExp_log)\n\n           estimate        se    zScore\nskew     -0.9043617 0.2034292 -4.445584\nkurtosis -0.4136699 0.4041614 -1.023527\n\n\nSeems like the answer is no.\nAn important question is whether the associations between your variables change after transformation, so let’s check that next:\n\n# correlation on original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation on transformed data\ncor.test(\n  gapminder_2007$gdpPercap_log,\n  gapminder_2007$lifeExp_log\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_log and gapminder_2007$lifeExp_log\nt = 14.752, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7060729 0.8372165\nsample estimates:\n      cor \n0.7800706 \n\n\nThe log transformed data is more strongly associated with each other than the original data. However, there are other transformations that are additive (plus or minus a value to the original data) or multiplicative (which can also include division) that will not change the association (check whether the r-value \\(cor\\) changes):\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data + 5 to one variable\ncor.test(\n  gapminder_2007$gdpPercap + 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap + 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data - 10 to one variable\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp - 10\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp - 10\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with multiplication of 5 to one variable\ncor.test(\n  gapminder_2007$gdpPercap * 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap * 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\nSome transformations will change the association if you apply them to one variable:\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with one squared variable\ncor.test(\n  gapminder_2007$gdpPercap ^2,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap^2 and gapminder_2007$lifeExp\nt = 7.8576, df = 140, p-value = 9.372e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4274353 0.6579774\nsample estimates:\n      cor \n0.5532109 \n\n\nbut not if you apply similar transformations to both variables\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with both variables squared\ncor.test(\n  sqrt(gapminder_2007$gdpPercap),\n  sqrt(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(gapminder_2007$gdpPercap) and sqrt(gapminder_2007$lifeExp)\nt = 12.981, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6539524 0.8057025\nsample estimates:\n      cor \n0.7390648"
  },
  {
    "objectID": "distributions/normal.html",
    "href": "distributions/normal.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "Parametric statistics often compare values to a normal distribution of expected data, based on the estimated mean and SD. Lets start by showing a (made up) normal distribution of heights in centimeters:\nSo lets say the average person’s height is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:\n\nRPythonExcelJASPSPSS\n\n\n\n# Plot a normal distribution of heights\npopulation_heights_x <- seq(\n  120,    # min\n  180,    # max\n  by = 1  \n)\npopulation_heights_y <- dnorm(\n  population_heights_x,\n  mean = 150,\n  sd   = 10\n)\nplot(\n  population_heights_x,\n  population_heights_y,\n  xlab = \"height\",\n  ylab = \"frequency\"\n)\n# Add line to show mean and median\nabline(\n  v=150,  # where the line for the mean will be \n  lwd=5\n)\n\n\n\n\n\n\n\n# Plot a normal distribution of heights\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\npopulation_heights_x = [x for x in range(120, 181, 1)]\n\n# vector for the y-axis\npopulation_heights_y = norm.pdf(population_heights_x, loc=150, scale=10)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n\n# plot\nplt.scatter(population_heights_x, population_heights_y, color='w', edgecolors='black')\n\n# Add line to show mean and median\nplt.axvline(x=150, color='black', ls='-', lw=5)\n\n# add title on the x-axis\nplt.xlabel(\"height\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n# show the plot\nplt.show()\n\n# show plot\nplt.show()\n\n\n\n\nNormal Distribution\n\n\n\n\nDownload and open the normal.xlsx file in this repository to see data being used to create the below figure:\n(note that making a vertical line to reflect the mean didn’t seem as easy in Excel as other languages, so this hasn’t been added)\n\n\n\nNormal Distribution using Excel\n\n\n\n\nDownload and open height.csv in JASP, and then complete the following steps to generate the figure below:\n\n\nClick on the Descriptives panel\nadd height as a variable\nopen the Basic plots interface and then select Distribution plots and Display density\n\n\n\nDownload and open height.sav in SPSS and then complete the following steps to generate the figure below:\n\n\nSelect Analyze –> Descriptive Statistics –> Explore…\n\n\n\nMove the height variable to the dependent list, then select Plots… and choose Histogram as a descriptive figure\n\n\n\n\n\nYou can see that the above fits a bell-curve, and the middle represents both the mean and the median as the data is symmetrical. In reality, almost no data is a perfect bell-curve, but there are ways to test if the data isn’t sufficiently normal to use parametric tests with.\nNext, we will look at how normal distributions allow you to transform your data to z-scores to compare to a z-distribution."
  },
  {
    "objectID": "distributions/normal.html#z-scores-and-the-z-distribution",
    "href": "distributions/normal.html#z-scores-and-the-z-distribution",
    "title": "Normal Distribution",
    "section": "Z-scores and the z-distribution",
    "text": "Z-scores and the z-distribution\nA z-score is a standardised value that captures how many standard deviations above or below the mean an individual value is. Thus, to calculate the z-score\n\\[\nZ = \\frac{individualScore-meanScore}{StandardDeviation}\n\\]\nOr in formal terminology:\n\\[\nZ = \\frac{x-\\bar{x}}{\\sigma}\n\\]\nThe calculated score can then be applied to a z-distribution, which is parametric/normally distributed. Lets have a look at a z-distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# vector for the x-axis\nz_score_x <- seq(\n  -3,    # min\n  3,    # max\n  by = .1  \n)\n\n# vector for the y-axis\nz_score_y <- dnorm(\n  z_score_x,\n  mean = 0,\n  sd   = 1\n)\n\nplot(\n  z_score_x,\n  z_score_y,\n  xlab = \"z-score (SDs from the mean)\",\n  ylab = \"frequency\"\n)\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\n# vector for the x-axis\nz_score_x = np.arange(-3.0, 3.1, 0.1)\n\n# vector for the y-axis\nz_score_y = norm.pdf(z_score_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(7, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\n# plot\nplt.scatter(z_score_x, z_score_y, color='w', edgecolors='black')\n\n# add title on the x-axis\nplt.xlabel(\"z-score (SDs from the mean)\")\n\n# add title on the y-axis\nplt.ylabel(\"frequency\")\n\n# show plot\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z - scores\n\n\n\n\nIt’s not clear that generating a z-distribution like this would be helpful to do with JASP, so this isn’t included.\n\n\nIt’s not clear that generating a z-distribution like this would be helpful to do in SPSS, so this isn’t included.\n\n\n\nIf you compare the height distribution above to the z-score distribution, you should see that they are identically distributed. This is useful, as we know what percentage of a population fits within each standard deviation of a normal distribution:\n\nRPythonExcelJASPSPSS\n\n\n\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_13.6 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n p +\n   \n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6, \n     aes(\n       x, \n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   \n   annotate(\n     \"text\", \n     x=0.5, \n     y=0.01, \n     label= \"34.1%\"\n   ) + \n   annotate(\n     \"text\", \n     x=1.5, \n     y=0.01, \n     label= \"13.6%\"\n   ) + \n   annotate(\n     \"text\", \n     x=2.3, \n     y=0.01, \n     label= \"2.3%\"\n   ) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='red', alpha=.4)\n\nplt.text(2.05, 0.001, '2.3%', fontsize = 10, color='black',weight='bold')\nplt.text(1.1, 0.001, '13.6%', fontsize = 10, color='black',weight='bold')\nplt.text(0.1, 0.001, '34.1%', fontsize = 10, color='black',weight='bold')\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_0_plus.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores with percentages for each boundary\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do with JASP, so this isn’t included.\n\n\nIt’s not clear that generating a distribution like this would be helpful to do in SPSS, so this isn’t included.\n\n\n\nThe above visualises how 34.1% of a population’s scores will be between 0 and 1 standard deviation from the mean, 13.6% of the population’s scores will be between 1 to 2 standard deviations above the mean, and 2.3% of the population will be more then 2 standard deviations above the mean. Remember that the normal distribution is symmetrical, so we also know that 34.1% of the population’s score will be between 0 to 1 standard deviations below the mean (or 0 to -1 SDs), 13.6% of the population’s score will be between -2 to -1 standard deviations from the mean, and 2.3% of the population’s score will be more negative than -2 standard deviations from the mean. Lets look at this cumulative distribution:\n\nRPythonExcelJASPSPSS\n\n\n\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"2.3%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"15.9%\") + \n   annotate(\"text\", x=-0.4, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nnorm_x = np.arange(-4.0, 4.1, 0.1)\n\nnorm_y = norm.pdf(norm_x, loc=0, scale=1)\n\nfig, ax = plt.subplots(figsize =(8, 5))\nax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n\nax.plot(norm_x, norm_y, color='black', alpha=1.00)\nax.fill_between(norm_x, norm_y, where= (norm_x < -2), color='gold', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-2 <= norm_x)&(norm_x <= -1), color='red', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (-1 <= norm_x)&(norm_x <= 0), color='green', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (0 <= norm_x)&(norm_x <= 1), color='cyan', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (1 < norm_x)&(norm_x <= 2), color='blue', alpha=.4)\nax.fill_between(norm_x, norm_y, where= (norm_x > 2), color='purple', alpha=.4)\n\nplt.text(2.05, 0.001, '100%', fontsize = 10,color='black',weight='bold')\nplt.text(1.2, 0.001, '97.7%', fontsize = 10,color='black',weight='bold')\nplt.text(0.2, 0.001, '84.1%', fontsize = 10,color='black',weight='bold')\nplt.text(-0.8, 0.001, '50%', fontsize = 10,color='black',weight='bold')\nplt.text(-1.85, 0.001, '15.9%', fontsize = 10,color='black',weight='bold')\nplt.text(-2.7, 0.001, '2.3%', fontsize = 10,color='black',weight='bold')\n\nplt.yticks(np.arange(0, 0.5, step=0.1))\n\n# add title on the x-axis\nplt.xlabel(\"Z-score\")\n\n# add title on the y-axis\nplt.ylabel(\"Frequency\")\n\n\nplt.show()\n\n\n\n\nZ-score Distribution\n\n\n\n\nDownload and open the normal_z_scores_percentages.xlsx file in this repository to see data being used to create the below figure. To get the colors you need to use the “Select Data…” option when right clicking on the chart:\n\n\n\nNormal Distribution Z-scores and percentages\n\n\n\n\nIt’s not clear that generating a distribution like this would be helpful to do with JASP, so this isn’t included.\n\n\nIt’s not clear that generating a distribution like this would be helpful to do in SPSS, so this isn’t included.\n\n\n\nThe above figure visualises how 13.6% of the population have score that is more negative than -2 standard deviations from the mean, 34.1% of the population have a standard deviation that is more negative than -1 standard deviations from the mean (this also include all the people who are more than -2 standard deviations from the mean), etc.\nWe can now use the above information to identify which percentile an individual is within a distribution.\nFor example, let’s imagine that an individual called Jane wants to know what percentile she’s at with her height. Lets imagine she is 170cm tall, the mean height of people 150cm, and the SD 10cm. That would make her z-score:\n\\[\nZ_{score} = \\frac{170 - 150}{10} = 2\n\\]\nAs we can see from the figure above, that puts her above 97.7% of the population, putting her in the top 2.3%."
  },
  {
    "objectID": "distributions/normal.html#consolidation-questions",
    "href": "distributions/normal.html#consolidation-questions",
    "title": "Normal Distribution",
    "section": "Consolidation questions",
    "text": "Consolidation questions\n\nQuestion 1\n\nrand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nquestion_1_result = { \n  if(question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\n\nQuestion 2\nUsing the above value, which percentile group would you put Jamie’s score into?\n\nquestion_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nquestion_2_result = { \n  if(question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(question_2_correct == question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  },
  {
    "objectID": "advancedR/fancyFigures.html",
    "href": "advancedR/fancyFigures.html",
    "title": "Fancy Figures",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "correlations/partialCorrelations.html",
    "href": "correlations/partialCorrelations.html",
    "title": "Partial Correlations",
    "section": "",
    "text": "As you may have heard, correlation does not equal causation. One possible reason for this is that there’s a third variable that explains an association. Let’s imagine that we are trying to understand whether life expectancy goes up over time and why. First of all, let’s check if lifeExpectancy is going up over time using the gapminder data:\n\nRPythonJulia\n\n\n\nlibrary(gapminder)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ngapminder %>% \n  group_by(year) %>% \n  summarise(\n    lifeExp = mean(lifeExp),\n    gdpPercap = mean(gdpPercap)\n  ) -> gapminder_by_year\n\ncor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$lifeExp\nt = 18.808, df = 10, p-value = 3.91e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9498070 0.9962336\nsample estimates:\n     cor \n0.986158 \n\nggplot(data = gapminder_by_year, aes(x = year, y = lifeExp)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# TBC\n\n\n\n\n# TBC\n\n\n\n\nSo now that we’ve confirmed that there is a positive association between the year and life expectancy, the next question is why? What changes from year to year that could explain increased life expectancy? Let’s investigate whether gdp per capita generally goes up each year, and whether it’s associated with life expectancy. If both of these things are true, then perhaps the increase in gdp per year is an explanation of the association between year and life expectancy.\n\nIs year and gdp per capita associated?\n\nR\n\n\n\ncor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$year and gapminder_by_year$gdpPercap\nt = 17.039, df = 10, p-value = 1.022e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9393538 0.9954265\nsample estimates:\n      cor \n0.9832101 \n\nggplot(data = gapminder_by_year, aes(x = year, y = gdpPercap)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhilst there are some outliers in earlier years, we seem to have found that gdp per capita has gone up.\n\n\n\n\n\nIs gdp per capita and life expectancy associated?\n\nR\n\n\n\ncor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_by_year$gdpPercap and gapminder_by_year$lifeExp\nt = 11.137, df = 10, p-value = 5.875e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8663785 0.9895601\nsample estimates:\n      cor \n0.9619721 \n\nggplot(data = gapminder_by_year, aes(x = gdpPercap, y = lifeExp)) + geom_point() + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nSo the above analysis suggests that all three variables are incredibly related. (you are unlikely to see such strong associations in real psychology experiments). But let’s now check in whether there’s still an association between the year and life expectancy once you control for GDP per capita. This partial correlation can be visualised as follows:\n\n\n\n\nflowchart LR\n    Year(X:Year) <---> GDP(Z:GDP per Capita)\n    Year <---> Life(Y:Life Expectancy)\n    GDP <---> Life \n  style Year color:white,fill:#159,stroke:#333,stroke-width:4px\n  style Life color:white,fill:#159,stroke:#333,stroke-width:4px\n  style GDP fill:#571,stroke:#fff,stroke-width:3px,color:#fff,stroke-dasharray: 5 5\n\n\n\n\n\n\n\n\nTo calculate the partial correlation r value you control for the association between the two variables\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{originalCorrelation - varianceExplainedByCovariate}{varianceNotExplainedByCovariate}\n\\]\n\n\\(x\\) = The Year\n\\(y\\) = Life expectancy\n\\(z\\) = GDP per capita\n\\(\\sqrt{1- r^2_{xz}}\\) = variance not explained by correlation between Year(\\(x\\)) and GDP (\\(z\\))\n\\(\\sqrt{1- r^2_{yz}}\\) = variance not explained by correlation between Life Expectancy (\\(y\\)) and GDP (\\(z\\))\n\nOne way to think of the formula above is that: - the top-half represents how much variance is explained by overlap between the two main variables, subtracted by the variance of each variable with the confound - the bottom-half represents how much variance there is left to explain once you’ve removed associations between each main variable and the covariate.\nLet’s see what the r value is after this partial correlation:\n\\[\nr_{xy*z} = \\frac{r_{xy} - r_{xz} * r_{yz}}{\\sqrt{(1-r^2_{xz})(1-r^2_{yz})}} = \\frac{.986158 - .9832101 * .9619721 }{\\sqrt{(1-.9832101^2)(1-.9619721^2)}} = \\frac{.040354}{.04984321} = .8092841\n\\]\nLet’s check if the manually calculated partial r-value is the same as what R gives us:\n\nlibrary(ppcor)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor(gapminder_by_year)\n\n$estimate\n               year    lifeExp  gdpPercap\nyear      1.0000000  0.8092844  0.7629391\nlifeExp   0.8092844  1.0000000 -0.2521280\ngdpPercap 0.7629391 -0.2521280  1.0000000\n\n$p.value\n                 year    lifeExp   gdpPercap\nyear      0.000000000 0.00254769 0.006309324\nlifeExp   0.002547690 0.00000000 0.454498736\ngdpPercap 0.006309324 0.45449874 0.000000000\n\n$statistic\n              year    lifeExp  gdpPercap\nyear      0.000000  4.1331001  3.5404830\nlifeExp   4.133100  0.0000000 -0.7816358\ngdpPercap 3.540483 -0.7816358  0.0000000\n\n$n\n[1] 12\n\n$gp\n[1] 1\n\n$method\n[1] \"pearson\"\n\n\nYes, the pearson correlation between year and life expectancy is .8092844, so the difference of .0000003 is a rounding error from the manual calculations above. Just to confirm that this is a rounding error, here’s what you get if you complete the same steps but with the estimate part of the correlation objects instead:\n\nx.y.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$lifeExp)\nx.z.cor <- cor.test(gapminder_by_year$year, gapminder_by_year$gdpPercap)\ny.z.cor <- cor.test(gapminder_by_year$gdpPercap, gapminder_by_year$lifeExp)\n\n(x.y.cor$estimate - x.z.cor$estimate * y.z.cor$estimate)/sqrt((1-x.z.cor$estimate^2) * (1 - y.z.cor$estimate^2))\n\n      cor \n0.8092844 \n\n\nConfirming that the difference in the manual calculation is a rounding error. Note that the numbers you get from R may be rounded numbers, and so your calculations may reflect the rounding.\nTo conclude, as there is still an association between Year and Life Expectancy once controlling for GDP, this partial correlation is consistent with GDP not being the only explanation for why Life Expectancy goes up each year."
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "Please make sure you’ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. We’ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data.Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\\(\\sum\\) is saying to add together everything\n\\(x_i\\) refers to each individual’s x-score\n\\(y_i\\) refers to each individual’s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\nThe \\(SD\\) (Standard deviation; which is just the square root of variance) of how data is distributed around the mean \\(life\\)\\(expectancy\\) (per capita) can be visualised as follows within the gapminder \\(gdp*lifeExpectancy\\) in the light blue box:\n\nRPython\n\n\n\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# ggsave(\"life_exp_resid.png\", life_exp_resid)\nlife_exp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add horizontal line for the mean of 'lifeExp'\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\n\n\n\nScatterplot with residuals of ‘Life expectancy’\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\nRPython\n\n\n\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  )\nggsave(\"gdp_resid.png\", gdp_resid)\n\nSaving 7 x 5 in image\n\ngdp_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('life_exp_resid.png')\n\n\n\n\n\n\n\nScatterplot with residuals of ‘GDP per capita’\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations",
    "section": "Total variance",
    "text": "Total variance\nA correlation aims to explain how much of the \\(total\\) \\(variance\\) is explained by the overlapping variance between the x and y axes. So we need to capture the \\(total\\) \\(variance\\) separately for the x and y axes. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nOr, to use the figures above:\n\n\n\n\n\n\n\n\n\n$Total$ $Var$ =\nsqrt( \\^2) $$ \\frac{}{N-1} $$\n$*$\nsqrt( \\^ 2) $$ \\frac{}{N-1} $$\n\n\n\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other.\n\nShared variance between \\(x\\) and \\(y\\)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\nRPython\n\n\n\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\nSaving 7 x 5 in image\n\nshared_resid\n\n\n\n\n\n\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=gapminder_2007[\"lifeExp\"], ymax=gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n# save the plot\nplt.savefig('shared_resid.png')\n\n\n\n\n\n\n\nScatterplot with shared residuals’\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearson’s R correlation we need to compare the amount that x and y vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\nRPython\n\n\n\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n[1] 0.6786624\n\n\n\n\n\n# Import math Library\nimport math\n\nvarxy = sum(\n    (gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())*\n    (gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean()))\n\n\ntotalvar = math.sqrt(\n   sum((gapminder_2007[\"gdpPercap\"] - gapminder_2007[\"gdpPercap\"].mean())**2)*\n   sum((gapminder_2007[\"lifeExp\"] - gapminder_2007[\"lifeExp\"].mean())**2))\nvarxy/totalvar\n\n\n\n\n0.6786623986777583\nIf the above calculation is correct, we’ll get exactly the same value when using the cor.test function:\n\nRPython\n\n\n\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\n\n\n\nimport scipy.stats\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n\n\n\n(0.6786623986777585, 1.6891897969647515e-20)\nTo visualise what proportion of the variance is captured by \\(var_{xy}\\):\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt( \\^2\n\n\n\nA question you might have at this point, is whether the above figure seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nThe bottom half of the figure above doesn’t give you that much insight into how consistently \\(x\\) and \\(y\\) are positive or negatively associated with each other, but the top half does;\n\nshared_resid\n\n\n\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\nRPython\n\n\n\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\ninverted_resid\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap\"], 125-gapminder_2007[\"lifeExp\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=125-gapminder_2007[\"lifeExp\"],xmin=gapminder_2007[\"gdpPercap\"], xmax=gapminder_2007[\"gdpPercap\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=125-gapminder_2007[\"lifeExp\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap\"],ymin=125-gapminder_2007[\"lifeExp\"], ymax=125-gapminder_2007[\"lifeExp\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy\")\n\n# show the plot\nplt.show()\n\n## save the plot\n#plt.savefig('inverted_resid.png')\n\n\n\n\n\n\n\nScatterplot with inverted shared residuals’\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there’s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like it’s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman’s Rho (AKA Spearman’s Rank) instead:\n\nRPython\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\n\n\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\ndef spssSkewKurtosis(x):\n    import pandas as pd\n    import math\n    \n    w=len(x)\n    m1=x.mean()\n    m2=sum((x-m1)**2)\n    m3=sum((x-m1)**3)\n    m4=sum((x-m1)**4)\n    s1=(x).std()\n    skew=w*m3/(w-1)/(w-2)/s1**3\n    sdskew=math.sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n    kurtosis=(w*(w+1)*m4 - 3*m2**2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1**4)\n    sdkurtosis=math.sqrt( 4*(w**2-1) * sdskew**2 / ((w-3)*(w+5)) )\n    \n    ## z-scores added by reading-psych\n    zskew = skew/sdskew\n    zkurtosis = kurtosis/sdkurtosis\n    \n    mat = pd.DataFrame([[skew, sdskew, zskew],[kurtosis, sdkurtosis, zkurtosis]], columns=['estimate','se','zScore'], index=['skew','kurtosis'])\n    \n    return mat\n  \nspssSkewKurtosis(gapminder_2007[\"gdpPercap\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp\"])\n\n\n\n\n\n\n\nSkewness and kurtosis for ‘gdpPercap’\n\n\n\n\n\nSkewness and kurtosis for ‘lifeExp’\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearman’s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations",
    "section": "Spearman’s Rank (AKA Spearman’s Rho)",
    "text": "Spearman’s Rank (AKA Spearman’s Rho)\nSpearman’s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e. unusually extreme) data points. Let’s now turn the gapminder data we’ve been working with above into ranks and then run a Pearson’s correlation on it to confirm this:\n\nRPython\n\n\n\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\n\n\ngapminder_2007[\"gdpPercap_rank\"] = gapminder_2007[\"gdpPercap\"].rank()\ngapminder_2007[\"lifeExp_rank\"] = gapminder_2007[\"lifeExp\"].rank()\n\n\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\nRPython\n\n\n\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\n\n\n\nspssSkewKurtosis(gapminder_2007[\"gdpPercap_rank\"])\nspssSkewKurtosis(gapminder_2007[\"lifeExp_rank\"])\n\n\n\n\n\n\n\nSkewness and kurtosis for ‘gdpPercap_rank’\n\n\n\n\n\nSkewness and kurtosis for ‘lifeExp_rank’\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e. be at a higher risk of false positives. This is evidence that using a Spearman’s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\nRPython\n\n\n\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\n\n\n\n# Pearson correlation on **ranked** data:\nscipy.stats.pearsonr(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n# Spearman correlation applied to original data (letting R do the ranking)\nscipy.stats.spearmanr(gapminder_2007[\"gdpPercap\"], gapminder_2007[\"lifeExp\"])\n\n\n\n\n(0.8565899189213543, 4.6229745362984015e-42)\n\nSpearmanrResult(correlation=0.8565899189213544, pvalue=4.62297453629821e-42)\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\nRPython\n\n\n\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\nrank_resid\n\n\n\n\n\n\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize =(7, 5))\n\n#scatter plot for the dataset\nplt.scatter(gapminder_2007[\"gdpPercap_rank\"], gapminder_2007[\"lifeExp_rank\"])\n\n# add vertical line for the mean of \"gdpPercap\"\nplt.axvline(x=gapminder_2007[\"gdpPercap_rank\"].mean(), color='b', ls='--')\n\n# add horizontal lines from the individual point to the mean of \"gdpPercap\"\nplt.hlines(y=gapminder_2007[\"lifeExp_rank\"],xmin=gapminder_2007[\"gdpPercap_rank\"], xmax=gapminder_2007[\"gdpPercap_rank\"].mean(), colors='red', lw=0.5)\n\n# add horizontal line for the mean of \"lifeExp\"\nplt.axhline(y=gapminder_2007[\"lifeExp_rank\"].mean(), color='b', ls='--')\n\n# add vertical lines from the individual point to the mean of \"lifeExp\"\nplt.vlines(x=gapminder_2007[\"gdpPercap_rank\"],ymin=gapminder_2007[\"lifeExp_rank\"], ymax=gapminder_2007[\"lifeExp_rank\"].mean(), colors='green', lw=0.5)\n\n# add title on the x-axis\nplt.xlabel(\"GDP per capita (RANK)\")\n\n# add title on the y-axis\nplt.ylabel(\"Life Expectancy (RANK)\")\n\nred_patch = mpatches.Patch(color='red', label='GDP Residuals')\ngreen_patch = mpatches.Patch(color='green', label='Life Expectancy Residuals')\n\nplt.legend(handles=[red_patch, green_patch],bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\nScatterplot for ‘Life Expectancy (RANK)’ and ‘GDP per capita (RANK)’ with residuals’\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\nshared_resid"
  },
  {
    "objectID": "excelIntro/anchoring.html",
    "href": "excelIntro/anchoring.html",
    "title": "Anchoring",
    "section": "",
    "text": "This section will keep using the averageifs tab in excel. There are a couple of tricks that'll allow you to copy and paste the formulas in a way that saves you having to write them out again with minor tinkering. The first is to make your formula as dynamic as possible. This means to refer to other cells where possible to make your formula do more for you.\nThis might be still be a bit abstract, so let's do something concrete. You'll see that we structured our output into a table:\n\nThis formula doesn't yet make use of the information in the table. Here's an example of one way that it could:\n\nSee how we're referring to the cell with the word \"Female\" in, rather than having to write \"Female\". In the Final assignment this sort of trick will be very helpful. Let's do the same to look at whether there's a mobile phone present:\n\nNow we're nearly ready to start copying and pasting this cell into the other 3 cells. However, if we did that now, the cells would only be partially correctly aligned, as the relative locations would work a little, but not entirely. Let's see that in practice. First, let's copy the cell one up from it's original location:\n\nSee that whilst it is good that one of the cells moved up to refer to \"male\" rather than \"female\", another of the cells moved to refer to \"mobile phone present\" instead of \"yes\", which is what we would have wanted.\nSimilarly, if we just copy the original cell one to the right instead, you might already guess what's going to go wrong:\n\nWhilst it was good that one referred to cell moved to the right to refer to the \"no\" value, we didn't want the other cell to move to the right to refer to \"384.5503\", we wanted it to stay on \"female\". You might also notice that all the columns we are referring to have moved over also, and are also misaligned!\nSo to prevent cells moving in ways you do not want them to, you anchor them. You anchor them by putting a dollar sign ($) before the row and/or column you want to stop changing. So let's fix the original cell to be appropriately anchored:\n\nIn particular, notice the new dollar signs ($) next to their respective rows and columns\n\nThis will require practice to become confident with. But it will be an extremely useful excel skill."
  },
  {
    "objectID": "extras/questionMaker.html",
    "href": "extras/questionMaker.html",
    "title": "Question Maker",
    "section": "",
    "text": "If you have any questions you would like to contribute to any of the consolidation questions, this page will help generate the code needed to include them. Alternatively, you can simply suggest a question at:\nhttps://github.com/Reading-Psych/jast/discussions/categories/suggest-content\nIf you would like to generate the code, please complete the following inputs:\nGive your question a unique name. It cannot have a space in it, so use underscores (e.g. “question_1” rather than “question 1”)\n\nviewof question_name = Inputs.text();\n\n\n\n\n\n\nWrite the question itself here:\n\nviewof question_text = Inputs.text();\n\n\n\n\n\n\nWhat type of question is it?\n\nviewof question_type = Inputs.radio([\n  \"multiple choice\", \n  \"numeric\",\n  \"text\"\n]);\n\n\n\n\n\n\nIf you selected multiple choice, write the options you would like the user to choose from. Please put a | (known as a “pipe”) character between each option\n\nviewof question_responses = Inputs.text();\n\n\n\n\n\n\nWhat is the correct answer? Be precise in how it’s written.\n\nviewof question_correct   = Inputs.text();\n\n\n\n\n\n\n\noutput_string = {\n  var accuracy_code = \"correct_\" + question_name + \" = '\" + question_correct + \"';\\n\" +\n    question_name + \"_result = {\\n\" +\n    \"  if(\" + question_name + \"_response == correct_\" + question_name + \"){\\n\" +\n    \"    return 'Correct!';\\n\" +\n    \"  } else {\\n\" +\n    \"    return 'Incorrect or incomplete.';\\n\" +\n    \"  };\\n\" +\n    \"}\\n\" +\n    \"```\\n\" +\n    \"${\" + question_name + \"_result}\";\n  switch(question_type){\n    case \"multiple choice\":\n      return question_text + \"\\n\" +\n        \"```{ojs}\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.radio(['\" + question_responses.split(\"|\").join(\"','\") + \"']);\\n\" +\n        accuracy_code;\n      break;\n    case \"numeric\":\n      return \"```{ojs}\\n\" +\n         \"viewof \" + question_name + \"_response = Inputs.number({label: '\" + question_text + \"'});\\n\" +\n         accuracy_code;\n      break;\n    case \"text\":\n      return \"```{ojs}\\n\" +\n        \"viewof \" + question_name + \"_response = Inputs.text({label: '\" + question_text + \"'});\\n\" +\n        accuracy_code;\n      break;\n  }  \n}"
  },
  {
    "objectID": "extras/allQuestions.html",
    "href": "extras/allQuestions.html",
    "title": "All questions",
    "section": "",
    "text": "rand_maths_score = 40 + Math.round(Math.random() * 60);\nmean_maths_score = 70\nsd_maths_score   = 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamie has just completed a mathematics test, where the maximum score is 100%. Their score was , the mean maths score was  and the SD was . What is their Z-score?\n\nviewof question_1_response = Inputs.number([-7,3], {label: \"Z-score\", step:.1});\ncorrect_z_score = (rand_maths_score - mean_maths_score)/sd_maths_score;\n\nquestion_1_result = { \n  if(question_1_response == correct_z_score){\n    return \"Correct! (\" + rand_maths_score + \" - \" + mean_maths_score + \")/\" + sd_maths_score + \" = \" + correct_z_score;\n  } else {\n    return \"Missing or incorrect. Remember that how Z is calculated by dividing the difference between a value and the mean value by the SD.\"\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\n\n\nUsing the above value, which percentile group would you put Jamie’s score into?\n\nquestion_2_correct = {\n  if(correct_z_score < -2){\n    return \"bottom 2.3%\";\n  } else if(correct_z_score < -1){\n    return \"bottom 15.9%\";\n  } else if(correct_z_score < 0){\n    return \"bottom 50%\";\n  } else if(correct_z_score < 1){\n    return \"top 50%\";\n  } else if(correct_z_score < 2){\n    return \"top 15.9%\";\n  } else {\n    return \"top 2.3%\";\n  }\n}\n\n\n\n\n\n\n\nviewof question_2_response = Inputs.radio([\n  \"bottom 2.3%\", \n  \"bottom 15.9%\",\n  \"bottom 50%\",\n  \"top 50%\",\n  \"top 15.9%\",\n  \"top 2.3%\", \n  ], {label: \"\", value: \"A\"});\nquestion_2_result = { \n  if(question_2_response == \"\"){\n    return \"awaiting your response\";\n  } else if(question_2_correct == question_2_response){\n    return \"Correct!\";\n  } else {\n    return \"Missing or Incorrect - have a look at the plots above to help you find the correct answer. Note, the distributions are symmetrical, so the pattern for the top half will mirror that for the bottom half.\";\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour answer is… .\n\nIf you want to practice with different numbers in these questions then please reload the page."
  }
]