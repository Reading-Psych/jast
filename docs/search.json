[
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell.\nInstall the VS Code Python Extension to enable running this cell interactively.\n\nimport os\nos.cpu_count()\n\n10"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\n\nCode\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\n\nIs Platykurtic vs. leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n\nCode\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "Please make sure you’ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. We’ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data.Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\\(\\sum\\) is saying to add together everything\n\\(x_i\\) refers to each individual’s x-score\n\\(y_i\\) refers to each individual’s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\nThe \\(SD\\) (Standard deviation; which is just the square root of variance) of how data is distributed around the mean \\(life\\)\\(expectancy\\) (per capita) can be visualised as follows within the gapminder \\(gdp*lifeExpectancy\\) in the light blue box:\n\n\nCode\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\n\nggsave(\"life_exp_resid.png\", life_exp_resid)\n\n\nSaving 7 x 5 in image\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\n\nCode\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\nggsave(\"gdp_resid.png\", gdp_resid)\n\n\nSaving 7 x 5 in image\n\n\nCode\ngdp_resid\n\n\n\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations",
    "section": "Total variance",
    "text": "Total variance\nA correlation aims to explain how much of the \\(total\\) \\(variance\\) is explained by the overlapping variance between the x and y axes. So we need to capture the \\(total\\) \\(variance\\) separately for the x and y axes. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nOr, to use the figures above:\n\n\n\n\n\n\n\n\n\n$Total$ $Var$ =\nsqrt( \\^2) $$ \\frac{}{N-1} $$\n$*$\nsqrt( \\^ 2) $$ \\frac{}{N-1} $$\n\n\n\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other.\n\nShared variance between \\(x\\) and \\(y\\)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\n\nCode\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\"\n    )\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\n\nSaving 7 x 5 in image\n\n\nCode\nshared_resid\n\n\n\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearson’s R correlation we need to compare the amount that x and y vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\n\nCode\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n\n[1] 0.6786624\n\n\nIf the above calculation is correct, we’ll get exactly the same value when using the cor.test function:\n\n\nCode\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\nTo visualise what proportion of the variance is captured by \\(var_{xy}\\):\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt( \\^2\n\n\n\nA question you might have at this point, is whether the above figure seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nThe bottom half of the figure above doesn’t give you that much insight into how consistently \\(x\\) and \\(y\\) are positive or negatively associated with each other, but the top half does;\n\n\nCode\nshared_resid\n\n\n\n\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\n\nCode\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\"\n    )\n  )\n\ninverted_resid\n\n\n\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there’s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like it’s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman’s Rho (AKA Spearman’s Rank) instead:\n\n\nCode\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearman’s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations",
    "section": "Spearman’s Rank (AKA Spearman’s Rho)",
    "text": "Spearman’s Rank (AKA Spearman’s Rho)\nSpearman’s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e. unusually extreme) data points. Let’s now turn the gapminder data we’ve been working with above into ranks and then run a Pearson’s correlation on it to confirm this:\n\n\nCode\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\n\nCode\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e. be at a higher risk of false positives. This is evidence that using a Spearman’s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\n\nCode\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n\nCode\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\n\nCode\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\n\nrank_resid\n\n\n\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\n\nCode\nshared_resid"
  },
  {
    "objectID": "pythonBasics/logic.html",
    "href": "pythonBasics/logic.html",
    "title": "Python Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\nFalse\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\nTrue"
  },
  {
    "objectID": "pythonBasics/logic.html#comparing-values-using",
    "href": "pythonBasics/logic.html#comparing-values-using",
    "title": "Python Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\nFalse\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\nTrue\n\n\nGreat! What you’re more likely to want to do is to compare a list to a value. So let’s imagine that you have asked your participants a question, and have a list that identifies whether someone got an answer correct or not. Let’s compare that list to the word “correct”:\n\nimport numpy as np\ncorrect_vector = np.array([\"correct\", \"incorrect\", \"correct\"])\ncorrect_vector == \"correct\"\n\narray([ True, False,  True])\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "pythonBasics/logic.html#indexingselecting-data",
    "href": "pythonBasics/logic.html#indexingselecting-data",
    "title": "Python Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\n# load the markdownTable module \nfrom markdownTable import markdownTable\nimport pandas as pd\n\nresponse_table ={'accuracy': correct_vector,\n  'response_times': [100, 105, 180]}\n  \nresponse_table = pd.DataFrame(response_table)\n\nresponse_table\n\n# create an index using the logical \"same as\" operator\nresponse_table[\"response_times\"][response_table[\"accuracy\"]==\"correct\"]\n\n0    100\n2    180\nName: response_times, dtype: int64\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "pythonBasics/logic.html#to-reverse-logic",
    "href": "pythonBasics/logic.html#to-reverse-logic",
    "title": "Python Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\nTrue\n\n\nor a ! before the logical object or statement that you want to reverse:\n\n[i for i, x in enumerate(correct_vector) if x == \"correct\"]\n[i for i, x in enumerate(correct_vector) if x != \"correct\"]\n\n[1]"
  },
  {
    "objectID": "pythonBasics/logic.html#and-using",
    "href": "pythonBasics/logic.html#and-using",
    "title": "Python Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_table[\"response_times_vector\"] = np.array([1200,600,800])\nresponse_table.loc[(response_table['response_times_vector']<1000) & (response_table['accuracy']=='correct')]\n\n\n\n\n\n  \n    \n      \n      accuracy\n      response_times\n      response_times_vector\n    \n  \n  \n    \n      2\n      correct\n      180\n      800\n    \n  \n\n\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "pythonBasics/logic.html#or-using",
    "href": "pythonBasics/logic.html#or-using",
    "title": "Python Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data ={\n  \"participant_number\" : [1,2,3,4,5],\n  \"eyesight\"           : [\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\"]\n}\neyesight_data = pd.DataFrame(eyesight_data)\n\neyesight_data\n\n\n\n\n\n  \n    \n      \n      participant_number\n      eyesight\n    \n  \n  \n    \n      0\n      1\n      colorblind\n    \n    \n      1\n      2\n      colorblind\n    \n    \n      2\n      3\n      uncorrected\n    \n    \n      3\n      4\n      uncorrected\n    \n    \n      4\n      5\n      glasses\n    \n  \n\n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\neyesight_data.loc[(eyesight_data['eyesight']==\"colorblind\") | (eyesight_data['eyesight']=='glasses')]\n\n\n\n\n\n  \n    \n      \n      participant_number\n      eyesight\n    \n  \n  \n    \n      0\n      1\n      colorblind\n    \n    \n      1\n      2\n      colorblind\n    \n    \n      4\n      5\n      glasses"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency",
    "section": "",
    "text": "Mean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n\nCode\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\n\nLoading required package: gapminder\n\n\nCode\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n\n  \n\n\n\nCode\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n[1] 67.00742\n\n\n\n\n\n\nCode\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n# a reminder of the data frame\ngapminder_2007\n\n# total of all years\n\n\n                 country continent  year  lifeExp       pop     gdpPercap\n11           Afghanistan      Asia  2007   43.828  31889923    974.580338\n23               Albania    Europe  2007   76.423   3600523   5937.029526\n35               Algeria    Africa  2007   72.301  33333216   6223.367465\n47                Angola    Africa  2007   42.731  12420476   4797.231267\n59             Argentina  Americas  2007   75.320  40301927  12779.379640\n...                  ...       ...   ...      ...       ...           ...\n1655             Vietnam      Asia  2007   74.249  85262356   2441.576404\n1667  West Bank and Gaza      Asia  2007   73.422   4018332   3025.349798\n1679         Yemen, Rep.      Asia  2007   62.698  22211743   2280.769906\n1691              Zambia    Africa  2007   42.384  11746035   1271.211593\n1703            Zimbabwe    Africa  2007   43.487  12311143    469.709298\n\n[142 rows x 6 columns]\n\n\nCode\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n67.00742253521126\n\n\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mean()\n\n\n67.00742253521126\n\n\n\n\n\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\nCode\nmedian(gapminder_2007$lifeExp)\n\n\n[1] 71.9355\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mean()\n\n\n67.00742253521126\n\n\nCode\ngapminder_2007['lifeExp'].median()\n\n\n71.93549999999999\n\n\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\nCode\nmedian(gapminder_2007$lifeExp)\n\n\n[1] 71.9355\n\n\nCode\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mode()\n\n\n0      39.613\n1      42.082\n2      42.384\n3      42.568\n4      42.592\n        ...  \n137    81.235\n138    81.701\n139    81.757\n140    82.208\n141    82.603\nName: lifeExp, Length: 142, dtype: float64\n\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp)\n\n\n[1] 142\n\n\nCode\nlength(unique(gapminder_2007$lifeExp))\n\n\n[1] 142\n\n\n\n\n\n\nCode\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\n\n\n142\n\n\nCode\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n\n142\n\n\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\n\nCode\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n\n[1] 2 4\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n   0\n0  2\n1  4\n\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most."
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n\nCode\n1 > 2\n\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n\nCode\n1 < 2\n\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n\nCode\n2 == 1\n\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n\nCode\n3/2 == 1.5\n\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\n\nCode\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\n\nCode\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n\n  \n\n\n\nCode\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n\nCode\n1 != 2\n\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\n\nCode\ncorrect_vector == \"correct\"\n\n\n[1]  TRUE FALSE  TRUE\n\n\nCode\n!correct_vector == \"correct\" \n\n\n[1] FALSE  TRUE FALSE\n\n\nCode\n# which is the same as\n!(correct_vector == \"correct\")\n\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\n\nCode\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\n\nCode\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\n\nCode\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "pythonBasics/fundamentals.html",
    "href": "pythonBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "Often, you can use R as a calculator, for example, if you want to know what two people’s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nIt’s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g. how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. “my object” would be an invalid object name.\nObject names are case-sensitive, so if your object is called “MyObject” you cannot refer to it as “myobject”.\n“.” and “_” are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou can’t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing “t.te” and pressing the tab will give you “t.test”\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you don’t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe “c” above is short for “combine” as you’re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the “typeof” function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (“heights”) is a “double” vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers can’t always be integers. Strings are identified as “character” objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the “mpg” dataframe. To look at it, just type in it’s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below won’t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see what’s in the “manufacturer” column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\n# if(!require(ggplot2)){install.packages(\"ggplot2\")}\n# library(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about “intelligent copy and paste” to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming you’ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\n# ggplot(mpg, aes(displ, hwy, colour = class)) + \n#   geom_point()\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code we’ve copied so that you can apply it to your own scripts. There’s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\n# ggplot(             # R will keep looking at your code until all the open brackets have been closed)\n#   mpg,              #\n#   aes(              #\n#     displ,          #\n#     hwy,            #\n#     colour = class  #\n#   )\n# ) +                 # R will look to the next line if you end a line with +\n# geom_point()        #\n\nHere’s how I would comment it out:\n\n# ggplot(\n#   mpg,              # dataframe\n#   aes(              # aesthetic properties\n#     displ,          # x-axis\n#     hwy,            # y-axis\n#     colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n#   )\n# ) + \n# geom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\n# ggplot(\n#   mpg,              # dataframe\n#   aes(              # aesthetic properties\n#     cty,            # x-axis - updated\n#     hwy,            # y-axis\n#     colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n#   )\n# ) + \n# geom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics",
    "section": "",
    "text": "In statistics, a variable is any (measurable) attribute that describes any organism or object. It’s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object.\nWhat is a hypothesis? A(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e. randomly). We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\nIt is the p-value threshold that identifies if a result is “significant” or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is “significant” (i.e. so unlikely that this would have happened by chance that we conclude this didn’t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e. we say there is an effect when there isn’t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Note: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n\nCode\n3.1e3 - 3.1 * 10^3\n\n\n[1] 0\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n\nCode\n2.5e-3 - 2.5 * 10^(-3)\n\n\n[1] 0"
  },
  {
    "objectID": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "href": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "title": "Multi-collinearity (incomplete)",
    "section": "Measuring multi-collinearity using Variance Inflation Factor (VIF)",
    "text": "Measuring multi-collinearity using Variance Inflation Factor (VIF)\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\n\nCode\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\ngdp_pop_predict_lifeExp <- lm(\n  formula = lifeExp ~ pop + gdpPercap,\n  data = gapminder_2007\n    \n)\n\n\ngdp_pop_predict_lifeExp$coefficients\n\n\n (Intercept)          pop    gdpPercap \n5.920520e+01 7.000961e-09 6.416085e-04 \n\n\nCode\npred_lm <- lm(lifeExp ~ pop, gapminder_2007)\npred_lm$coefficients[2]\n\n\n         pop \n3.889069e-09 \n\n\nCode\n1-sqrt(pred_lm$coefficients[2])\n\n\n      pop \n0.9999376 \n\n\nCode\nvif(gdp_pop_predict_lifeExp)\n\n\n      pop gdpPercap \n 1.003109  1.003109"
  },
  {
    "objectID": "regressions/simpleRegressions.html#prediction-using-regression",
    "href": "regressions/simpleRegressions.html#prediction-using-regression",
    "title": "Simple regression (incomplete)",
    "section": "Prediction using regression",
    "text": "Prediction using regression\nSimple regression, also known as linear regression, builds on correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\n\nCode\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression can’t tell us exactly what is influencing what (i.e. whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\nGradient reflects how steep the line is\nIntercept is the point at which the regression line crosses the y-axis\n\nLet’s use coding magic to find out the intercept and the gradient (AKA slope):\n\n\nCode\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nThe above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more useful terms, for every extra $10,000 dollars per person, the life expectancy goes up by 6 years).\nFor the above equation we will always retrieve values from the graph, except residuals, which is the ‘error’ and so a more complete formula for the outcome can be represented by the following formula\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nResidual reflects what’s left over, and is not represented in the line of best fit formula because you can’t predict what’s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\n\nCode\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\n\nThese residuals can be thought of the error, i.e. what the model failed to predict. In more mathematical terms, the model would be:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "regressions/simpleRegressions.html#notes-for-anthony",
    "href": "regressions/simpleRegressions.html#notes-for-anthony",
    "title": "Simple regression (incomplete)",
    "section": "Notes for Anthony",
    "text": "Notes for Anthony\nR2 = SSR/SSTO = 1 - SSE/SSTO\n\n\nCode\nsse  = sum((gapminder_2007$lifeExp - gapminder_2007$fitted)^2)\nssto = sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\nrsqr = 1-(sse/ssto)\n\n\nIf your model is minimising the error, then what happens if you have 2 predictors:\n\\[\nY = a + b_1X_1 + b_2X_2 + e\n\\]\nSave the residuals after 1 correlation:\n\n\nCode\nres_gdp = gapminder_2007$lifeExp - gapminder_2007$fitted\ncor.test(gapminder_2007$pop, res_gdp)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and res_gdp\nt = 1.3842, df = 140, p-value = 0.1685\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.04948127  0.27564447\nsample estimates:\n      cor \n0.1161931"
  },
  {
    "objectID": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "href": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "title": "Simple regression (incomplete)",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\nIn correlations we discussed how the strength of association is the proportion of variance of y explained by x. For simple regression, this is also the case:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply the above formula to see what R is for \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\n\nCode\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap))\n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) \n  )\n\n\n[1] 0.6786624\n\n\nCode\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)\n\n\n[1] 0.6786624\n\n\nCode\n# r^2\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)^2 \n\n\n[1] 0.4605827\n\n\nWe can confirm that squaring R gives r^2\n\n\nCode\nsummary(life_expectancy_model)\n\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.828  -6.316   1.922   6.898  13.128 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(>|t|)    \n(Intercept) 59.56565008  1.01040864   58.95 <0.0000000000000002 ***\ngdpPercap    0.00063713  0.00005827   10.93 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.899 on 140 degrees of freedom\nMultiple R-squared:  0.4606,    Adjusted R-squared:  0.4567 \nF-statistic: 119.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nNow lets see if the same logic works when there are 2 or more predictors that interact to predict each other:\n\\[\nr = \\frac{var_{xyz}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})(z_i-\\bar{z})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2 * \\sum(z_i-\\bar{z})^2}}\n\\]\n\n\nCode\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap)) *\n  (gapminder_2007$pop-mean(gapminder_2007$pop))  \n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n    sum((gapminder_2007$pop - mean(gapminder_2007$pop))^2) \n  )\n\n\n[1] -0.007330224\n\n\n\n\nCode\nsummary(lm(lifeExp ~  gdpPercap + pop, data = gapminder_2007))\n\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap + pop, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                   Estimate      Std. Error t value            Pr(>|t|)    \n(Intercept) 59.205198140717  1.040398672164  56.906 <0.0000000000000002 ***\ngdpPercap    0.000641608517  0.000058176209  11.029 <0.0000000000000002 ***\npop          0.000000007001  0.000000005068   1.381               0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.87 on 139 degrees of freedom\nMultiple R-squared:  0.4679,    Adjusted R-squared:  0.4602 \nF-statistic: 61.11 on 2 and 139 DF,  p-value: < 0.00000000000000022\n\n\n\nWhy use regression?\nRegression builds on correlation by providing a more detailed view of your data and with this provides an equation that can be used for any future predicting and optimizing of your data.\n\n\n[1] 4\n\n\nThe differences between regression and correlation"
  }
]