[
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell.\nInstall the VS Code Python Extension to enable running this cell interactively.\n\nimport os\nos.cpu_count()\n\n10"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\n\nCode\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\n\nIs Platykurtic vs. leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n\nCode\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "Please make sure you’ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. We’ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data.Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\\(\\sum\\) is saying to add together everything\n\\(x_i\\) refers to each individual’s x-score\n\\(y_i\\) refers to each individual’s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\nThe \\(SD\\) (Standard deviation; which is just the square root of variance) of how data is distributed around the mean \\(life\\)\\(expectancy\\) (per capita) can be visualised as follows within the gapminder \\(gdp*lifeExpectancy\\) in the light blue box:\n\n\nCode\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\n\nggsave(\"life_exp_resid.png\", life_exp_resid)\n\n\nSaving 7 x 5 in image\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\n\nCode\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\nggsave(\"gdp_resid.png\", gdp_resid)\n\n\nSaving 7 x 5 in image\n\n\nCode\ngdp_resid\n\n\n\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations",
    "section": "Total variance",
    "text": "Total variance\nA correlation aims to explain how much of the \\(total\\) \\(variance\\) is explained by the overlapping variance between the x and y axes. So we need to capture the \\(total\\) \\(variance\\) separately for the x and y axes. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nOr, to use the figures above:\n\n\n\n\n\n\n\n\n\n$Total$ $Var$ =\nsqrt( \\^2) $$ \\frac{}{N-1} $$\n$*$\nsqrt( \\^ 2) $$ \\frac{}{N-1} $$\n\n\n\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other.\n\nShared variance between \\(x\\) and \\(y\\)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\n\nCode\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\"\n    )\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\n\nSaving 7 x 5 in image\n\n\nCode\nshared_resid\n\n\n\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearson’s R correlation we need to compare the amount that x and y vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\n\nCode\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n\n[1] 0.6786624\n\n\nIf the above calculation is correct, we’ll get exactly the same value when using the cor.test function:\n\n\nCode\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\nTo visualise what proportion of the variance is captured by \\(var_{xy}\\):\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt( \\^2\n\n\n\nA question you might have at this point, is whether the above figure seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nThe bottom half of the figure above doesn’t give you that much insight into how consistently \\(x\\) and \\(y\\) are positive or negatively associated with each other, but the top half does;\n\n\nCode\nshared_resid\n\n\n\n\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\n\nCode\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\"\n    )\n  )\n\ninverted_resid\n\n\n\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there’s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like it’s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman’s Rho (AKA Spearman’s Rank) instead:\n\n\nCode\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearman’s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations",
    "section": "Spearman’s Rank (AKA Spearman’s Rho)",
    "text": "Spearman’s Rank (AKA Spearman’s Rho)\nSpearman’s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e. unusually extreme) data points. Let’s now turn the gapminder data we’ve been working with above into ranks and then run a Pearson’s correlation on it to confirm this:\n\n\nCode\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\n\nCode\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e. be at a higher risk of false positives. This is evidence that using a Spearman’s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\n\nCode\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n\nCode\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\n\nCode\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) + theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\n\nrank_resid\n\n\n\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\n\nCode\nshared_resid"
  },
  {
    "objectID": "pythonBasics/logic.html",
    "href": "pythonBasics/logic.html",
    "title": "Python Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2"
  },
  {
    "objectID": "pythonBasics/logic.html#comparing-values-using",
    "href": "pythonBasics/logic.html#comparing-values-using",
    "title": "Python Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\nGreat! What you’re more likely to want to do is to compare a list to a value. So let’s imagine that you have asked your participants a question, and have a list that identifies whether someone got an answer correct or not. Let’s compare that list to the word “correct”:\n\n# import numpy as np\n# correct_vector = np.array([\"correct\", \"incorrect\", \"correct\"])\n# correct_vector == \"correct\"\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "pythonBasics/logic.html#indexingselecting-data",
    "href": "pythonBasics/logic.html#indexingselecting-data",
    "title": "Python Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\n# load the markdownTable module \nfrom markdownTable import markdownTable\nimport pandas as pd\n\nresponse_table ={'accuracy': correct_vector,\n  'response_times': [100, 105, 180]}\n  \nresponse_table = pd.DataFrame(response_table)\n\nresponse_table\n\n# create an index using the logical \"same as\" operator\nresponse_table[\"response_times\"][response_table[\"accuracy\"]==\"correct\"]\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "pythonBasics/logic.html#to-reverse-logic",
    "href": "pythonBasics/logic.html#to-reverse-logic",
    "title": "Python Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\nor a ! before the logical object or statement that you want to reverse:\n\n[i for i, x in enumerate(correct_vector) if x == \"correct\"]\n[i for i, x in enumerate(correct_vector) if x != \"correct\"]"
  },
  {
    "objectID": "pythonBasics/logic.html#and-using",
    "href": "pythonBasics/logic.html#and-using",
    "title": "Python Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_table[\"response_times_vector\"] = np.array([1200,600,800])\nresponse_table.loc[(response_table['response_times_vector']<1000) & (response_table['accuracy']=='correct')]\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "pythonBasics/logic.html#or-using",
    "href": "pythonBasics/logic.html#or-using",
    "title": "Python Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data ={\n  \"participant_number\" : [1,2,3,4,5],\n  \"eyesight\"           : [\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\"]\n}\neyesight_data = pd.DataFrame(eyesight_data)\n\neyesight_data\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\neyesight_data.loc[(eyesight_data['eyesight']==\"colorblind\") | (eyesight_data['eyesight']=='glasses')]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency",
    "section": "",
    "text": "Mean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n\nCode\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\n\nLoading required package: gapminder\n\n\nCode\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n\n  \n\n\n\nCode\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n[1] 67.00742\n\n\n\n\n\n\nCode\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n# a reminder of the data frame\ngapminder_2007\n\n# total of all years\n\n\n                 country continent  year  lifeExp       pop     gdpPercap\n11           Afghanistan      Asia  2007   43.828  31889923    974.580338\n23               Albania    Europe  2007   76.423   3600523   5937.029526\n35               Algeria    Africa  2007   72.301  33333216   6223.367465\n47                Angola    Africa  2007   42.731  12420476   4797.231267\n59             Argentina  Americas  2007   75.320  40301927  12779.379640\n...                  ...       ...   ...      ...       ...           ...\n1655             Vietnam      Asia  2007   74.249  85262356   2441.576404\n1667  West Bank and Gaza      Asia  2007   73.422   4018332   3025.349798\n1679         Yemen, Rep.      Asia  2007   62.698  22211743   2280.769906\n1691              Zambia    Africa  2007   42.384  11746035   1271.211593\n1703            Zimbabwe    Africa  2007   43.487  12311143    469.709298\n\n[142 rows x 6 columns]\n\n\nCode\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n67.00742253521126\n\n\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mean()\n\n\n67.00742253521126\n\n\n\n\n\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\nCode\nmedian(gapminder_2007$lifeExp)\n\n\n[1] 71.9355\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mean()\n\n\n67.00742253521126\n\n\nCode\ngapminder_2007['lifeExp'].median()\n\n\n71.93549999999999\n\n\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\n\nCode\nmean(gapminder_2007$lifeExp)\n\n\n[1] 67.00742\n\n\nCode\nmedian(gapminder_2007$lifeExp)\n\n\n[1] 71.9355\n\n\nCode\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\n\nCode\ngapminder_2007['lifeExp'].mode()\n\n\n0      39.613\n1      42.082\n2      42.384\n3      42.568\n4      42.592\n        ...  \n137    81.235\n138    81.701\n139    81.757\n140    82.208\n141    82.603\nName: lifeExp, Length: 142, dtype: float64\n\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\n\nCode\nlength(gapminder_2007$lifeExp)\n\n\n[1] 142\n\n\nCode\nlength(unique(gapminder_2007$lifeExp))\n\n\n[1] 142\n\n\n\n\n\n\nCode\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\n\n\n142\n\n\nCode\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n\n142\n\n\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\n\nCode\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n\n[1] 2 4\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n   0\n0  2\n1  4\n\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most."
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n\nCode\n1 > 2\n\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n\nCode\n1 < 2\n\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n\nCode\n2 == 1\n\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n\nCode\n3/2 == 1.5\n\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\n\nCode\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\n\nCode\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n\n  \n\n\n\nCode\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n\nCode\n1 != 2\n\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\n\nCode\ncorrect_vector == \"correct\"\n\n\n[1]  TRUE FALSE  TRUE\n\n\nCode\n!correct_vector == \"correct\" \n\n\n[1] FALSE  TRUE FALSE\n\n\nCode\n# which is the same as\n!(correct_vector == \"correct\")\n\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\n\nCode\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\n\nCode\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\n\nCode\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  }
]