[
  {
    "objectID": "describingData/centralTendency.html",
    "href": "describingData/centralTendency.html",
    "title": "Central Tendency",
    "section": "",
    "text": "Mean vs. Median vs. Mode\nThe mean is often called the “average” informally, but is actually a specific type of “average”. The mean is the average you get when add together a group of numbers, and then divide by the number of items you combined. For example, to calculate the mean life expectancy of countries in 2007, we’ll use gapminder data\n\nRPython\n\n\n\n# install (if required) and load the gapminder data\nif(!require(gapminder)){install.packages(\"gapminder\")}\n\nLoading required package: gapminder\n\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# a reminder of the data frame\nrmarkdown::paged_table(head(gapminder_2007))                 \n\n\n\n  \n\n\n# total of all years\nsum_life_expectancy  = sum(gapminder_2007$lifeExp)\n\n# count the people\nn_life_expectancy    = length(gapminder_2007$lifeExp)        \nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n[1] 67.00742\n\n\n\n\n\n# load the gapminder module and import the gapminder dataset\nfrom gapminder import gapminder\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 = gapminder.loc[gapminder['year'] == 2007]\n\n# a reminder of the data frame\ngapminder_2007\n\n# total of all years\nsum_life_expectancy  = gapminder_2007['lifeExp'].sum()\n\n# count the people\nn_life_expectancy    = gapminder_2007['lifeExp'].count() \n\n# calculate mean life expectancy\nmean_life_expectancy = sum_life_expectancy/n_life_expectancy \nmean_life_expectancy\n\n\n\n\nFor those of you who like to double check these things (which is a good instinct), lets see what number you get if you use the r function for mean:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\n\n\n\n\nWhew - it’s the same as the manual calculation above.\nNow median is less known than mean, but median is the value in the middle once you order all your data. It’s well explained in the first paragraph on wikipedia: https://en.wikipedia.org/wiki/Median, so I would suggest looking there. As you can see below, the mean and median are not always the same (in fact, they are usually at least slightly different):\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\n\n\n\ngapminder_2007['lifeExp'].mean()\ngapminder_2007['lifeExp'].median()\n\n\n\n\nFinally, the mode is simply the most frequent number in your data. So lets now see if the mode is closer to the mean or median:\n\nRPython\n\n\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n# Solution for calculating mode found at https://stackoverflow.com/a/2547918 as there doesn't seem to be a native function:\nlibrary(modeest)\nmlv(gapminder_2007$lifeExp, method = \"mfv\")\n\n  [1] 39.613 42.082 42.384 42.568 42.592 42.731 43.487 43.828 44.741 45.678\n [11] 46.242 46.388 46.462 46.859 48.159 48.303 48.328 49.339 49.580 50.430\n [21] 50.651 50.728 51.542 51.579 52.295 52.517 52.906 52.947 54.110 54.467\n [31] 54.791 55.322 56.007 56.728 56.735 56.867 58.040 58.420 58.556 59.443\n [41] 59.448 59.545 59.723 60.022 60.916 62.069 62.698 63.062 63.785 64.062\n [51] 64.164 64.698 65.152 65.483 65.528 65.554 66.803 67.297 69.819 70.198\n [61] 70.259 70.616 70.650 70.964 71.164 71.338 71.421 71.688 71.752 71.777\n [71] 71.878 71.993 72.235 72.301 72.390 72.396 72.476 72.535 72.567 72.777\n [81] 72.801 72.889 72.899 72.961 73.005 73.338 73.422 73.747 73.923 73.952\n [91] 74.002 74.143 74.241 74.249 74.543 74.663 74.852 74.994 75.320 75.537\n[101] 75.563 75.635 75.640 75.748 76.195 76.384 76.423 76.442 76.486 77.588\n[111] 77.926 78.098 78.242 78.273 78.332 78.400 78.553 78.623 78.746 78.782\n[121] 78.885 79.313 79.406 79.425 79.441 79.483 79.762 79.829 79.972 80.196\n[131] 80.204 80.546 80.653 80.657 80.745 80.884 80.941 81.235 81.701 81.757\n[141] 82.208 82.603\n\n\n\n\n\ngapminder_2007['lifeExp'].mode()\n\n\n\n\nThe mode for this data was actually every value perhaps because each value was unique! Lets double check that:\n\nRPython\n\n\n\nlength(gapminder_2007$lifeExp)\n\n[1] 142\n\nlength(unique(gapminder_2007$lifeExp))\n\n[1] 142\n\n\n\n\n\n# count number of elements in the dataset\ngapminder_2007['lifeExp'].count()\n\n# create a vector with the unique values presented in the dataset\nnum_values = gapminder_2007['lifeExp'].unique()\n\n# get the lenght of the vector\nlen(num_values)\n\n\n\n\nThe length of the whole vector and the unique values of the vector is the same, confirming that there’s no repetition in this data (and so no number is the mode). Lets make up some data so that we can look at what the mode is:\n\nRPython\n\n\n\nmode_example_vector <- c(1,2,2,3,4,4)\nmlv(mode_example_vector, method = \"mfv\")\n\n[1] 2 4\n\n\n\n\n\nimport pandas as pd\n\n# create a vector\nmode_example_vector = [1,2,2,3,4,4]\n\n# convert the vector to a pandas dataframe\nmode_example_vector = pd.DataFrame(mode_example_vector)\n\n# get the mode\nmode_example_vector.mode()\n\n\n\n\nIn the above data, there are 2 modes, as the numbers 2 and 4 occur the most."
  },
  {
    "objectID": "describingData/describingData.html",
    "href": "describingData/describingData.html",
    "title": "Describing Data",
    "section": "",
    "text": "Univariate statistics is a term for statistics that describes a numerical and continuous variable. We will be describing the data along the following parameters:\n\ncentral tendency (mean and median)\ndispersion (range, variance and standard deviation)\ndistribution (skewness and kurtosis)"
  },
  {
    "objectID": "describingData/dispersion.html",
    "href": "describingData/dispersion.html",
    "title": "Dispersion",
    "section": "",
    "text": "To understand distribution (later), it’s helpful to clarify some more basic concepts around how data is dispersed or spread."
  },
  {
    "objectID": "describingData/dispersion.html#range",
    "href": "describingData/dispersion.html#range",
    "title": "Dispersion",
    "section": "Range",
    "text": "Range\nRange simply captures the min and the maximum values. Lets look at the min and max for the life expectancy data from 2007:\n\n# load the gapminder data\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmin(gapminder_2007$lifeExp)\n\n[1] 39.613\n\nmax(gapminder_2007$lifeExp)\n\n[1] 82.603\n\n\nSo the range for life expectancy in 2007 was between 39.613 and 82.603."
  },
  {
    "objectID": "describingData/dispersion.html#variance",
    "href": "describingData/dispersion.html#variance",
    "title": "Dispersion",
    "section": "Variance",
    "text": "Variance\n\nPopulation Variance\nVariance is how much the data varies around a mean. To capture this, we compare each individual’s score with the mean, so lets do this with our gapminder data’s life expectancy:\n\nlife_expectancy_variance_table <- data.frame(\n  life_expectancy = gapminder_2007$lifeExp,\n  diff_from_mean  = gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp)\n)\n\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\nSo we know for each country how different their life expectacy is to the mean life expectancy. But ideally we would like a single value to summarise variance. Lets see what would happen if we tried to summarise these differences from the mean by calculating the mean difference from the mean:\n\nmean(life_expectancy_variance_table$diff_from_mean) \n\n[1] 5.153937e-15\n\n\nWe get a number that is effectively zero (go here for an explanation about e-numbers), because all the values above the mean balance out those below the mean. So to address this, we can square the differences to force all the numbers to be positive:\n\nlife_expectancy_variance_table$diff_squared = life_expectancy_variance_table$diff_from_mean^2\nrmarkdown::paged_table(life_expectancy_variance_table)\n\n\n\n  \n\n\n\nIf we calculate the average of this, then we get a summary of the variance that is more informative:\n\nmean(life_expectancy_variance_table$diff_squared)\n\n[1] 144.7314\n\n\nHowever, as mean is what you get when you add all the items together and then divide it by the number of items, this can also be done in 2 steps in R (this will help us understand the formula later):\n\nsum_of_squares = sum(life_expectancy_variance_table$diff_squared)\nthis_variance  = sum_of_squares/length(life_expectancy_variance_table$diff_squared)\nthis_variance\n\n[1] 144.7314\n\n\nWe can represent the above in the following formula for the population’s (remember, this is when you have everyone from the group you are measuring) variance:\n\\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N}\n\\]\nLet’s break down each of the above symbols: σ^2 is population variance Σ is sum xi refers to the value for each participant x̄ refers to the mean for all participants N refers to the number of participants\n(note that the above is written as if we’re looking at the variance of a group of participants, but the principles still work if looking at non-participant data)\n\n\nSample variance\nTo calculate the variance for a sample of participants, rather than every participant in the group you’re measuring, you need a slightly different formula:\n\\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )^2)} {N - 1}\n\\] Note that Sample variance is represented by S^2 rather than σ^2\nSo why do we divide by N-1 rather than N? This is because the sample variance is an estimate rather than the actual population variance. When estimating the population variance you take into account the actual number of people (N) in the sample, whereas when you are estimating what happens generally in the population based on your sample, you take into account the degrees of freedom (N-1). In broad terms this reduces the risk of you under-estimating the variance of the population. You don’t necessarily need to understand degrees of freedom beyond the idea that you are controlling for the fact that you are analysing a sample rather than the population they represent, so don’t worry if the next section isn’t completely clear."
  },
  {
    "objectID": "describingData/dispersion.html#degrees-of-freedom",
    "href": "describingData/dispersion.html#degrees-of-freedom",
    "title": "Dispersion",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom calculations can be useful to address statistics that are vulnerable to bias within a sample (i.e. the sample being distorted compared to the population). Interestingly, mean is not, but variance is. Lets see this by looking at differences between the population and sample for mean and variance, looking at the height of three people, and combining them into every combination of 2 people possible:\n\nthree_heights = c(150,160,170)\npopulation_height_mean = mean(three_heights)\npopulation_height_variance = sum((three_heights - population_height_mean)^2)/3\n#sample participants in pairs\nsample_heights = data.frame(\n  pp1 = c(150,150,NA),\n  pp2 = c(160,NA,160),\n  pp3 = c(NA,170,170),\n  pair = c(\n    \"1 and 2\",\n    \"1 and 3\",\n    \"2 and 3\"\n  )\n)\n\nsample_heights$mean = c(\n  mean(c(three_heights[1], three_heights[2])),\n  mean(c(three_heights[1], three_heights[3])),\n  mean(c(three_heights[2], three_heights[3]))\n)\n\nsample_heights$pop_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/3,\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/3,\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/3\n)\n\nsample_heights$sample_var = c(\n  sum((c(three_heights[1], three_heights[2]) - mean(c(three_heights[1], three_heights[2])))^2)/(3-1),\n  sum((c(three_heights[1], three_heights[3]) - mean(c(three_heights[1], three_heights[3])))^2)/(3-1),\n  sum((c(three_heights[2], three_heights[3]) - mean(c(three_heights[2], three_heights[3])))^2)/(3-1)\n)\n\n\nrmarkdown::paged_table(sample_heights)\n\n\n\n  \n\n\nmean_sample_mean <- mean(sample_heights$mean)\nmean_sample_variance <- mean(sample_heights$sample_var)\nmean_population_variance <- mean(sample_heights$pop_var)\n\nWhen comparing the population mean to the mean sample mean (i.e., what is the typical mean for any sample), they’re identical (i.e. NOT biased):\n\npopulation_height_mean\n\n[1] 160\n\nmean_sample_mean\n\n[1] 160\n\n\nWhereas when comparing the actual population variance (population_height_variance) to the mean (to identify what is a typical) estimate of variance using the population formula that should not be used for samples (mean_population_variance) finds the estimate of variance is typically smaller than the actual variance in the population:\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_population_variance\n\n[1] 33.33333\n\n\nAs this bias (almost) always underestimates the population variance, degrees of freedom is a useful correction to address this within calculations of sample variance. Lets compare the actual population height variance (population_height_variance) to the mean estimate using degrees of freedom that should be used for samples (mean_sample_variance).\n\npopulation_height_variance\n\n[1] 66.66667\n\nmean_sample_variance\n\n[1] 50\n\n\nSo, not perfect, but this is less under-representative of the variance.\nOne thing to bear in mind is that calculation of some statistics does not require use of the degrees of freedom to correct for bias (as seen above, mean was not susceptible to bias).\nIf you would like to understand how degrees of freedom are determined, and what the thinking is behind this term, read on for a brief description of this (otherwise, feel free to skip to the next section).\nDegrees of freedom refers to how many values could change in your variable once you know what the outcome of the relevant statistic is. For example, if you’re interested in the variance of the height of the three people, then you only have 2 degrees of freedom, because once you know the height of 2 of the participants AND the variance of the height, then there the remaining participant only has a 2 possible heights (so their height isn’t free to change)."
  },
  {
    "objectID": "describingData/dispersion.html#standard-deviation-sd",
    "href": "describingData/dispersion.html#standard-deviation-sd",
    "title": "Dispersion",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is the square root of the variance. This takes into account that that the variance includes the square of the difference between the individual values and the mean:\nPopulation Variance \\[\n\\sigma^2 = \\frac{\\sum((x_i- \\bar{x}{})\\color{Red}{^2}\\color{Black})} {N}\n\\]\nPopulation SD \\[\n\\sigma = \\sqrt\\frac{\\sum((x_i- \\bar{x}{})\\color{Red}{^2}\\color{Black})} {N}\n\\]\nSample Variance \\[\nS^2 = \\frac{\\sum((x_i- \\bar{x}{} )\\color{red}{^2}\\color{Black})} {N - 1}\n\\] Sample SD \\[\nS = \\sqrt\\frac{\\sum((x_i- \\bar{x}{} )\\color{red}{^2}\\color{Black})} {N - 1}\n\\]"
  },
  {
    "objectID": "rBasics/logic.html",
    "href": "rBasics/logic.html",
    "title": "R Logic",
    "section": "",
    "text": "If you want to see if one object is larger than (>) or smaller (<) than another object, you can use the > and < operators:\n\n1 > 2\n\n[1] FALSE\n\n\nUnsurprising that the above is false, as 1 is not greater than 2. Lets double check if 1 is less than 2:\n\n1 < 2\n\n[1] TRUE"
  },
  {
    "objectID": "rBasics/logic.html#comparing-values-using",
    "href": "rBasics/logic.html#comparing-values-using",
    "title": "R Logic",
    "section": "Comparing values using ==",
    "text": "Comparing values using ==\nIf you want to see if 2 objects are the same, then you can use ==. Lets check if 1 is the same as 2:\n\n2 == 1\n\n[1] FALSE\n\n\nUnsurprisingly, 2 is not the same as 1. Lets see if 3/2 is the same as 1.5:\n\n3/2 == 1.5\n\n[1] TRUE\n\n\nGreat! What you’re more likely to want to do is to compare a vector to a value. So let’s imagine that you have asked your participants a question, and have a vector that identifies whether someone got an answer correct or not. Let’s compare that vector to the word “correct”:\n\ncorrect_vector <- c(\"correct\", \"incorrect\", \"correct\")\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n\nThis creates an logical vector of TRUE and FALSE values. Let’s use this now to select data:"
  },
  {
    "objectID": "rBasics/logic.html#indexingselecting-data",
    "href": "rBasics/logic.html#indexingselecting-data",
    "title": "R Logic",
    "section": "Indexing/Selecting data",
    "text": "Indexing/Selecting data\nSometimes you want to only focus on certain data, and indexing is a way to do this. We’re now going to create a data frame for a participant who has completed 3 trials of a reaction time task. This will include whether they were correct or not, and what their response time is. We will then using indexing to select the response times when the participant was correct:\n\nresponse_table <- data.frame(\n  accuracy = correct_vector, # see the vector created above\n  response_times = c(100,105,180)\n)\nrmarkdown::paged_table(response_table)\n\n\n\n  \n\n\n# create an index using the logical \"same as\" operator\naccuracy_index <- response_table$accuracy == \"correct\"\n\n# use square brackets to use an index to select\naccurate_trials_response_times <- response_table$response_times[accuracy_index]\n# show the valid response times for accurate trials:\naccurate_trials_response_times\n\n[1] 100 180\n\n\nIndexing is useful to remove unwanted data. In this case, most researchers think that response times when a participant makes an invalid response are not very informative, so they remove those response times using indexing above."
  },
  {
    "objectID": "rBasics/logic.html#to-reverse-logic",
    "href": "rBasics/logic.html#to-reverse-logic",
    "title": "R Logic",
    "section": "! to reverse logic",
    "text": "! to reverse logic\nSometimes you’ll want to flip the logic so that you get a FALSE when it would be TRUE, or TRUE when it would be FALSE. To do this, put in either a != instead of ==:\n\n1 != 2\n\n[1] TRUE\n\n\nor a ! before the logical object or statement that you want to reverse:\n\ncorrect_vector == \"correct\"\n\n[1]  TRUE FALSE  TRUE\n\n!correct_vector == \"correct\" \n\n[1] FALSE  TRUE FALSE\n\n# which is the same as\n!(correct_vector == \"correct\")\n\n[1] FALSE  TRUE FALSE"
  },
  {
    "objectID": "rBasics/logic.html#and-using",
    "href": "rBasics/logic.html#and-using",
    "title": "R Logic",
    "section": "And using &",
    "text": "And using &\nIf you want to get a TRUE outcome only if multiple statements are all TRUE, then you can use the “&” operator. Lets imagine you want to only focus on responses in your data that are correct AND quick enough i.e. less than 1000ms:\n\nresponse_times_vector <- c (1200,600,800)\nvalid_responses <- response_times_vector < 1000 & correct_vector == \"correct\"\nvalid_responses\n\n[1] FALSE FALSE  TRUE\n\n\nSo only the third response was both correct and quick enough."
  },
  {
    "objectID": "rBasics/logic.html#or-using",
    "href": "rBasics/logic.html#or-using",
    "title": "R Logic",
    "section": "OR using |",
    "text": "OR using |\nOR statements can be used to get a TRUE outcome if at least one of the logical statements is TRUE. Lets imagine that you were trying to select a subset of participants who either were colorblind or wore glasses. Your data might look like this:\n\neyesight_data <- data.frame(\n  participant_number = c(1,2,3,4,5),\n  eyesight           = c(\"colorblind\",\"colorblind\",\"uncorrected\",\"uncorrected\",\"glasses\")\n)\nrmarkdown::paged_table(eyesight_data)\n\n\n\n  \n\n\n\nIf we just wanted the rows that had people who were colorblind or wore glasess, we could create the following logical vector:\n\ncolorblind_glasses_vector <- eyesight_data$eyesight == \"colorblind\" | eyesight_data$eyesight == \"glasses\"\ncolorblind_glasses_vector\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE"
  },
  {
    "objectID": "rBasics/fundamentals.html",
    "href": "rBasics/fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "Often, you can use R as a calculator, for example, if you want to know what two people’s heights are together you can add them:\n\n120 + 130\n\n[1] 250\n\n\nIt’s helpful to store these calculations into objects using <- as shown below:\n\n# this # is starting a comment, code that will be ignored but allows you to annotate your work\nant_height <- 120 # this is exactly the same as writing ant_height = 5 + 2, but <- is encouraged in R to avoid confusion with other uses of = (e.g. == operator when you are comparing if two values are identical)\nbob_height <- 130\nant_height # to show what the value 120 is now stored in the object \"ant_height\"\n\n[1] 120\n\nbob_height # to show what the value 130 is now stored in the object \"bob_height\"\n\n[1] 130\n\n\nThis means that you can compare objects to each other later, e.g. how much taller is Bob than Ant:\n\nbob_height - ant_height\n\n[1] 10\n\n\nSome advice/rules for Objects (sometimes known as variables in other coding languages):\n\nYou cannot have a space in an object name. “my object” would be an invalid object name.\nObject names are case-sensitive, so if your object is called “MyObject” you cannot refer to it as “myobject”.\n“.” and “_” are acceptable characters in variable names.\nYou can overwrite objects in the same way that you define an object, but it arguably will make your code more brittle, so be careful doing so:\n\n\nant_age <- 35 # at timepoint 1\nant_age # to confirm what the age was at this point\n\n[1] 35\n\n# wait a year\nant_age <- 36 # at timepoint 1\nant_age # to confirm that the age has been updated\n\n[1] 36\n\n\n\nYou can’t start an object name with a number\nbe careful to not give an object the same name as a function! This will overwrite the function. To check if the name already exists, you can start typing it and press tab. So typing “t.te” and pressing the tab will give you “t.test”\n\n\nFunctions\nIn a variety of coding languages like R, functions are lines of code you can apply each time you call the function, and apply it to input(s) to get an output. If you wanted to make a function that multiplied two numbers together, it could look something like:\n\nto_the_power_of <- function( # Define your function by stating it's name \n    input_1,           # You can have as many inputs as you like\n    input_2            # \n){ \n  output = input_1 ^ input_2  # creates an output object \n  return (output)             # gives the output back to the user when they run the function\n}\nto_the_power_of(input_1 = 4, input_2 = 3) # should give you 64\n\n[1] 64\n\nto_the_power_of(4,3)                      # should also give you 64\n\n[1] 64\n\n\nThe great news is that you don’t need to write functions 99% of the time in R, there are a wide variety of functions that are available. Some of which will be used in the next section.\n\n\nTypes of Objects\n\nVectors\nVectors store a series of values. Often these will be a series of numbers:\n\nheights_vector = c(120,130,135)\nheights_vector\n\n[1] 120 130 135\n\n\nThe “c” above is short for “combine” as you’re combining values together to make this vector. Strings are values that have characters (also known as letters) in them. Lets see if we can make a vector of strings:\n\nnames_vector = c(\"ant\", \"bob\", \"charles\")\nnames_vector\n\n[1] \"ant\"     \"bob\"     \"charles\"\n\n\nLooks like we can. But what happens if you mix strings and numbers in a vector:\n\nyear_group = c(1, \"2a\", \"2b\")\nyear_group\n\n[1] \"1\"  \"2a\" \"2b\"\n\n\nR seems to be happy to put them into a single vector. But there are different types of values and vectors, so lets ask R what each type of (using the “typeof” function) vectors we have above:\n\ntypeof(heights_vector)\n\n[1] \"double\"\n\ntypeof(names_vector)\n\n[1] \"character\"\n\ntypeof(year_group)\n\n[1] \"character\"\n\n\nThe numeric vector (“heights”) is a “double” vector. Double refers to the fact that the numbers can include decimals, as opposed to integer numbers which have to be whole numbers. Interestingly, R has assumed the list of numbers should be double rather than integer, which seems like the more robust thing to do, as integer numbers can always be double, but double numbers can’t always be integers. Strings are identified as “character” objects, because they are made of characters.\n\n\nData frames\nData frames look like tables, in which you have a series of columns with headers that describe each column.\nSome data frames are already loaded into RStudio when you run it, such as the “mpg” dataframe. To look at it, just type in it’s name (and press CTRL-ENTER on the line, or CTRL-SHIFT-ENTER within the chunk. Note that the below won’t work properly if you write it in the console, you should be running this within an rMarkdown or rNotebook):\n\n# To make a nice looking table in your html output from the \"mpg\" dataframe from the ggplot2 package:\nrmarkdown::paged_table(head(ggplot2::mpg))\n\n\n\n  \n\n\n\nNote that you may see 2 tables above, but they should be identical if so\nThe mpg dataframe has information about a variety of cars, their manufacturers, models, as described https://ggplot2.tidyverse.org/reference/mpg.html. You will need to refer to data frames and their columns, the convention for this being to write data frame$column. Lets do this to see what’s in the “manufacturer” column:\n\nggplot2::mpg$manufacturer\n\n  [1] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n  [6] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [11] \"audi\"       \"audi\"       \"audi\"       \"audi\"       \"audi\"      \n [16] \"audi\"       \"audi\"       \"audi\"       \"chevrolet\"  \"chevrolet\" \n [21] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [26] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [31] \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\"  \"chevrolet\" \n [36] \"chevrolet\"  \"chevrolet\"  \"dodge\"      \"dodge\"      \"dodge\"     \n [41] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [46] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [51] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [56] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [61] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [66] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"     \n [71] \"dodge\"      \"dodge\"      \"dodge\"      \"dodge\"      \"ford\"      \n [76] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [81] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [86] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [91] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"ford\"      \n [96] \"ford\"       \"ford\"       \"ford\"       \"ford\"       \"honda\"     \n[101] \"honda\"      \"honda\"      \"honda\"      \"honda\"      \"honda\"     \n[106] \"honda\"      \"honda\"      \"honda\"      \"hyundai\"    \"hyundai\"   \n[111] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[116] \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"    \"hyundai\"   \n[121] \"hyundai\"    \"hyundai\"    \"jeep\"       \"jeep\"       \"jeep\"      \n[126] \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"       \"jeep\"      \n[131] \"land rover\" \"land rover\" \"land rover\" \"land rover\" \"lincoln\"   \n[136] \"lincoln\"    \"lincoln\"    \"mercury\"    \"mercury\"    \"mercury\"   \n[141] \"mercury\"    \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[146] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"    \n[151] \"nissan\"     \"nissan\"     \"nissan\"     \"nissan\"     \"pontiac\"   \n[156] \"pontiac\"    \"pontiac\"    \"pontiac\"    \"pontiac\"    \"subaru\"    \n[161] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[166] \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"     \"subaru\"    \n[171] \"subaru\"     \"subaru\"     \"subaru\"     \"toyota\"     \"toyota\"    \n[176] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[181] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[186] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[191] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[196] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[201] \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"     \"toyota\"    \n[206] \"toyota\"     \"toyota\"     \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[211] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[216] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[221] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[226] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n[231] \"volkswagen\" \"volkswagen\" \"volkswagen\" \"volkswagen\"\n\n\n\n\n\nPackages\nWhilst a lot of the functions you will need are in the base code that is active by default, you will at times need extra packages of code to do more powerful things. A commonly used package is ggplot2 [https://ggplot2.tidyverse.org/], which allows you to make beautiful figures in R. To use ggplot2 use need to install it and then load it from the library\n\nif(!require(ggplot2)){install.packages(\"ggplot2\")}\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nNow that you have a package for making beautiful plots, lets learn about “intelligent copy and paste” to make use of it.\n\n\nIntelligent copy and paste\nPeople experienced with coding do not write all their code from memory. They often copy and paste code from the internet and/or from their old scripts. So, assuming you’ve installed and loaded ggplot2 as described above, lets copy and paste code from their website (as of September 2022; https://ggplot2.tidyverse.org/)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\nGood news is that we have a nice looking figure. But now we need to work out how to understand the code we’ve copied so that you can apply it to your own scripts. There’s a lot to unpack, so making the code more vertical can help you break it down and comment it out. Using the below and a description of the mpg dataframe (https://ggplot2.tidyverse.org/reference/mpg.html), can you comment it out\n\nggplot(             # R will keep looking at your code until all the open brackets have been closed)\n  mpg,              #\n  aes(              #\n    displ,          #\n    hwy,            #\n    colour = class  #\n  )\n) +                 # R will look to the next line if you end a line with +\ngeom_point()        #\n\n\n\n\nHere’s how I would comment it out:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    displ,          # x-axis\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.)\n\n\n\n\nFormatting code like above to be clearer to read is useful when sharing your scripts with other researchers so that they can understand it!\nNow to understand the above code, try running it after changing lines. For example, what happens if you change the x-axis:\n\nggplot(\n  mpg,              # dataframe\n  aes(              # aesthetic properties\n    cty,            # x-axis - updated\n    hwy,            # y-axis\n    colour = class  # which column I will base the color on (often \"color\" is safer spelling in code)\n  )\n) + \ngeom_point()        # what I would like drawn on (as opposed to boxplots, lines, etc.\n\n\n\n\nTo make beautiful figures in R, you can largely google the type of plot you want, copy the example code that the website has, and then swap in the relevant features for your plot. This principle of copying and pasting code, (making it vertical to make it legible is not necessary, but can be helpful), and then editing it to work for your own script is an essential skill to speed up your coding."
  },
  {
    "objectID": "rBasics/filetypes.html",
    "href": "rBasics/filetypes.html",
    "title": "Types of Scripts",
    "section": "",
    "text": "type it directly into the console (generally not recommended)\nsave it as a script (better) note that script can refer to any of the below, but in this case is being used to describe a script that doesn’t generate a notebook\nsave it as an R Markdown (better still) - this allows you to make beautiful documents\nsave it as an R Notebook (arguably better than R Markdown) - this allows you to make beautiful documents, and is quicker\n\n\nThe Console\nAt the bottom left of RStudio you should have a console that looks something like what’s highlighted in red below:\n\n\n\nconsole\n\n\nYou can type straight into the console, to get a result. You can scroll through your previous commands by pressing the up arrow in the console. Each time code is run in the console it updates the environment in the top right of R-Studio:\n\n\n\nenvironment\n\n\n\n\nScripts\nThe word “script” can be interpreted specifically, to refer to a type of R file that includes a lot of code, or generally to refer to any file that includes both R code and code that allows you make a nice looking report. In this subsection, we will be focusing on “script” as a particular type of file. To create a script, click on File -> New File -> R Script\n\n\n\nnewScript\n\n\nYou will then be shown a blank script, in which you can write a series of functions, and then run them. To run the lines of code, select a line, and then press CTRL-ENTER, or highlight a chunk of code and then press CTRL-ENTER. In either case, the code will be sent to the console and run there.\nAn advantage of a script over just using the console is that you can analyse your data in both structured and complex ways which is difficult if you are typing code directly into the console.\n\n\nR Markdown\nAs highlighted above, R Markdown is a type of “script” in the general sense of the word, but allows you to create beautiful .html notebooks (.html files are what internet pages are based on). You are in fact reading an example of what can be produced by R Markdown (and R Notebooks). To make an R Markdown file, click on File -> New File -> R Markdown. You will be asked for a title, author and what output you would like. I would suggest “first markdown”, your name and “html” as the respective answers. You should then see something like:\n\n\n\nmarkdown\n\n\nThe following points apply to both R Markdown and R Notebooks\nIf you look above, you may notice that there are 2 types of code: Markdown (to write a nice looking report) and R (in grey chunks). I think these are well explained here: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf so I’ll just explain that R Markdowns run all the code in the chunks each time they generate the output file (e.g. html file). This is important to know, because R Notebooks do not run all the code in all the chunks when you generate them (see below for more on this).\n\n\nR Notebooks\nR Notebooks can be created by clicking on File -> New File -> R Notebook. They look quite similar to R Markdowns, but automatically generate the .html output each time you save the notebook. The output file will be a .nb.html file in the same folder as your notebook.\nVery importantly - the .nb.html file will be built based on what happened the last time you run each R chunk. If you never ran the R Chunk, then the nb.html file will not use the output from that chunk. This makes R Notebooks quicker than R Markdowns, because you don’t have to generate the output from scratch each time, as it will just use whatever was generated the last time the chunk was run. However, this means that there’s a risk your .html output will not be what you expect if you failed to run all of your chunks before saving your file. To address this risk (when you’ve finished editing your file), you can select the “run all” option.\n\n\n\nrunAllChunks"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Developed at the University of Reading School of Psychology and Clinical Language Sciences. This is not developed by the same team as JASP (although JASP is great).\nThis textbook is under-development (https://github.com/Reading-Psych/jast), and is aimed at students in the school of Psychology and Clinical Language Sciences. The aim will be to focus on statistics taught in MSc students in Reading using the following software:\nDo make use of the search-bar in the top-right to find any content within the website."
  },
  {
    "objectID": "index.html#contributions-in-alphabetical-order",
    "href": "index.html#contributions-in-alphabetical-order",
    "title": "About",
    "section": "Contributions (in alphabetical order)",
    "text": "Contributions (in alphabetical order)\n\n\n\nSurname\nFirst Name\nContribution\n\n\n\n\nBiagi\nNico\nArchitect, Author\n\n\nBrady\nDan\nArchitect, Author\n\n\nGoh\nVera\nSuggestions\n\n\nHaffey\nAnthony\nArchitect, Author\n\n\nMathews\nImogen\nAuthor\n\n\nPritchard\nKatherine\nSuggestions\n\n\nSahni\nAngad\nContributor\n\n\n\n\nArchitects have managed the formatting of this website/textbook\nAuthors have written (sub)sections\nContributors have contributed text for a subsection\nSuggestions are requests for elaborations and clarifications"
  },
  {
    "objectID": "categorical/ contingency.html",
    "href": "categorical/ contingency.html",
    "title": "Contingency(incomplete)",
    "section": "",
    "text": "You can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "statsBasics/statsBasics.html",
    "href": "statsBasics/statsBasics.html",
    "title": "Statistics Basics (incomplete)",
    "section": "",
    "text": "In statistics, a variable is any (measurable) attribute that describes any organism or object. It’s called a variable because they vary from organism to organism or object to object. Height is a good example of a variable within humans, as height changes from person to person.\nWithin coding, a variable tends to refer to a particular object in your code, such as a specific value, list, dataset, etc. In R the terminology for a variable tends to be object.\nWhat is a hypothesis? A(n experimental) hypothesis is a possible outcome for the study you will run. Sometimes researchers think in terms of null hypotheses, which is what you would expect if your (experimental) hypothesis is incorrect."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-a-p-value",
    "href": "statsBasics/statsBasics.html#what-is-a-p-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nOversimplification: A p-value tells you how likely your hypothesis is correct\nBetter definition: How likely you would get your current results by chance (i.e. randomly) if your main hypothesis were not true. We assume that a result is meaningful if there is only a very small chance that they could happen by accident.\nTechnical definition: The p-value is the probability of observing a particular (or more extreme) effect under the assumption that the null hypothesis is true (or the probability of the data given the null hypothesis: \\(Pr(Data|H_0)\\)).\nTo give a more concrete example:\n\nIf the observed difference between two means is small, then there is a high probability that the data underlying this difference could have occurred if the null (there is no difference) is true, and so the resulting p-value would be large.\nIn contrast, if the difference is huge, then the data underlying this difference is much less likely to have occurred if the null is true, and the subsequent p-value will be smaller to reflect the lower probability."
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "href": "statsBasics/statsBasics.html#what-is-the-alpha-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is the alpha value?",
    "text": "What is the alpha value?\nIt is the p-value threshold that identifies if a result is “significant” or not. Within psychology, the alpha value is .05, in which we believe that if the p-value is less than .05 then the result is “significant” (i.e. so unlikely that this would have happened by chance that we conclude this didn’t happen randomly).\nTechnical definition: The alpha-level is the expected rate of false-positives or type 1 errors (in the long run). Under the null hypothesis all p-values are equally probable, and so the alpha value sets the chance that a null hypothesis is rejected incorrectly (i.e. we say there is an effect when there isn’t one).\nSetting alpha at 0.05 is a convention that means we would only do this 5% of the time, and if we wanted to be more or less strict with the false-positive rate, we could adjust this value (this has been a contentious issue in recent years, see here and here)."
  },
  {
    "objectID": "statsBasics/eNumbers.html",
    "href": "statsBasics/eNumbers.html",
    "title": "Scientific Notation",
    "section": "",
    "text": "Note: for numbers larger than one the exponent is positive (\\(10^9\\)) and for numbers less than one the exponent is negative (\\(10^{-7}\\))\ne values are used to express scientific notation within R (and other programming languages) and essentially the \\(\\text{e}\\) replaces the \\(\\times 10\\) part of the notation.\nFor example, \\(3.1\\text{e}3\\) is the same as \\(3.1 \\times 10^3\\) (which is the same as 3100):\n\n3.1e3 - 3.1 * 10^3\n\n[1] 0\n\n\nLikewise, \\(2.5\\text{e-}3\\) is the same as \\(2.5 \\times 10^{-3}\\) (which is the same as .0025):\n\n2.5e-3 - 2.5 * 10^(-3)\n\n[1] 0"
  },
  {
    "objectID": "templates/ojs.html",
    "href": "templates/ojs.html",
    "title": "ojs",
    "section": "",
    "text": "Note that you won’t be able to preview figures created using this\n\ndata = FileAttachment(\"palmerpenguins.csv\").csv({ typed: true })\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\nathletes = FileAttachment(\"athletes.csv\").csv({typed: true})\n\n\ndotplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndotplot = Plot.dot(athletes, {x: \"weight\", y: \"height\", stroke: \"sex\"}).plot()\n\n\n//dotplot.legend(\"color\")\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"bill_depth_mm\", stroke: \"species\"}).plot()\n//dotplot = Plot.dot(filtered, {x: \"body_mass_g\", y: \"count\", stroke: \"sex\"}).plot()\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  )\n).plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "templates/tabsets.html",
    "href": "templates/tabsets.html",
    "title": "tabsets template",
    "section": "",
    "text": "RPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n\n1 + 2\n\n3"
  },
  {
    "objectID": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "href": "regressions/multiCollinearity.html#measuring-multi-collinearity-using-variance-inflation-factor-vif",
    "title": "Multi-collinearity (incomplete)",
    "section": "Measuring multi-collinearity using Variance Inflation Factor (VIF)",
    "text": "Measuring multi-collinearity using Variance Inflation Factor (VIF)\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\n\nCode\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\ngdp_pop_predict_lifeExp <- lm(\n  formula = lifeExp ~ pop + gdpPercap,\n  data = gapminder_2007\n    \n)\n\n\ngdp_pop_predict_lifeExp$coefficients\n\n\n (Intercept)          pop    gdpPercap \n5.920520e+01 7.000961e-09 6.416085e-04 \n\n\nCode\npred_lm <- lm(lifeExp ~ pop, gapminder_2007)\npred_lm$coefficients[2]\n\n\n         pop \n3.889069e-09 \n\n\nCode\n1-sqrt(pred_lm$coefficients[2])\n\n\n      pop \n0.9999376 \n\n\nCode\nvif(gdp_pop_predict_lifeExp)\n\n\n      pop gdpPercap \n 1.003109  1.003109"
  },
  {
    "objectID": "regressions/simpleRegressions.html#prediction-using-regression",
    "href": "regressions/simpleRegressions.html#prediction-using-regression",
    "title": "Simple regression (incomplete)",
    "section": "Prediction using regression",
    "text": "Prediction using regression\nSimple regression, also known as linear regression, builds on correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\n\nCode\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression can’t tell us exactly what is influencing what (i.e. whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\nGradient reflects how steep the line is\nIntercept is the point at which the regression line crosses the y-axis\n\nLet’s use coding magic to find out the intercept and the gradient (AKA slope):\n\n\nCode\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nThe above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more useful terms, for every extra $10,000 dollars per person, the life expectancy goes up by 6 years).\nFor the above equation we will always retrieve values from the graph, except residuals, which is the ‘error’ and so a more complete formula for the outcome can be represented by the following formula\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nResidual reflects what’s left over, and is not represented in the line of best fit formula because you can’t predict what’s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\n\nCode\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\n\nThese residuals can be thought of the error, i.e. what the model failed to predict. In more mathematical terms, the model would be:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "regressions/simpleRegressions.html#notes-for-anthony",
    "href": "regressions/simpleRegressions.html#notes-for-anthony",
    "title": "Simple regression (incomplete)",
    "section": "Notes for Anthony",
    "text": "Notes for Anthony\nR2 = SSR/SSTO = 1 - SSE/SSTO\n\n\nCode\nsse  = sum((gapminder_2007$lifeExp - gapminder_2007$fitted)^2)\nssto = sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\nrsqr = 1-(sse/ssto)\n\n\nIf your model is minimising the error, then what happens if you have 2 predictors:\n\\[\nY = a + b_1X_1 + b_2X_2 + e\n\\]\nSave the residuals after 1 correlation:\n\n\nCode\nres_gdp = gapminder_2007$lifeExp - gapminder_2007$fitted\ncor.test(gapminder_2007$pop, res_gdp)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$pop and res_gdp\nt = 1.3842, df = 140, p-value = 0.1685\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.04948127  0.27564447\nsample estimates:\n      cor \n0.1161931"
  },
  {
    "objectID": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "href": "regressions/simpleRegressions.html#proportion-of-variance-explained",
    "title": "Simple regression (incomplete)",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\nIn correlations we discussed how the strength of association is the proportion of variance of y explained by x. For simple regression, this is also the case:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply the above formula to see what R is for \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\n\nCode\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap))\n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) \n  )\n\n\n[1] 0.6786624\n\n\nCode\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)\n\n\n[1] 0.6786624\n\n\nCode\n# r^2\ncor(gapminder_2007$lifeExp, gapminder_2007$gdpPercap)^2 \n\n\n[1] 0.4605827\n\n\nWe can confirm that squaring R gives r^2\n\n\nCode\nsummary(life_expectancy_model)\n\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.828  -6.316   1.922   6.898  13.128 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(>|t|)    \n(Intercept) 59.56565008  1.01040864   58.95 <0.0000000000000002 ***\ngdpPercap    0.00063713  0.00005827   10.93 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.899 on 140 degrees of freedom\nMultiple R-squared:  0.4606,    Adjusted R-squared:  0.4567 \nF-statistic: 119.5 on 1 and 140 DF,  p-value: < 0.00000000000000022\n\n\nNow lets see if the same logic works when there are 2 or more predictors that interact to predict each other:\n\\[\nr = \\frac{var_{xyz}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})(z_i-\\bar{z})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2 * \\sum(z_i-\\bar{z})^2}}\n\\]\n\n\nCode\nsum(\n  (gapminder_2007$lifeExp-mean(gapminder_2007$lifeExp)) * \n  (gapminder_2007$gdpPercap-mean(gapminder_2007$gdpPercap)) *\n  (gapminder_2007$pop-mean(gapminder_2007$pop))  \n  )/\n  sqrt(\n    sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2) *\n    sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n    sum((gapminder_2007$pop - mean(gapminder_2007$pop))^2) \n  )\n\n\n[1] -0.007330224\n\n\n\n\nCode\nsummary(lm(lifeExp ~  gdpPercap + pop, data = gapminder_2007))\n\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap + pop, data = gapminder_2007)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -6.119   1.899   7.018  13.383 \n\nCoefficients:\n                   Estimate      Std. Error t value            Pr(>|t|)    \n(Intercept) 59.205198140717  1.040398672164  56.906 <0.0000000000000002 ***\ngdpPercap    0.000641608517  0.000058176209  11.029 <0.0000000000000002 ***\npop          0.000000007001  0.000000005068   1.381               0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.87 on 139 degrees of freedom\nMultiple R-squared:  0.4679,    Adjusted R-squared:  0.4602 \nF-statistic: 61.11 on 2 and 139 DF,  p-value: < 0.00000000000000022\n\n\n\nWhy use regression?\nRegression builds on correlation by providing a more detailed view of your data and with this provides an equation that can be used for any future predicting and optimizing of your data.\n\n\n[1] 4\n\n\nThe differences between regression and correlation"
  },
  {
    "objectID": "distributions/skewness.html",
    "href": "distributions/skewness.html",
    "title": "Skewness",
    "section": "",
    "text": "Parametric analyses are based on the assumption that the data you are analysing is normally distributed (see below):\n\n\nCode\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.001)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\np<- ggplot(\n  data = norm_data_frame, \n  aes(\n    x = x,\n    y = y\n  )\n) + geom_line()\n\n p +\n   xlim(c(-3,3)) +\n   geom_col(aes(fill=x)) +\n   scale_fill_gradient2(low = \"red\", \n                       high = \"blue\",\n                       mid =  \"white\",\n                       midpoint = median(norm_data_frame$x))+\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n   annotate(\"text\", x=-1.5, y=0.01, label= \"34.1%\") + \n   annotate(\"text\", x=-0.6, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.25, y=0.01, label= \"100%\") +\n   geom_vline(xintercept = -2) +\n   geom_vline(xintercept = -1) +\n   geom_vline(xintercept = 0) +\n   geom_vline(xintercept = 1) +\n   geom_vline(xintercept = 2) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\nWhite in the above figure represents the median. Note that the mean and median overlap in a normal distribution.\nIf your data fits a normal distribution, then you can draw conclusions based on certain facts about this distribution, e.g. the fact that 97.7% of your population should have a score that is more negative than +2 standard deviations above the mean (because Z-scores represent standard deviations from the mean). As a result, if your data is skewed:\n\n\nCode\nlibrary(fGarch)\n\nskew_data_frame<-data.frame(\n  x = norm_x,\n  y = dsnorm(\n    x = norm_x,\n    mean = 0,\n    sd = 1,\n    xi = 2,\n  )\n)\n\nggplot(\n  data = skew_data_frame, \n  aes(\n    x = x,\n    y = y\n  )\n) + \n  geom_line() + \n  geom_col(aes(fill=x)) +\n  scale_fill_gradient2(low = \"red\", \n                       high = \"blue\",\n                       mid =  \"white\",\n                       midpoint = -.15)+\n   theme(legend.position=\"none\") +\n  xlim(-3,3) +\n  ylab(\"Frequency\") +\n  xlab(\"Z-score\")\n\n\n\n\n\nWhite represents the median in the figure above.\nAs you can see from the above skewed distribution, the median is below the mean, consistent with the data being skewed. Importantly, the assumptions that we can make about what proportion of the population are 1 standard deviation above and below the mean are no longer valid, as more than half the population are below the mean in this case. This would suggest that non-parametric analyses could be more appropriate if your data is skewed.\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is.\nThe next section is a breakdown of the formula for those interested in it (but this is not crucial).\n\nOptional\nIf we want to manually calculate skewness:\n\\[\n\\tilde{\\mu_{3}} = \\sum((\\frac{x_i- \\bar{x}{}} {\\sigma} )^3) * \\frac{N}{(N-1) * (N-2)}\n\\] To do this in R you could calculate it manually. We’ll use Gapminder’s data from 2007:\n\n\nCode\nlibrary(gapminder)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nhist(gapminder_2007$lifeExp)\n\n\n\n\n\nCode\npositive_skew_n = length(gapminder_2007$lifeExp)\npositive_skewness = sum(((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))/sd(gapminder_2007$lifeExp))^3) * (\n  positive_skew_n / ((positive_skew_n-1) * (positive_skew_n - 2))\n)\n# to show the output:\npositive_skewness\n\n\n[1] -0.6887771\n\n\n… or just\nUse code from https://stackoverflow.com/a/54369572 to give you values for skewness (this has been chosen as this gives skewness and its standard error as calculated by major software like SPSS and JASP):\n\n\nCode\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nThe above function generated three columns: \\(estimate\\) of skewness (and kurtosis, but we’ll deal with kurtosis separately, \\(standard\\) \\(error\\) (SE) and the \\(skewness_z\\) score. Z-scores can capture how unlikely the \\(skewness\\) estimate is considering what you would normally expect. In this case, when you divide skewness by standard error, you get a z-score, and if the absolute value (i.e. ignoring whether it is positive or negative) of the z-score is greater than 1.96 then it is significantly skewed. If you want to understand why 1.96 is the main number, check out the subsection on normal distributions.\nOne way to deal with skewed data is to transform the data."
  },
  {
    "objectID": "distributions/skewness.html#skewness",
    "href": "distributions/skewness.html#skewness",
    "title": "Skewness - incomplete",
    "section": "Skewness",
    "text": "Skewness\nData can be negatively skewed, where the mean is less than the median:\n\n\nCode\n# Skewed to the right\nnegative_skew = rbeta(10000,5,2)\nhist(negative_skew, main = \"Negatively skewed data\")\nabline(\n  v=mean(negative_skew),  # where the line for the mean will be \n  lwd=5\n)\nabline(\n  v=median(negative_skew), \n  lwd=3,\n  lty=3\n)\n\n\n\n\n\nThe thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see central tendancy below)\nOr positively skewed, where the median is less than the mean:\n\n\nCode\n# Skewed to the left\npositive_skew = rbeta(10000,2,5)\nhist(positive_skew, main=\"Positively skewed data\")\nabline(\n  v=mean(positive_skew),  # where the line for the mean will be \n  lwd=5\n)\nabline(\n  v=median(positive_skew), # where the line for the median will be\n  lwd=3,\n  lty=3\n)\n\n\n\n\n\nThe thick lines represents the mean; the dashed lines represents the median. The bigger the distance between these, the less normally distributed your data is. (see central tendancy below)\nSo now that we know what skewed distributions look like, we now need to quantify how much of a problem with skewness there is. If we add together the amount of skewness for each data point together and then divide by a general summary of the total standard deviation, then you get an estimate of skewness. The next section is a breakdown of the formula for those interested in it (but this is not crucial), but the key point point is that outliers skew the data, and so outliers that are larger than the mean positively skew the data, and outliers below the mean negatively skew it. If there are an equal number of outliers on either side of the mean then the data will not be skewed.\n\nOptional\nIf we want to manually calculate skewness:\n\\[\n\\tilde{\\mu_{3}} = \\sum((\\frac{x_i- \\bar{x}{}} {\\sigma} )^3) * \\frac{N}{(N-1) * (N-2)}\n\\] To do this in R you could calculate it manually\n\n\nCode\n# applying this to the positive skew data above\npositive_skew_n = length(positive_skew)\npositive_skewness = sum(((positive_skew - mean(positive_skew))/sd(positive_skew))^3) * (\n  positive_skew_n / ((positive_skew_n-1) * (positive_skew_n - 2))\n)\n# to show the output:\npositive_skewness\n\n\n[1] 0.5822704\n\n\n… or just\n\nUse code from https://stackoverflow.com/a/54369572 to give you values for skewness (this has been chosen as this gives skewness and its standard error as calculated by major software like SPSS and JASP):\n\n\nCode\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(positive_skew)\n\n\n           estimate         se   zScore\nskew      0.5822704 0.02449122 23.77465\nkurtosis -0.1851028 0.04897755 -3.77934\n\n\nThe above function generated three columns: \\(estimate\\) of skewness (and kurtosis, but we’ll deal with kurtosis separately, \\(standard\\) \\(error\\) (SE) and the \\(skewness_z\\) score. Z-scores can capture how unlikely the \\(skewness\\) estimate is considering what you would normally expect. In this case, when you divide skewness by standard error, you get a z-score, and if the absolute value (i.e. ignoring whether it is positive or negative) of the z-score is greater than 1.96 then it is significantly skewed. If you want to understand why 1.96 is the main number, check out the subsection on normal distributions.\nOne way to deal with skewed data is to transform the data."
  },
  {
    "objectID": "distributions/ttests.html",
    "href": "distributions/ttests.html",
    "title": "t-tests (incomplete)",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for …\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "distributions/kurtosis.html",
    "href": "distributions/kurtosis.html",
    "title": "Kurtosis (incomplete)",
    "section": "",
    "text": "Kurtosis refers to how influenced a distribution is by its tails.\n\\(kurtosis=\\frac{(N*(N+1)*m4 - 3*m2^2*(w-1))}{((N-1)*(N-2)*(N-3)*s1^4)}\\)\n\\(kurtosis_{SE} = sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)))\\)\n\n\nCode\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\n\n\n\nIs Platykurtic vs. leptokurtic data more sensitive to false positives!\nor overly clustered around the mean (leptokurtik)\n\n\nCode\n# \n# library(ggplot2)\n# # https://stackoverflow.com/a/12429538\n# norm_x<-seq(-4,4,0.01)\n# norm_y<-dnorm(-4,4,0.0)/2\n# \n# norm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n# \n# \n# shade_2.3 <- rbind(\n#   c(-8,0), \n#   subset(norm_data_frame, x > -8), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_13.6 <- rbind(\n#   c(-2,0), \n#   subset(norm_data_frame, x > -2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_34.1 <- rbind(\n#   c(-1,0), \n#   subset(norm_data_frame, x > -1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_50 <- rbind(\n#   c(0,0), \n#   subset(norm_data_frame, x > 0), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# shade_84.1 <- rbind(\n#   c(1,0), \n#   subset(norm_data_frame, x > 1), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# shade_97.7 <- rbind(\n#   c(2,0), \n#   subset(norm_data_frame, x > 2), \n#   c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n# \n# \n# p<-qplot(\n#   x=norm_data_frame$x,\n#   y=norm_data_frame$y,\n#   geom=\"line\"\n# )\n# \n#  p +\n#    geom_polygon(\n#      data = shade_2.3,\n#      aes(\n#        x,\n#        y,\n#        fill=\"2.3\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_13.6,\n#      aes(\n#        x,\n#        y,\n#        fill=\"13.6\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_34.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"34.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_50,\n#      aes(\n#        x,\n#        y,\n#        fill=\"50\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_84.1,\n#      aes(\n#        x,\n#        y,\n#        fill=\"84.1\"\n#       )\n#     ) +\n#    geom_polygon(\n#      data = shade_97.7, \n#      aes(\n#        x, \n#        y,\n#        fill=\"97.7\"\n#       )\n#     ) +\n#    xlim(c(-4,4)) +\n#    \n#    annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n#    annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n#    annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n#    annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n#    annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n#    annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n#    xlab(\"Z-score\") +\n#    ylab(\"Frequency\") +\n#    theme(legend.position=\"none\")\n\n\nor underly clustered around the mean (platykurtik)"
  },
  {
    "objectID": "distributions/transforming.html",
    "href": "distributions/transforming.html",
    "title": "Transforming Data (incomplete)",
    "section": "",
    "text": "A lot of analyses is dependent on data being normally distributed. One problem with your data might be that it is skewed. Lets focus on the gapminder data from 2007 to see if the \\(gdp\\) and \\(life\\) \\(expectancy\\) data is skewed, and how this could be addressed.\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  \n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nSo it looks like both the \\(gdp\\) and \\(life\\) \\(expectancy\\) are skewed (as their z-scores are greater than 1.96). Lets double check with a quick plot:\n\nplot(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n\n\nIt’s relatively easy to see the skewness of \\(gdp\\), but \\(life\\) \\(expectancy\\) is a bit more subtle. We can complete a logarithmic transformation to reduce the skewness, so lets do that to both variables and then replot the data:\n\ngapminder_2007$gdpPercap_log <- log(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_log <- log(gapminder_2007$lifeExp)\nplot(\n  gapminder_2007$gdpPercap_log,\n  gapminder_2007$lifeExp_log\n)\n\n\n\n\nLets check if the skewness has changed for the \\(gdp\\):\n\n# original gdp\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\n# transformed gdp (log)\nspssSkewKurtosis(gapminder_2007$gdpPercap_log)\n\n           estimate        se     zScore\nskew     -0.1540524 0.2034292 -0.7572778\nkurtosis -1.1256815 0.4041614 -2.7852277\n\n\nSo, transforming the \\(gdp\\) did reduce skewness but increased Kurtsosis, so beware that applying a transformation may cause other problems! Lets check whether the log transformation reduced skewness for \\(life\\) \\(expectancy\\):\n\n# original life expectancy\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n# transformed life expectancy\nspssSkewKurtosis(gapminder_2007$lifeExp_log)\n\n           estimate        se    zScore\nskew     -0.9043617 0.2034292 -4.445584\nkurtosis -0.4136699 0.4041614 -1.023527\n\n\nSeems like the answer is no.\nAn important question is whether the associations between your variables change after transformation, so let’s check that next:\n\n# correlation on original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation on transformed data\ncor.test(\n  gapminder_2007$gdpPercap_log,\n  gapminder_2007$lifeExp_log\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_log and gapminder_2007$lifeExp_log\nt = 14.752, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7060729 0.8372165\nsample estimates:\n      cor \n0.7800706 \n\n\nThe log transformed data is more strongly associated with each other than the original data. However, there are other transformations that are additive (plus or minus a value to the original data) or multiplicative (which can also include division) that will not change the association (check whether the r-value \\(cor\\) changes):\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data + 5 to one variable\ncor.test(\n  gapminder_2007$gdpPercap + 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap + 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with original data - 10 to one variable\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp - 10\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp - 10\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with multiplication of 5 to one variable\ncor.test(\n  gapminder_2007$gdpPercap * 5,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap * 5 and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\nSome transformations will change the association if you apply them to one variable:\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with one squared variable\ncor.test(\n  gapminder_2007$gdpPercap ^2,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap^2 and gapminder_2007$lifeExp\nt = 7.8576, df = 140, p-value = 9.372e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4274353 0.6579774\nsample estimates:\n      cor \n0.5532109 \n\n\nbut not if you apply similar transformations to both variables\n\n# correlation with original data\ncor.test(\n  gapminder_2007$gdpPercap,\n  gapminder_2007$lifeExp\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n# correlation with both variables squared\ncor.test(\n  sqrt(gapminder_2007$gdpPercap),\n  sqrt(gapminder_2007$lifeExp)\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(gapminder_2007$gdpPercap) and sqrt(gapminder_2007$lifeExp)\nt = 12.981, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6539524 0.8057025\nsample estimates:\n      cor \n0.7390648"
  },
  {
    "objectID": "distributions/normal.html#bell-curve-aka-normal-distribution",
    "href": "distributions/normal.html#bell-curve-aka-normal-distribution",
    "title": "Normal Distribution",
    "section": "Bell curve (AKA normal distribution)",
    "text": "Bell curve (AKA normal distribution)\nParametric statistics often compare values to a normal distribution of expected data, based on the estimated mean and SD. Lets start by showing a (made up) normal distribution of heights in centimeters:\nSo lets say the average person’s height is 150cm, and the standard deviation of height across the population is 10cm. The data would look something like:\n\n\nCode\n# Plot a normal distribution of heights\npopulation_heights_x <- seq(\n  120,    # min\n  180,    # max\n  by = 1  \n)\npopulation_heights_y <- dnorm(\n  population_heights_x,\n  mean = 150,\n  sd   = 10\n)\nplot(\n  population_heights_x,\n  population_heights_y,\n  xlab = \"height\",\n  ylab = \"frequency\"\n)\n# Add line to show mean and median\nabline(\n  v=150,  # where the line for the mean will be \n  lwd=5\n)\n\n\n\n\n\nYou can see that the above fits a bell-curve, and the line in the middle represents both the mean and the median as the data is symmetrical. In reality, almost no data is a perfect bell-curve, but there are ways to test if the data isn’t sufficiently normal to use parametric tests with.\nNext, we will look at how normal distributions allow you to transform your data to z-scores to compare to a z-distribution."
  },
  {
    "objectID": "distributions/normal.html#z-scores-and-the-z-distribution",
    "href": "distributions/normal.html#z-scores-and-the-z-distribution",
    "title": "Normal Distribution",
    "section": "Z-scores and the z-distribution",
    "text": "Z-scores and the z-distribution\nA z-score is a standardised value that captures how many standard deviations above or below the mean an individual value is. Thus, to calculate the z-score\n\\[\nZ = \\frac{individualScore-averageScore}{StandardDeviation}\n\\]\nOr in formal terminology:\n\\[\nZ = \\frac{x-\\bar{x}}{\\sigma}\n\\]\nThe calculated score can then be applied to a z-distribution, which is parametric/normally distributed. Lets have a look at a z-distribution:\n\n\nCode\n# vector for the x-axis\nz_score_x <- seq(\n  -3,    # min\n  3,    # max\n  by = .1  \n)\n\n# vector for the y-axis\nz_score_y <- dnorm(\n  z_score_x,\n  mean = 0,\n  sd   = 1\n)\n\nplot(\n  z_score_x,\n  z_score_y,\n  xlab = \"z-score (SDs from the mean)\",\n  ylab = \"frequency\"\n)\n\n\n\n\n\nIf you compare the height distribution above to the z-score distribution, you should see that they are identically distributed. This is useful, as we know what percentage of a population fits within each standard deviation of a normal distribution:\n\n\nCode\nlibrary(ggplot2)\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_13.6 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\nCode\n p +\n   \n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6, \n     aes(\n       x, \n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   \n   annotate(\n     \"text\", \n     x=0.5, \n     y=0.01, \n     label= \"34.1%\"\n   ) + \n   annotate(\n     \"text\", \n     x=1.5, \n     y=0.01, \n     label= \"13.6%\"\n   ) + \n   annotate(\n     \"text\", \n     x=2.3, \n     y=0.01, \n     label= \"2.3%\"\n   ) +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\nThe above visualises how 34.1% of a population’s scores will be between 0 and 1 standard deviation from the mean, 13.6% of the population’s scores will be between 1-2 standard deviations from the mean, and 2.3% of the population will be more then 2 standard deviations from the mean. Remember that the normal distribution is symmetrical, so we also know that 34.1% of the population’s score will be between -1 to 0 standard deviations from the mean, 13.6% of the population’s score will be between -2 to -1 standard deviations from the mean, and 2.3% of the population’s score will be more negative than -2 standard deviations from the mean. Lets look at this cumulative distribution:\n\n\nCode\n# https://stackoverflow.com/a/12429538\nnorm_x<-seq(-4,4,0.01)\nnorm_y<-dnorm(-4,4,0.0)\n\nnorm_data_frame<-data.frame(x=norm_x,y=dnorm(norm_x,0,1))\n\n\nshade_2.3 <- rbind(\n  c(-8,0), \n  subset(norm_data_frame, x > -8), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_13.6 <- rbind(\n  c(-2,0), \n  subset(norm_data_frame, x > -2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_34.1 <- rbind(\n  c(-1,0), \n  subset(norm_data_frame, x > -1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_50 <- rbind(\n  c(0,0), \n  subset(norm_data_frame, x > 0), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\nshade_84.1 <- rbind(\n  c(1,0), \n  subset(norm_data_frame, x > 1), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\nshade_97.7 <- rbind(\n  c(2,0), \n  subset(norm_data_frame, x > 2), \n  c(norm_data_frame[nrow(norm_data_frame), \"X\"], 0))\n\n\np<-qplot(\n  x=norm_data_frame$x,\n  y=norm_data_frame$y,\n  geom=\"line\"\n)\n\n p +\n   geom_polygon(\n     data = shade_2.3,\n     aes(\n       x,\n       y,\n       fill=\"2.3\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_13.6,\n     aes(\n       x,\n       y,\n       fill=\"13.6\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_34.1,\n     aes(\n       x,\n       y,\n       fill=\"34.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_50,\n     aes(\n       x,\n       y,\n       fill=\"50\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_84.1,\n     aes(\n       x,\n       y,\n       fill=\"84.1\"\n      )\n    ) +\n   geom_polygon(\n     data = shade_97.7, \n     aes(\n       x, \n       y,\n       fill=\"97.7\"\n      )\n    ) +\n   xlim(c(-4,4)) +\n   \n   annotate(\"text\", x=-2.3, y=0.01, label= \"13.6%\") + \n   annotate(\"text\", x=-1.4, y=0.01, label= \"34.1%\") + \n   annotate(\"text\", x=-0.3, y=0.01, label= \"50%\") + \n   annotate(\"text\", x=0.5, y=0.01, label= \"84.1%\") + \n   annotate(\"text\", x=1.5, y=0.01, label= \"97.7%\") + \n   annotate(\"text\", x=2.3, y=0.01, label= \"100%\") +\n   xlab(\"Z-score\") +\n   ylab(\"Frequency\") +\n   theme(legend.position=\"none\")\n\n\n\n\n\nThe above figure visualises how 13.6% of the population have score that is more negative than -2 standard deviations from the mean, 34.1% of the population have a standard deviation that is more negative than -1 standard deviations from the mean (this also include all the people who are more than -2 standard deviations from the mean), etc.\nWe can now use the above information to identify which percentile an individual is within a distribution.\nFor example, let’s imagine that an individual called Jane wants to know what percentile she’s at with her height. Lets imagine she is 170cm tall, the mean height of people 150cm, and the SD 10cm. That would make her z-score:\n\\[\nZ_{score} = \\frac{170 - 150}{10} = 2\n\\]\nAs we can see from the figure above, that puts her above 97.7% of the population, putting her in the top 2.3%."
  },
  {
    "objectID": "correlations/correlations.html",
    "href": "correlations/correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "Please make sure you’ve read about variance within the dispersion section before proceeding with this page.\nCorrelations capture how much two variables are associated with each other by calculating the proportion of the total variance explained by how much the two variables vary together (explained below). To understand this, we need to think about how each variable varies independently, together and compare the two. We’ll use the gapminder data to look at how how life expectancy correlated with GDP in 2007:\nNote that in the figure above each dot represents an individual point from our data.Each dot represents an individual country (with the x-coordinte being the GDP per capita, and the y-coordinate being the Life Expectancy).\nGenerally speaking, a correlation tells you how much of the total variance is explained by how much the variables vary together. To understand this, lets start by clarifying how you understand the variance of individual variables."
  },
  {
    "objectID": "correlations/correlations.html#variance-of-individual-variables",
    "href": "correlations/correlations.html#variance-of-individual-variables",
    "title": "Correlations",
    "section": "Variance of individual variables",
    "text": "Variance of individual variables\nFor more insight into variance as a concept, have a look at dispersion, but here we will focus on variance within the context of correlations. You have 2 variables, x (for the x-axis) and y (for the y-axis), and the variance for each of those is:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\n\\[\nvar_y = \\frac{\\sum(y_i-\\bar{y})^2}{N-1}\n\\]\nJust a reminder of what each part of the formula is:\n\\(\\sum\\) is saying to add together everything\n\\(x_i\\) refers to each individual’s x-score\n\\(y_i\\) refers to each individual’s y-score\n\\(\\bar{x}\\) refers to the mean x-score across all participants\n\\(\\bar{y}\\) refers to the mean y-score across all participants\n\\(N\\) refers to the number of participants\n\\(N-1\\) is degrees of freedom, used for this calculation as you are calculating the variance within a sample, rather than variance within the whole population (which you would just use N for; this is explained further in the dispersion section).\nThe \\(SD\\) (Standard deviation; which is just the square root of variance) of how data is distributed around the mean \\(life\\)\\(expectancy\\) (per capita) can be visualised as follows within the gapminder \\(gdp*lifeExpectancy\\) in the light blue box:\n\n\nCode\n# Basic scatter plot\n\nlife_exp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) + \n  theme(\n    legend.position = \"none\",\n    text=element_text(\n      family=\"gochi\",\n      size = 20\n    )\n  )\n\nggsave(\"life_exp_resid.png\", life_exp_resid)\n\n\nSaving 7 x 5 in image\n\n\nNote that in the figure above the horizontal blue dotted line represent the mean of Life Expectancy. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom.\nLets look at the variance of GDP per capita:\n\n\nCode\ngdp_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"resid\"\n    )\n  )\nggsave(\"gdp_resid.png\", gdp_resid)\n\n\nSaving 7 x 5 in image\n\n\nCode\ngdp_resid\n\n\n\n\n\nNote that in the figure above the vertical blue dotted line represents the mean gdp per capita. Variance is the total after squaring all the residuals (pink lines) and dividing this total by the degrees of freedom."
  },
  {
    "objectID": "correlations/correlations.html#total-variance",
    "href": "correlations/correlations.html#total-variance",
    "title": "Correlations",
    "section": "Total variance",
    "text": "Total variance\nA correlation aims to explain how much of the \\(total\\) \\(variance\\) is explained by the overlapping variance between the x and y axes. So we need to capture the \\(total\\) \\(variance\\) separately for the x and y axes. We do this by multiplying the variance for \\(x\\) by the variance for \\(y\\) (and square rooting to control for the multiplication itself):\n\\[\ntotalVariance = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}*\\sqrt{\\frac{\\sum(y_i-\\bar{y})^2}{N-1}}\n\\]\n(Which is the same as:\n\\[\ntotalVariance = \\frac{\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}{N-1}\n\\]\n)\nOr, to use the figures above:\n\n\n\n\n\n\n\n\n\n$Total$ $Var$ =\nsqrt( \\^2) $$ \\frac{}{N-1} $$\n$*$\nsqrt( \\^ 2) $$ \\frac{}{N-1} $$\n\n\n\nThis is analogous to understanding the total area of a rectangle by multiplying the length of each side with each other.\n\nShared variance between \\(x\\) and \\(y\\)\nAn important thing to note, is that variance of a single variable, in this case x:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})^2}{N-1}\n\\]\ncould also be written as:\n\\[\nvar_x = \\frac{\\sum(x_i-\\bar{x})(x_i-\\bar{x})}{N-1}\n\\]\nTo look at the amount that x and y vary together, we can adapt a formula for how much \\(x\\) varies (with itself as written above) to now look at how much \\(x\\) varies with \\(y\\):\n\\[\nvar_{xy} = \\frac{\\sum(x_i-\\bar{x})(\\color{red}{y_i-\\bar{y}})}{N-1}\n\\]\nThis can be visualised as the residuals from the means multiplied by each other for each data point:\n\n\nCode\nshared_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\nggsave(\"shared_resid.png\", shared_resid) \n\n\nSaving 7 x 5 in image\n\n\nCode\nshared_resid\n\n\n\n\n\n\n\nComparing \\(shared\\) \\(variance\\) (\\(var_{xy}\\)) to \\(total\\) \\(variance\\)\nTo complete a Pearson’s R correlation we need to compare the amount that x and y vary together to the total variance (in which you calculate how much x and y vary separately and multiply them) to calculate the proportion of \\(total\\) \\(variance\\) is explained by the \\(shared\\) \\(variance\\) (\\(var_{xy}\\)).\n\\[\n\\frac{var_{xy}}{totalVariance} = \\frac{(\\sum(x_i-\\bar{x})(y_i-\\bar{y}))/(N-1)}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}/(N-1)}\n\\]\nNote that both \\(var_{xy}\\) and \\(totalVariance\\) correct for the degrees of freedom, so the \\(N-1\\)s cancel each other out:\n\\[\nr = \\frac{var_{xy}}{totalVariance} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sqrt{\\sum(x_i-\\bar{x})^2*\\sum(y_i-\\bar{y})^2}}\n\\]\nLets apply this to the gapminder data above to calculate \\(r\\):\n\n\nCode\nvarxy = \n  sum(\n    (gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap)) * \n    (gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))\n   )\n\n\ntotalvar = sqrt(\n  sum((gapminder_2007$gdpPercap - mean(gapminder_2007$gdpPercap))^2) * \n  sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n)\nvarxy/totalvar\n\n\n[1] 0.6786624\n\n\nIf the above calculation is correct, we’ll get exactly the same value when using the cor.test function:\n\n\nCode\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nt = 10.933, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5786217 0.7585843\nsample estimates:\n      cor \n0.6786624 \n\n\nTo visualise what proportion of the variance is captured by \\(var_{xy}\\):\n\n\n\n\n\n\n\n\n\n\n\n\nsqrt( \\^2\n\n\n\nA question you might have at this point, is whether the above figure seems consistent with 67.9% of \\(total\\) \\(variance\\) being explained by overlapping variance between \\(x\\) and \\(y\\)?\nIf \\(x\\) and \\(y\\) vary together, then you would expect either:\n\na higher \\(x\\) data point should be associated with a higher \\(y\\) data point (positive association)\na higher \\(x\\) data point should be associated with a lower \\(y\\) data point (negative association)\n\nThe bottom half of the figure above doesn’t give you that much insight into how consistently \\(x\\) and \\(y\\) are positive or negatively associated with each other, but the top half does;\n\n\nCode\nshared_resid\n\n\n\n\n\nIf there is a positive association, then you would expect there to be consistency in \\(x\\) and \\(y\\) both being above their own respective means, or both being below their respective means consistently, which is what we see above.\nIf there is a negative association, you would expect \\(y\\) to generally be below its mean when \\(x\\) is above its mean, and vice-versa. Lets visualise this by transformingthe \\(life\\) \\(expectancy\\) to be inverted by subtracting it from 100. This will make younger people older and older people younger:\n\n\nCode\ninverted_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap, \n    y=125-lifeExp\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita\") +\n  ylab(\"Life Expectancy\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap),\n      yend = 125-lifeExp,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = mean(125-lifeExp),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(125-gapminder_2007$lifeExp), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\ninverted_resid\n\n\n\n\n\nThe \\(Pearson's\\) \\(r\\) is now the reverse of the data before this transformation, i.e. r=-.679. Notice how there’s consistency in the above average \\(x\\) values being associated with below average \\(y\\) values, and vice-versa.\nYou may have noticed that the data above looks like it’s not normally distributed, so lets check skewness and kurtosis to see if we should use Spearman’s Rho (AKA Spearman’s Rank) instead:\n\n\nCode\n# Skewness and kurtosis and their standard errors as implement by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis=function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n\n  ## z-scores added by reading-psych\n  zskew = skew/sdskew\n  zkurtosis = kurtosis/sdkurtosis\n\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis, zskew, zkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\",\"zScore\")))\n  return(mat)\n}\nspssSkewKurtosis(gapminder_2007$gdpPercap)\n\n\n          estimate        se    zScore\nskew     1.2241977 0.2034292 6.0178067\nkurtosis 0.3500942 0.4041614 0.8662238\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp)\n\n\n           estimate        se    zScore\nskew     -0.6887771 0.2034292 -3.385832\nkurtosis -0.8298204 0.4041614 -2.053191\n\n\nAs GDP and Life Expectancy skewness and (kurtosis for life expectancy) estimates are more than 1.96 * their standard errors (i.e. their z-scores are above 1.96), we have significant evidence that the data for both variabels is not normally distributed, and thus we can/should complete a Spearman’s Rank/Rho correlation (in the next subsection)."
  },
  {
    "objectID": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "href": "correlations/correlations.html#spearmans-rank-aka-spearmans-rho",
    "title": "Correlations",
    "section": "Spearman’s Rank (AKA Spearman’s Rho)",
    "text": "Spearman’s Rank (AKA Spearman’s Rho)\nSpearman’s Rank correlation is identical to a Pearson correlation (described above), but adds a step of converting all the data into ranks before conducting any analyses. This is useful because ranks are not vulnerable to outlier (i.e. unusually extreme) data points. Let’s now turn the gapminder data we’ve been working with above into ranks and then run a Pearson’s correlation on it to confirm this:\n\n\nCode\ngapminder_2007$gdpPercap_rank <- rank(gapminder_2007$gdpPercap)\ngapminder_2007$lifeExp_rank <- rank(gapminder_2007$lifeExp)\n\n\nLets do a quick check to see that ranking the data addresses the problems with skewness and kurtosis:\n\n\nCode\nspssSkewKurtosis(gapminder_2007$gdpPercap_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nCode\nspssSkewKurtosis(gapminder_2007$lifeExp_rank)\n\n\n         estimate        se    zScore\nskew          0.0 0.2034292  0.000000\nkurtosis     -1.2 0.4041614 -2.969111\n\n\nThis has successfully removed any issue with skewness of the data, but has made the data more platykurtic (i.e. flatter). A problem with platykurtic data is that parametric tests might be over sensitive to identifying significant effects (see kurtosis), i.e. be at a higher risk of false positives. This is evidence that using a Spearman’s Rank may increase a risk of a false-positive (at least with this data), so another transformation of the data may be more appropriate to avoid this problem with kurtosis.\nFor now, lets focus on how much of the variance in ranks is explained in the overlap in variance of \\(gdp\\) and \\(life\\) \\(expectancy\\) ranks:\n\n\nCode\n# Pearson correlation on **ranked** data:\ncor.test(gapminder_2007$gdpPercap_rank, gapminder_2007$lifeExp_rank, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gapminder_2007$gdpPercap_rank and gapminder_2007$lifeExp_rank\nt = 19.642, df = 140, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8055253 0.8950257\nsample estimates:\n      cor \n0.8565899 \n\n\nCode\n# Spearman correlation applied to original data (letting R do the ranking)\ncor.test(gapminder_2007$gdpPercap, gapminder_2007$lifeExp, method = \"spearman\")\n\n\n\n    Spearman's rank correlation rho\n\ndata:  gapminder_2007$gdpPercap and gapminder_2007$lifeExp\nS = 68434, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8565899 \n\n\nThe \\(r\\) value is now .857, suggesting that the overlap between \\(gdp\\) and \\(life\\) \\(expectancy\\) explains 85.7% of the total variance of the ranks for both of them.\nLets visualise this using similar principles above on the ranks of \\(gdp\\) and \\(life\\) \\(expectancy\\):\n\n\nCode\nrank_resid <- ggplot(\n  data = gapminder_2007, \n  aes(\n    x=gdpPercap_rank, \n    y=lifeExp_rank\n  )\n) + \n  geom_point() +\n  geom_vline(\n    xintercept = mean(gapminder_2007$gdpPercap_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  ) +\n  #coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"GDP per capita (RANK)\") +\n  ylab(\"Life Expectancy (RANK)\") +\n  geom_segment(\n    aes(\n      xend = mean(gdpPercap_rank),\n      yend = lifeExp_rank,\n      color = \"GDP residuals\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      xend = gdpPercap_rank,\n      yend = mean(lifeExp_rank),\n      color = \"Life Expectancy Residuals\"\n    )\n  ) +\n  geom_hline(\n    yintercept = mean(gapminder_2007$lifeExp_rank), \n    linetype   = \"dashed\",  \n    color      = \"#006599\", \n    size       = 1\n  )\n\nrank_resid\n\n\n\n\n\nYou may notice that the variance from the mean in X and Y is more aligned in this figure than it was in the data before it was transformed into ranks (and is less skewed!):\n\n\nCode\nshared_resid"
  },
  {
    "objectID": "installing.html",
    "href": "installing.html",
    "title": "Installing R(Studio)",
    "section": "",
    "text": "Installing RStudio\n\nhttps://www.rstudio.com/products/rstudio/"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html",
    "href": "GeneralLinearModels/generalLinearModels.html",
    "title": "General Linear Models and Sum of Squares",
    "section": "",
    "text": "General linear models allow you to analyse data in which the dependent variable is continuous. For example, if you are analysing the height of a group of individuals, you might use one of the following analyses:\n\nt-test, comparisons between two conditions e.g. are males taller than females?\nregression, one or more predictors of a single outcome e.g. does foot size, weight etc. predict height? (Note that correlations are equivalent to a regression with a single predictor)\nANOVA, comparisons between 3 or more conditions or between multiple categorical factors, e.g. are there differences in height between sexes and nationalities?\n\nLinear refers to the dependent variable being continuous.\nGeneral refers to the fact that the independent variables can both be continuous (e.g. regression) or categorical (e.g. t-test or ANOVA).\nIn general linear models all analyses involve creating a model, and capturing what is and isn’t explained by the model (i.e. the error of the model). All analyses in general linear models can be formulated as:\n\\[\nData = Model + Error\n\\]\nData: The dependent variable in your analysis Model: A model which predicts a phenomenon. This could be multiple independent variables. Error: What data isn’t explained by the model."
  },
  {
    "objectID": "regressions/simpleRegressions.html",
    "href": "regressions/simpleRegressions.html",
    "title": "Simple regression (incomplete)",
    "section": "",
    "text": "Simple regression, also known as linear regression, builds on correlation. However, unlike correlation (which quantifies the strength of the linear relationship between a pair of variables), simple regression allows you to make predictions of an outcome variable based on a predictor variable.\nFor example, regression can be used to predict Life Expectancy in 2007 from GDP. Lets start by visualising the association between them:\n\n\nCode\nlibrary(gapminder)\nlibrary(ggplot2)\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\")\n\n\n\n\n\nLinear regression analysis operates by drawing the best fitting line (AKA the regression line; see the blue line above) through the data points. But this does not imply causation, as regression only models the data. Simple linear regression can’t tell us exactly what is influencing what (i.e. whether GDP per capita increases life expectancy), this will depend on the design of your study or your broader theoretical understanding. But for now, we can investigate whether \\(gdp\\) predicts \\(life\\) \\(expectancy\\). The formula for the above line could be written as:\n\\[\nLife Expectancy = intercept + gradient * GDP\n\\]\n\nGradient reflects how steep the line is\nIntercept is the point at which the regression line crosses the y-axis\n\nLet’s use coding magic to find out the intercept and the gradient (AKA slope):\n\n\nCode\n# turn off scientific notation so that the numbers are not e-numbers (and thus easier to read)\noptions(scipen = 999)\n\n# Make a model of a regression\nlife_expectancy_model <- lm(\n  data = gapminder_2007,\n  formula = lifeExp ~ gdpPercap # predict life expectancy from GDP\n)\n\n# report the intercept and the gradient (AKA slope) of each predictor (which will only be GDP)\nlife_expectancy_model$coefficients\n\n\n  (Intercept)     gdpPercap \n59.5656500780  0.0006371341 \n\n\nThe above shows that the intercept if 59.566, and that for every 1 unit ($) of GDP there is .0006 units more of life expectancy (or, in more useful terms, for every extra $10,000 dollars per person, the life expectancy goes up by 6 years).\nFor the above equation we will always retrieve values from the graph, except residuals, which is the ‘error’ and so a more complete formula for the outcome can be represented by the following formula\n\\[\noutcome = intercept + gradient * predictor + residual\n\\]\n\nResidual reflects what’s left over, and is not represented in the line of best fit formula because you can’t predict what’s left over. Residuals reflect the gap between each data point and the line of best fit:\n\n\n\nCode\ngapminder_2007$fitted = life_expectancy_model$coefficients[1] + # intercept\n  life_expectancy_model$coefficients[2]                       * # gradient\n  gapminder_2007$gdpPercap\n\nggplot(\n  data = gapminder_2007,\n  aes(\n    x = gdpPercap,\n    y = lifeExp,\n  )\n) + \n  # add data points as dots\n  geom_point() + \n  # add a line of best fit:\n  geom_smooth(\n    method='lm',  # linear model\n    formula=y~x   # predict y from x\n  ) +\n  # clearer x-axis label\n  xlab(\"GDP per capita\") +\n  # clearer y-axis label\n  ylab(\"Life expectancy\") +\n  \n  # add lines to show the residuals\n  geom_segment(\n    aes(\n      xend = gdpPercap,\n      yend = fitted,\n      color = \"resid\"\n    )\n  )\n\n\n\n\n\nThese residuals can be thought of the error, i.e. what the model failed to predict. In more mathematical terms, the model would be:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "href": "GeneralLinearModels/generalLinearModels.html#mean-as-the-simplest-model-of-data",
    "title": "General Linear Models and Sum of Squares",
    "section": "Mean as the simplest model of data",
    "text": "Mean as the simplest model of data\nIf you want to estimate what someone’s life expectancy would be in 2007, you could look at the mean life expectancy using the gapminder data. In terms of how this corresponds to the above model:\n\\[\nData = Model + Error\n\\]\n\\[\nestimatedLifeExpectancy = mean(lifeExpectancy) + Error\n\\]\n\nlibrary(gapminder)\n# create a new data frame that only focuses on data from 2007\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007     \n)\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\n\\[\nestimatedLifeExpectancy = 67.01 + Error\n\\]\nWhich could be visualised as:\n\nlibrary(ggplot2)\nggplot(\n  gapminder_2007, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_jitter() +\n  geom_hline(yintercept = mean(gapminder_2007$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFig. 1.\nIn English, the above model and figure allow you to predict that anyone’s life expectancy will be 67 years. However, as you can also see, there’s a huge amount of error, i.e. variance in life expectancy that is not explained by the model. These errors can be squared and summed to give the sum of squares, a statistic of how much error there is around the model:\n\\[\nSS = \\sum(Y_i-\\bar{Y})^2\n\\]\nWhich can be visualised as follows:\n\nggplot(\n  gapminder_2007, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFig. 2.\nYou can directly compare fig. 1. and fig. 2. to see how much error is associated with each data point compared to the model. Fig. 2. is positive because it is the distance of the data-point from the mean squared. If you added together all the squares (pink lines) in fig. 2. that would give you the sum of squares.\nAs you may have guessed, it is possible to have more precise models that have less error, and thus a smaller sum of squares. The sum of squares around the mean is also the total sum of squares, and the total variance. When we calculate the proportion of the variance that a model explains, we are comparing it to this variance around the mean.\nLet’s explore those possibilities now."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "href": "GeneralLinearModels/generalLinearModels.html#t-tests",
    "title": "General Linear Models and Sum of Squares",
    "section": "T-Tests",
    "text": "T-Tests\nT-tests are restricted to comparisons between 2 conditions/groups, so we will restrict the Gapminder data to allow a comparison between 2 continents. To see if life expectancy was different if you are born in Europe compared to the Americas, let’s first check what the sum of squares is when you just use the mean as the model of life expectancy across these contents:\n\ngapminder_americas_europe <- subset(\n  gapminder_2007,   # the data set\n  continent == \"Europe\" | continent == \"Americas\"\n)\n\nggplot(\n  gapminder_americas_europe, aes(x=rank(lifeExp), y=lifeExp)\n) + \n  geom_point() +\n  geom_hline(yintercept = mean(gapminder_americas_europe$lifeExp), color=\"blue\") +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = mean(lifeExp),\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFig. 3. The errors around the mean of life expectancy across Europe and American countries.\nOnce we square the errors in the pink lines above, we’ll get the squares:\n\nggplot(\n  gapminder_americas_europe, \n  aes(\n    x=rank(lifeExp), \n    # y is the square of the difference between each data point and the mean across all data poins. Once these are summed you will get the sum of squares.\n    y=(lifeExp-mean(lifeExp))^2\n  )\n) + \n  geom_point() +\n  geom_segment(\n    aes(\n      xend = rank(lifeExp),\n      yend = 0,\n      color = \"resid\"\n    )\n  ) +\n  theme(legend.position = \"none\")\n\n\n\nsum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\n\n[1] 953.4478\n\n\nAnd when you add all of these together:\n\\[\nSumOfSquares = \\sum(Y_i-\\bar{Y})^2 = 953.4478\n\\]\nSo if the model we create for a t-test would result in a smaller sum of squares then that suggests it’s a more precise model for estimating life expectancy than simply using the mean as a model. This is because this would mean there’s less error in this model. Let’s model this using a t-test. For this we will need to dummy code country:\n\n# create a column to place 1 or -1 for each row dependent on the country\ncontEffect = NA\ncontEffect[gapminder_americas_europe$continent == \"Europe\"] = 1\ncontEffect[gapminder_americas_europe$continent == \"Americas\"] = -1\ngapminder_americas_europe = cbind(contEffect,gapminder_americas_europe)\nrmarkdown::paged_table(head(gapminder_americas_europe))\n\n\n\n  \n\n\n\nNow that we have dummy coded the continent, we can create a new model to try to predict an individual’s life expectancy based on which continent they are from\n\\[\nY = intercept + \\beta * dummyVariable + Error\n\\]\n\\[\nlifeExp = mean(lifeExp) + \\beta * contEffect + Error\n\\]\n\nY being the predicted life expectancy.\n\\(\\bar{Y}\\) being the mean life expectancy regardless of continent. For a t-test this is also the \\(intercept\\).\n\\(\\beta\\) being how much to adjust the prediction based on which continent the person is from\n\\(contEffect\\) being 1 (Europe) or -1 (Americas) to reflect which continent the participant is from\n\\(Error\\) being any error in the prediction not captured by the model\n\nTo get the \\(intercept\\) and \\(\\beta\\) for the above formula let’s use the lm function in R:\n\ncontinent_ttest <- lm(lifeExp ~ contEffect, gapminder_americas_europe)\n\ncontinent_ttest$coefficients[1] \n\n(Intercept) \n   75.62836 \n\ncontinent_ttest$coefficients[2]\n\ncontEffect \n   2.02024 \n\ngapminder_americas_europe$t_fit = continent_ttest$coefficients[1] + # intercept\n  continent_ttest$coefficients[2]                       * # gradient\n  gapminder_americas_europe$contEffect\n\n\nggplot(gapminder_americas_europe, aes(x = contEffect, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = contEffect,\n      yend = t_fit,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = -1.9, \n    xend = -.1, \n    y = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n    yend = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = 0.1, \n      xend = 1.9, \n      y = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n      yend = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]\n    ),\n    color = \"blue\"\n  ) + \n  geom_segment(\n    aes(\n      x = - 1.9,\n      xend = 1.9,\n      y = mean(gapminder_americas_europe$lifeExp),\n      yend = mean(gapminder_americas_europe$lifeExp)\n    ),\n    color = \"dark green\"\n  )\n\nWarning: Use of `gapminder_americas_europe$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\nUse of `gapminder_americas_europe$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\n\n\n\n\n\nFig. X. Countries in the americas are dummy coded as -1 and countries in Europe are dummy coded as 1. Note that jittering has been used to help visualise variation within continents, and so all countries in Americas had a \\(contEffect\\) score of -1, even if the jittering above makes it look like participants from Europe had slightly different \\(contEffect\\) values to each other.\nSo now that we’ve visualised the predictions and the error, lets summarise these errors with their sum of squares:\n\n#temp_summary <- summary(lm(lifeExp ~ contEffect, data = gapminder_americas_europe))\nsummary(aov(lifeExp ~ contEffect, data = gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# between\noverall_mean <- mean(gapminder_americas_europe$lifeExp)\neurope_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1])\namerica_mean <- mean(gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1])\nss_between <- \n  sum(gapminder_americas_europe$contEffect == 1) * (europe_mean - overall_mean)^2 +\n  sum(gapminder_americas_europe$contEffect == -1) * (america_mean - overall_mean)^2\n  \ntop_half = ss_between\n\nss_within = (\n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] - europe_mean)^2) + \n  sum((gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1] - america_mean)^2)\n)\n  \nbottom_half = (ss_within/(length(gapminder_americas_europe$lifeExp) - 2))\n\ntop_half/bottom_half\n\n[1] 16.14453\n\n# Compare with a t-test\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\n4.018^2\n\n[1] 16.14432\n\n# look at a t-distribution compared to an f-distribution\n\nSo the new sum of squares is 730.8276, which is smaller than it was when we just used the mean regardless of continent (953.4478) which also summarises the total variance (around the mean). In fact, we can use these 2 numbers to calculate the \\(r^2\\) value (i.e. what proportion of the variance around the mean is explained by the model). The amount of variance explained by the model can be calculated by:\n\\[\ntotalSumOfSquares - modelSumOfSquares = totalError - modelError\n\\]\nThis allows us to calculate an r-value and thus a p-value:\n\nthis_r2 = 1 - sum(gapminder_americas_europe$t_res_squared)/sum((gapminder_americas_europe$lifeExp - mean(gapminder_americas_europe$lifeExp))^2)\nthis_r = sqrt(this_r2)\nthis_r\n\n[1] 1\n\nt.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  var.equal = T\n)\n\n\n    Two Sample t-test\n\ndata:  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1] and gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1]\nt = 4.018, df = 53, p-value = 0.0001864\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.023525 6.057435\nsample estimates:\nmean of x mean of y \n 77.64860  73.60812 \n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nthis_r2\n\n[1] 1\n\n\nYou may notice above that the manually calculated \\(r^2\\) value is identical to the “Multiple R-Squared”, rather than the “Adjusted R-squared”. So what’s the difference between r-squared and adjusted r-squared?\n\nEffect sizes (eta-squared and partial eta-squared)\n\nsummary(aov(lifeExp ~ contEffect, gapminder_americas_europe))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncontEffect   1  222.6  222.62   16.14 0.000186 ***\nResiduals   53  730.8   13.79                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\n\nCall:\nlm(formula = lifeExp ~ contEffect, data = gapminder_americas_europe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6921  -2.1364   0.4494   2.5671   7.0449 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  75.6284     0.5028 150.416  < 2e-16 ***\ncontEffect    2.0202     0.5028   4.018 0.000186 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.713 on 53 degrees of freedom\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.219 \nF-statistic: 16.14 on 1 and 53 DF,  p-value: 0.0001864\n\n222.62 /(222.62 +13.79)\n\n[1] 0.9416691\n\n\n\nmale_female_height <- data.frame(\n  sex = c(\"male\",\"male\",\"female\",\"female\",\"female\",\"female\",\"female\",\"female\"),\n  height = c(2.5,2.2,1.5,1.5,1.4,1.4,1.3,1.3),\n  sex_dummy = c(-1,-1,1,1,1,1,1,1)\n)\nmean(male_female_height$height[male_female_height$sex == \"male\"])\n\n[1] 2.35\n\nmean(male_female_height$height[male_female_height$sex == \"female\"])\n\n[1] 1.4\n\nsummary(lm(height ~ sex_dummy, data = male_female_height))\n\n\nCall:\nlm(formula = height ~ sex_dummy, data = male_female_height)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.15  -0.10   0.00   0.10   0.15 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.87500    0.04859  38.587 2.02e-08 ***\nsex_dummy   -0.47500    0.04859  -9.775 6.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.119 on 6 degrees of freedom\nMultiple R-squared:  0.9409,    Adjusted R-squared:  0.9311 \nF-statistic: 95.56 on 1 and 6 DF,  p-value: 6.592e-05\n\n\nTo show that we’ve achieved the same as a t-test, let’s run a between subjects t-test that assumes the variance is equal between the groups (which is an assumption of a general linear model), and see if the p-values are the same:\n\n#953.4478/730.8276\ncontinent_ttest <- t.test(\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == -1],\n  gapminder_americas_europe$lifeExp[gapminder_americas_europe$contEffect == 1],\n  # general linear models assume the variance between conditions is equal\n  var.equal = T\n)\ncontinent_model <- summary(lm(lifeExp ~ contEffect, gapminder_americas_europe))\n\ncontinent_ttest$p.value\n\n[1] 0.0001863749\n\ncontinent_model$coefficients[2,4] # p-value for the continent as a predictor\n\n[1] 0.0001863749\n\n\nThere are some advantages of conducting a t-test using the “lm” functionality:\n\nYou can capture residuals\nYou have more flexibility to make more complex models\n\nLet’s now see how we can proceed if we have a more complex design, i.e. 3 or more levels and/or more than 1 factor, using ANOVAs.\n\n## Automatic calculation\ngapminder_3_continents <- subset(\n  gapminder_2007, \n  continent == \"Europe\" | continent == \"Americas\" | continent == \"Africa\"\n)\n\nsummary(aov(lifeExp ~ factor(continent), data = gapminder_3_continents))\n\n                   Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(continent)   2  12017    6008   114.4 <2e-16 ***\nResiduals         104   5461      53                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Manual calculation\nss_between = (\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Europe\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Americas\") +\n  ((mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"])-mean(gapminder_3_continents$lifeExp))^2) * sum(gapminder_3_continents$continent==\"Africa\") \n)\n\ntotalSS = sum((gapminder_3_continents$lifeExp - mean(gapminder_3_continents$lifeExp))^2)\n\nshortcut_ss_within = totalSS - ss_between\n\nss_within_long = sum(\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Americas\"]))^2,\n  (gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]-mean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Africa\"]))^2\n)\n\n(ss_between/2)/(ss_within_long/(length(gapminder_3_continents$lifeExp)-3))\n\n[1] 114.4212\n\n\nTo visualise this\n\nlm_3_continents <- summary(lm(lifeExp ~ factor(continent), data = gapminder_3_continents))\nlm_3_continents$coefficients\n\n                          Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)               54.80604   1.004904 54.53856 2.466786e-78\nfactor(continent)Americas 18.80208   1.763600 10.66119 2.243512e-18\nfactor(continent)Europe   22.84256   1.661388 13.74908 3.900175e-25\n\nmean(gapminder_3_continents$lifeExp[gapminder_3_continents$continent==\"Europe\"])\n\n[1] 77.6486\n\ngapminder_3_continents$continent_mean = lm_3_continents$coefficients[1,1] # intercept, which is mean for Africa as there are only \"Americas\" and \"Europe\" factors.\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Americas\"] + lm_3_continents$coefficients[2,1]\n\ngapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] = gapminder_3_continents$continent_mean[gapminder_3_continents$continent == \"Europe\"] + lm_3_continents$coefficients[3,1]\n\n\n\nggplot(gapminder_3_continents, aes(x = continent, y = lifeExp)) +\n  geom_segment(\n    position = \"jitter\",\n    #arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n    aes(\n      xend = continent,\n      yend = continent_mean,\n      color = \"resid\"\n    )\n  ) + \n  geom_segment(aes(\n    x = 0.55, \n    xend = 1.45, \n    y = lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 1.55, \n    xend = 2.45, \n    y = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[2,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) + \n  geom_segment(aes(\n    x = 2.55, \n    xend = 3.45, \n    y = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1],\n    yend = lm_3_continents$coefficients[3,1] + lm_3_continents$coefficients[1,1]),\n    color = \"blue\"\n  ) +\n  geom_segment(\n    aes(\n      x = 0.55,\n      xend = 3.45,\n      y = mean(gapminder_3_continents$lifeExp),\n      yend = mean(gapminder_3_continents$lifeExp),\n      color = \"Overall Mean\"\n    )\n  )\n\nWarning: Use of `gapminder_3_continents$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\nUse of `gapminder_3_continents$lifeExp` is discouraged.\nℹ Use `lifeExp` instead.\n\n\n\n\n\nFig. XXX. Our model in the figure above considers the distance from each of the continent’s mean to the overall mean as part of the explained variance. For each data point, the squared distance from the the mean line to the overall mean line is part of the sum of squares at the top of the formula.\n\n\n2 x 2 ANOVA\nLet’s create data to allow us to compare between 2 years, and between Europe and Americas\n\n# create a new data frame that only focuses on data from 2007\ngapminder_2_by_2 <- subset(\n  gapminder,   # the data set\n  year == 2002 & continent == \"Africa\" |\n  year == 2007 & continent == \"Africa\" |\n  year == 2002 & continent == \"Europe\" |\n  year == 2007 & continent == \"Europe\"\n)\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nmanual calculation of f-value for 2 x 2\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\noverallMeanLifeExp = mean(gapminder_2_by_2$lifeExp)\ntotalVar = sum((gapminder_2_by_2$lifeExp - mean(gapminder_2_by_2$lifeExp))^2)\ngapminder_2_by_2 %>%\n  group_by(continent, year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> year_continent_means\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nyear_continent_means\n\n# A tibble: 4 × 5\n# Groups:   continent [2]\n  continent  year mean_lifeExp countries betweenSS\n  <fct>     <int>        <dbl>     <int>     <dbl>\n1 Africa     2002         53.3        52     4396.\n2 Africa     2007         54.8        52     3094.\n3 Europe     2002         76.7        30     6033.\n4 Europe     2007         77.6        30     6866.\n\nsum(year_continent_means$betweenSS)\n\n[1] 20389.47\n\ntotalVar\n\n[1] 30311.89\n\nsum(year_continent_means$betweenSS)/totalVar\n\n[1] 0.6726556\n\n(sum(year_continent_means$betweenSS))/totalVar\n\n[1] 0.6726556\n\ndf_total <- length(gapminder_2_by_2$country) - 1\ndf_res <- length(gapminder_2_by_2$country) - \n  1 - #data points\n  3   # predictors\n\nss_res = totalVar - sum(year_continent_means$betweenSS)\n\n1 - (ss_res/df_res)/(totalVar/df_total)\n\n[1] 0.6665179\n\n##\n# break down of types of variance\n##\ncontinent_df <- gapminder_2_by_2 %>%\n  group_by(continent) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(continent_df$betweenSS)\n\n[1] 20318.97\n\nyear_df <- gapminder_2_by_2 %>%\n  group_by(year) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  )\n\nsum(year_df$betweenSS)\n\n[1] 67.79278\n\n##\n# interaction\n##\nsum(year_continent_means$betweenSS) - sum(continent_df$betweenSS) - sum(year_df$betweenSS)\n\n[1] 2.70036\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\n20319 +\n68 +\n3\n\n[1] 20390\n\n\n\n\n3 way ANOVA\n\ngapminder_2_by_2$pop_split = \"high\"\ngapminder_2_by_2$pop_split[gapminder_2_by_2$pop < median(gapminder_2_by_2$pop)] = \"low\"\n\ngapminder_2_by_2 %>%\n  group_by(continent, year, pop_split) %>%\n  summarise(\n    mean_lifeExp = mean(lifeExp),\n    countries    = length(lifeExp),\n    betweenSS    = countries * ((overallMeanLifeExp - mean_lifeExp)^2)\n  ) -> three_way_summary\n\n`summarise()` has grouped output by 'continent', 'year'. You can override using\nthe `.groups` argument.\n\nsum(three_way_summary$betweenSS)\n\n[1] 20403.63\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent), data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1930  -4.7758  -0.1898   3.1180  22.4188 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               53.3252     1.0921  48.830   <2e-16\nfactor(year)2007                           1.4808     1.5444   0.959    0.339\nfactor(continent)Europe                   23.3754     1.8055  12.947   <2e-16\nfactor(year)2007:factor(continent)Europe  -0.5328     2.5533  -0.209    0.835\n                                            \n(Intercept)                              ***\nfactor(year)2007                            \nfactor(continent)Europe                  ***\nfactor(year)2007:factor(continent)Europe    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.875 on 160 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.6665 \nF-statistic: 109.6 on 3 and 160 DF,  p-value: < 2.2e-16\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent), gapminder_2_by_2))\n\n                                Df Sum Sq Mean Sq F value Pr(>F)    \nfactor(year)                     1     68      68   1.093  0.297    \nfactor(continent)                1  20319   20319 327.645 <2e-16 ***\nfactor(year):factor(continent)   1      3       3   0.044  0.835    \nResiduals                      160   9922      62                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n                                                  Df Sum Sq Mean Sq F value\nfactor(year)                                       1     68      68   1.067\nfactor(continent)                                  1  20319   20319 319.911\nfactor(pop_split)                                  1     13      13   0.212\nfactor(year):factor(continent)                     1      3       3   0.046\nfactor(year):factor(pop_split)                     1      0       0   0.000\nfactor(continent):factor(pop_split)                1      0       0   0.007\nfactor(year):factor(continent):factor(pop_split)   1      0       0   0.000\nResiduals                                        156   9908      64        \n                                                 Pr(>F)    \nfactor(year)                                      0.303    \nfactor(continent)                                <2e-16 ***\nfactor(pop_split)                                 0.646    \nfactor(year):factor(continent)                    0.830    \nfactor(year):factor(pop_split)                    0.989    \nfactor(continent):factor(pop_split)               0.931    \nfactor(year):factor(continent):factor(pop_split)  0.988    \nResiduals                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm(lifeExp ~ factor(year) * factor(continent) * factor(pop_split), gapminder_2_by_2))\n\n\nCall:\nlm(formula = lifeExp ~ factor(year) * factor(continent) * factor(pop_split), \n    data = gapminder_2_by_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5189  -4.8868  -0.3127   3.1208  22.0864 \n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   52.96628\nfactor(year)2007                                               1.53805\nfactor(continent)Europe                                       23.52019\nfactor(pop_split)low                                           0.69131\nfactor(year)2007:factor(continent)Europe                      -0.59779\nfactor(year)2007:factor(pop_split)low                         -0.06377\nfactor(continent)Europe:factor(pop_split)low                  -0.26305\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low  0.07923\n                                                              Std. Error\n(Intercept)                                                      1.59392\nfactor(year)2007                                                 2.21201\nfactor(continent)Europe                                          2.60286\nfactor(pop_split)low                                             2.21201\nfactor(year)2007:factor(continent)Europe                         3.65535\nfactor(year)2007:factor(pop_split)low                            3.12825\nfactor(continent)Europe:factor(pop_split)low                     3.65535\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    5.16944\n                                                              t value Pr(>|t|)\n(Intercept)                                                    33.230  < 2e-16\nfactor(year)2007                                                0.695    0.488\nfactor(continent)Europe                                         9.036 5.91e-16\nfactor(pop_split)low                                            0.313    0.755\nfactor(year)2007:factor(continent)Europe                       -0.164    0.870\nfactor(year)2007:factor(pop_split)low                          -0.020    0.984\nfactor(continent)Europe:factor(pop_split)low                   -0.072    0.943\nfactor(year)2007:factor(continent)Europe:factor(pop_split)low   0.015    0.988\n                                                                 \n(Intercept)                                                   ***\nfactor(year)2007                                                 \nfactor(continent)Europe                                       ***\nfactor(pop_split)low                                             \nfactor(year)2007:factor(continent)Europe                         \nfactor(year)2007:factor(pop_split)low                            \nfactor(continent)Europe:factor(pop_split)low                     \nfactor(year)2007:factor(continent)Europe:factor(pop_split)low    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.97 on 156 degrees of freedom\nMultiple R-squared:  0.6731,    Adjusted R-squared:  0.6585 \nF-statistic: 45.89 on 7 and 156 DF,  p-value: < 2.2e-16\n\n20319 +\n68 +\n3 + \n  13\n\n[1] 20403\n\n20403.63\n\n[1] 20403.63"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#anovas",
    "href": "GeneralLinearModels/generalLinearModels.html#anovas",
    "title": "General Linear Models and Sum of Squares",
    "section": "ANOVAs",
    "text": "ANOVAs\nANOVAs are useful to compare between categorical conditions if you have more than 2 conditions you want to compare or if you have multiple categorical predictor factors you want to investigate. Let’s start with an example of a 2 x 2 design, in which continent and whether the population is “large” or “small” are categorical predictors of life expectancy. First, we need to make a new categorical variable, in which countries with a population greater than medium have a “large” population, and other countries have a “small” population:\n\ngapminder_americas_europe$popCategorical = \"small\"\ngapminder_americas_europe$popDummy = -1\ngapminder_americas_europe$popCategorical[gapminder_americas_europe$pop > median(gapminder_americas_europe$pop)] = \"large\"\ngapminder_americas_europe$popDummy[gapminder_americas_europe$pop > median(gapminder_americas_europe$pop)] = 1\n\nanova_df <- gapminder_americas_europe[ ,c(\n  \"popCategorical\",\n  \"popDummy\",\n  \"continent\",\n  \"contDummy\",\n  \"lifeExp\"\n)]\nrmarkdown::paged_table(head(anova_df))\n\n\n\n  \n\n\n\nWe can now analyse this using a similar general linear model as the t-test above, but now have a second factor of \\(popDummy\\) to try to make a more specific model.\n\nglm_anova_model <- summary(lm(lifeExp ~ popDummy * contDummy, data = anova_df))\nglm_anova_model\n\n\nCall:\nlm(formula = lifeExp ~ popDummy * contDummy, data = anova_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1815  -2.3171   0.4615   2.6663   6.6845 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         75.6598     0.4901 154.383  < 2e-16 ***\npopDummy             0.6759     0.4901   1.379  0.17388    \ncontDummy            1.9888     0.4901   4.058  0.00017 ***\npopDummy:contDummy  -0.8977     0.4901  -1.832  0.07282 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.618 on 51 degrees of freedom\nMultiple R-squared:  0.2999,    Adjusted R-squared:  0.2587 \nF-statistic: 7.281 on 3 and 51 DF,  p-value: 0.0003721\n\n\nLet’s see if this model has less error (i.e. a smaller sum of squares than the t-test; 730.8276):\n\nsum(glm_anova_model$residuals^2)\n\n[1] 667.5443\n\n\nYep, so this model is giving us a bit more insight as there is less error. Let’s visualise how much error there is:\n\nglm_anova_model$coefficients[1,1]\n\n[1] 75.65983\n\nglm_anova_model$coefficients[2]\n\n[1] 0.6758679\n\n# gapminder_americas_europe$t_fit = continent_ttest$coefficients[1] + # intercept\n#   continent_ttest$coefficients[2]                       * # gradient\n#   gapminder_americas_europe$contDummy\n# \n# \n# ggplot(gapminder_americas_europe, aes(x = contDummy, y = lifeExp)) +\n#   geom_segment(\n#     position = \"jitter\",\n#     arrow = arrow(length = unit(0.01, \"npc\"),ends = \"first\"),\n#     aes(\n#       xend = contDummy,\n#       yend = t_fit,\n#       color = \"resid\"\n#     )\n#   ) + \n#   geom_segment(aes(\n#     x = -1.9, \n#     xend = -.1, \n#     y = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n#     yend = -1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),\n#     color = \"blue\"\n#   ) + \n#   geom_segment(aes(\n#     x = 0.1, \n#     xend = 1.9, \n#     y = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1],\n#     yend = 1 * continent_ttest$coefficients[2] + continent_ttest$coefficients[1]),\n#     color = \"blue\"\n#   )\n\nLet’s compare the output of this lm function to an ANOVA function in R to confirm that these are the same thing (i.e. that ANOVA is a general linear model):\n\n# car::Anova(aov(lifeExp ~ popCategorical + continent + popCategorical:continent, data = anova_df), type = \"3\")\n# temp_model <- lm(lifeExp ~ popCategorical + continent + popCategorical:continent, data = anova_df)\n# summary(temp_model)\n# car::Anova(lm(lifeExp ~ popCategorical + continent + popCategorical:continent, data = anova_df), type = \"3\")\n\nYou’ll see that the 2 x 2 interaction p-values are identical between analyses, but that the individual factors are similar but not identical."
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#regression",
    "href": "GeneralLinearModels/generalLinearModels.html#regression",
    "title": "General Linear Models and Sum of Squares",
    "section": "Regression",
    "text": "Regression\n\n\n\n\n\nNow lets see how this looks for the above analyses:"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#simple-regressions-and-t-tests",
    "href": "GeneralLinearModels/generalLinearModels.html#simple-regressions-and-t-tests",
    "title": "General Linear Models and Sum of Squares",
    "section": "Simple Regressions and t-tests",
    "text": "Simple Regressions and t-tests\nAs described in more detail in the simple regression section, the simplest general linear model could be formulated as:\n\\[\nY = a + bX + e\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "href": "GeneralLinearModels/rVsAdjustedRSquared.html",
    "title": "R-squared vs. Adjusted R-squared",
    "section": "",
    "text": "When completing a regression, there’s always a risk of “overfitting” the data, i.e. creating a model that includes predictors that have no meaningful association with the outcome variable. One reason that overfitting the data is a problem is that it is almost impossible for a predictor to have no association with an outcome variable. For that to happen you would need any data points that suggested a positive association between the outcome and the predictor to be equally balanced out by data points that suggest a negative association:\n\nlibrary(ggplot2)\nno_association_df <- data.frame(\n  predictor = c(1,1,1,2,2,2,3,3,3),\n  outcome   = c(1,2,3,1,2,3,1,2,3)\n)\n\nggplot(no_association_df, aes(x = predictor, y = outcome)) + geom_point() + geom_smooth(method=lm, formula = 'y ~ x')\n\n\n\n\nFig. X. An example of how unrealistically balanced your data needs to be to find no association. As this (almost) never happens in reality, samples are biased towards finding associations between predictor and outcome variables even when there aren’t any in the population. For example, let’s generate some random data, and see what R-Values we find. Remember, random data really shouldn’t have any association between predictor and outcome variables.\n\nrandom_df = data.frame(\n  random_iv_1 = runif(100),\n  random_iv_2 = runif(100),\n  random_iv_3 = runif(100),\n  random_dv = runif(100)\n)\nrmarkdown::paged_table(random_df)\n\n\n\n  \n\n\n\n\nrandom_lm <- lm(random_dv ~ random_iv_1, random_df)\nrandom_summary <- summary(random_lm)\nrandom_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1, data = random_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45534 -0.18720 -0.01486  0.19149  0.55019 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.53255    0.04676  11.390   <2e-16 ***\nrandom_iv_1 -0.15960    0.08336  -1.915   0.0585 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2562 on 98 degrees of freedom\nMultiple R-squared:  0.03606,   Adjusted R-squared:  0.02622 \nF-statistic: 3.666 on 1 and 98 DF,  p-value: 0.05846\n\n\nNote that the above output is generated each time this page is rendered (generated), and so by chance may happen to look like the random predictor is significant. If so, there’s a 95% chance that this predictor will not be significant next time the page is rendered.\nLooking at the output above, we can see that 3.6056% of the variance of random_dv was explained by random_iv_1 before correction. Considering that these were randomly generated numbers, that’s 3.6056% too much. However, the Adjusted R-squared is only 0.0262. Note that Adjusted R-squared can be a negative number, and a negative number suggests that based on the sample, the predictor(s) has(/have) no association with the outcome variable in the population.\nA formula for the adjusted r-squared is:\n\\[\n\\bar{R^2} = 1-\\frac{SS_{res}/df_{res}}{SS_{tot}/df_{tot}}\n\\] \\(\\bar{R^2}\\) is the Adjusted R-Squared \\(SS_{total}\\) is the Sum of Squares of the total (i.e. how much total variance there is around the mean to explain) \\(SS_{res}\\) is the Sum of Squares of the residuals (i.e. how much isn’t explained by the model) \\(df_{total}\\) is the Degrees of Freedom of the total. This is the number of data points - 1, so is N - 1 \\(df_{res}\\) is the Degrees of Freedom of the residuals. The degrees of freedom for the residuals takes into account the number of data points and the number of predictors, and so is N - 1 - 1\nLet’s use the above formula to manually calculate the Adjusted R Squared\n\nss_res <- sum(random_lm$residuals^2)\nss_total <- sum(\n  (\n    random_df$random_dv - mean(random_df$random_dv)\n  )^2\n)\n\n\nrandom_r_square = ss_total - ss_res\ndf_total <- length(random_lm$residuals) - 1\ndf_res <- length(random_lm$residuals) -\n  1 - # remove 1 from the number of data points\n  1 # remove another 1 to reflect there being 1 predictor\nadjusted_random_r_square = 1 - (ss_res/df_res)/(ss_total/df_total)\n\nadjusted_random_r_square\n\n[1] 0.02622009\n\n\nThe number above should match the Adjusted R-Squared from the multiple regression above. Let’s explore what happens when we have multiple predictors:\n\nrandom_lm_multiple <- lm(random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, random_df)\nrandom_multiple_summary <- summary(random_lm_multiple)\nrandom_multiple_summary\n\n\nCall:\nlm(formula = random_dv ~ random_iv_1 + random_iv_2 + random_iv_3, \n    data = random_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45803 -0.17642 -0.00832  0.17020  0.53051 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.54314    0.07902   6.873  6.3e-10 ***\nrandom_iv_1 -0.16095    0.08381  -1.920   0.0578 .  \nrandom_iv_2  0.05953    0.09282   0.641   0.5228    \nrandom_iv_3 -0.07405    0.08566  -0.865   0.3894    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2573 on 96 degrees of freedom\nMultiple R-squared:  0.0473,    Adjusted R-squared:  0.01753 \nF-statistic: 1.589 on 3 and 96 DF,  p-value: 0.1971\n\n\nTwo things to look for from the above: - The model with 3 predictors has higher (Multiple) R-Squared than the model with only 1 predictor. This reflects problems with over-fitting the model: the more predictors you include in your sample, the more variance in the outcome that will be explained by the predictors, even if those associations between the predictors are arbitrary (i.e. don’t reflect anything about the general population). - Adjusted R-squared values are less susceptible to this bias of overfitting the data (but is not completely invulnerable to it). All statistical tests are vulnerable to false positives and including Adjusted R-squared values.\nRemember, the adjusted r-square is necessary for us to make claims about the general population. If we just wanted to make a claim about our sample, we would just use the r-squared, as we don’t need to correct our estimate."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html",
    "href": "GeneralLinearModels/TTests.html",
    "title": "T-Tests(incomplete)",
    "section": "",
    "text": "In all general linear models you are trying to compare how much of the variance is explained by a model compared to what’s not being explained by a model. In short\n\\[\n\\frac{var_{explained}}{var_{unexplained}} = \\frac{SS_{explained}}{SS_{unexplained}}\n\\]\nFor each type of t-test, the way we calculate this is slightly different:"
  },
  {
    "objectID": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "href": "GeneralLinearModels/generalLinearModels.html#dummy-vs.-effect-coding-for-categorical-variables-in-a-model",
    "title": "General Linear Models and Sum of Squares",
    "section": "Dummy vs. effect coding for categorical variables in a model",
    "text": "Dummy vs. effect coding for categorical variables in a model\nGeneral Linear Models need numerical values for the predictors. As categorical variables (e.g. Sex) don’t have a numeric value by default, we have to substitute the categories with numbers:\n\nEffect coding can be used when you have a binary categorical variable, and you allocate one level 1 and the other -1. For example, you could allocate all females the score 1, and all non-female participants -1. A disadvantage of this approach is that it works best when you have binary categorical variable, but doesn’t work as well when you have 3 or more levels. For example, coding female, male and non-binary sex doesn’t work well with effect coding.\nDummy coding involves allocating a 1 if someone is in a cateogory, and 0 if they are outside of the category. For example, you could allocate 1 to all your female participants, and 0 to all participants who aren’t female to a variable “sex_female”. An advantage of this approach is that you have flexibility to have more than 2 levels, such as having “sex_female”, “sex_male” and “sex_nonbinary” as variables that are all either 1 or 0."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "href": "GeneralLinearModels/TTests.html#one-sample-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "One-Sample t-tests",
    "text": "One-Sample t-tests\n\nGLM approach\nOne sample t-tests try to explain whether variance of data is better explained around one specific value (sample mean) compared to another (previously assumed value). For example, imagine that you wanted to test whether life expectancy is higher than 55 across the world:\n\nYour \\(\\mu\\) would be 55. This can be thought of as the assumed population mean that we want to use our sample to test.\nYour \\(\\bar{x}\\) would be the sample mean.\n\nLet’s visualise these values using gapminder data from 2007:\n\nlibrary(ggplot2)\nlibrary(gapminder)\ngapminder_2007 <- subset(\n  gapminder,   # the data set\n  year == 2007\n)\nggplot(gapminder_2007, aes(x=year,y=lifeExp)) + \n  geom_jitter() + \n  xlab(\"\") + \n  theme(axis.text.x = element_blank()) +\n  theme(axis.ticks.x = element_blank()) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = 55,\n      yend = 55,\n      color = \"Mu\"\n    )\n  ) +\n  geom_segment(\n    aes(\n      x = 2006.6,\n      xend = 2007.4,\n      y = mean(lifeExp),\n      yend = mean(lifeExp),\n      color = \"Sample Mean\"\n    )\n  )\n\n\n\n\nWe want to create a model that explains any variance around the population mean (\\(\\mu\\)). The sample mean could be modeled as such:\n\\[\ny = \\bar{y} + e\n\\]\n\nY is the data point value you are trying to predict. Note that for this formula you will always have the same prediction.\n\\(\\bar{y}\\) is mean of y. You are only interested in whether predicting y based on y’s mean captures a significant amount of the variance of the y-values around the \\(\\mu\\).\n\\(e\\) is the error, i.e. the residuals that the module do not predict effectively.\n\nIf the sample mean is a useful model, then it will explain a large proportion of the variance around the “population” mean (and will also suggested that there is significant reason to reject the population mean). The total variance using sum of squares is thus:\n\\[\nSS_{total} = \\sum(x_i-\\mu)^2\n\\]\nWhich for the above data would give us:\n\nsum((gapminder_2007$lifeExp - 55)^2)\n\n[1] 41025.16\n\n\nSo your explained variance by this model is any difference between the Mu (\\(\\mu\\)) and the sample mean (\\(\\bar{x}\\)). To summarise this using sum of squares, for each data point you subtract the two from each other and square them, as this difference is what we can explain of variance away from the MU:\n\\[\nSS_{explained} = N * (\\mu - \\bar{x})^2\n\\]\nWhich for the above data would give us:\n\nlength(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2\n\n[1] 20473.3\n\n\nUnexplained variance would be the residuals around the sample mean, as this is variance that is not explained by the model. Conveniently, we can calculate the sum of squared around the sample mean quite elegantly:\n\\[\nSS_{unexplained} = \\sum(x_i-\\bar{x})^2\n\\]\nWhich for the above data would give us\n\nsum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2)\n\n[1] 20551.85\n\n\nSo the F-value should be:\n\\[\nF = \\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}} = \\frac{20473.3/(Predictors)}{20551.85/(N-1)} = \\frac{20473.3/1}{20551.85/141}\n\\]\n\nf_value = (length(gapminder_2007$lifeExp) * (55- mean(gapminder_2007$lifeExp))^2) / (\n  (sum((gapminder_2007$lifeExp - mean(gapminder_2007$lifeExp))^2))/(length(gapminder_2007$lifeExp)-1)\n  \n)\nf_value\n\n[1] 140.4611\n\n\nF-values are squares of t-values, so let’s see if this is true here also:\n\nsqrt(f_value)\n\n[1] 11.85163\n\nt.test(gapminder_2007$lifeExp, mu=55)\n\n\n    One Sample t-test\n\ndata:  gapminder_2007$lifeExp\nt = 11.852, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 65.00450 69.01034\nsample estimates:\nmean of x \n 67.00742 \n\n\nGreat. So now that we’ve highlighted the GLM approach works for t-tests, can we see how our formula for a GLM simplifies to the formula we usually use for one-sample t-tests:\n\\[\nT = \\sqrt{F} = \\sqrt{\\frac{SS_{explained}/df_{explained}}{SS_{unexplained}/df_{unexplained}}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(levelsOfPredictors - 1)}{\\sum(x_i-\\bar{x})^2/(N-1)}} = \\sqrt{\\frac{N * (\\mu - \\bar{x})^2/(2-1)}{\\sigma^2}} = \\frac{\\sqrt{N * (\\mu - \\bar{x})^2}}{\\sqrt{\\sigma^2}} = \\frac{\\sqrt{(\\mu - \\bar{x})^2}}{\\sigma/\\sqrt{N}} = \\frac{\\mu - \\bar{x}}{\\sigma/\\sqrt{N}}\n\\] where:\n\nT is the t-value\nF is the f-value\n\\(SS_{explained}\\) is the sum of squares of the data explained by the model\n\\(SS_{unexplained}\\) is the sum of squares of the data not explained by the model (i.e. the residuals)\n\\(df_{explained}\\) is the degrees of freedom for the model. As there is only one predictor (the sample mean) and it’s only got 2 levels (1 or 0, however, in all cases the model is comparing the data to the mean, so it’s less intuitive that there are 2 levels)."
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#paired-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Paired samples t-tests",
    "text": "Paired samples t-tests\nPaired samples t-tests can be approached like 1-sample t-tests, but you first of all need to collapse the data to have a single variable to compare to a \\(\\mu\\) of zero. Let’s do this for gapminder data, comparing life expectancies between 2002 and 2007:\n\ngapminder_2002_2007_life_exp <- gapminder$lifeExp[gapminder$year == 2007] - gapminder$lifeExp[gapminder$year == 2002]\nt.test(gapminder_2002_2007_life_exp, mu = 0)\n\n\n    One Sample t-test\n\ndata:  gapminder_2002_2007_life_exp\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean of x \n   1.3125 \n\n\nThe above suggests that life expectancy was significanctly different. Let’s see if we get the exact same value when we use a paired t-test in R:\n\nt.test(gapminder$lifeExp[gapminder$year == 2007],gapminder$lifeExp[gapminder$year == 2002], paired=T)\n\n\n    Paired t-test\n\ndata:  gapminder$lifeExp[gapminder$year == 2007] and gapminder$lifeExp[gapminder$year == 2002]\nt = 14.665, df = 141, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.135561 1.489439\nsample estimates:\nmean difference \n         1.3125 \n\n\nLooks identical. Let’s compare formulas to see why this is:\n\\[\nt_{paired} = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sigma_{pooled}/\\sqrt{N}} = \\frac{\\bar{x_3}}{\\sigma_{pooled}/\\sqrt{N}}\n\\]\nWhere\n\n\\(\\bar{x_1}\\) is the mean of condition 1\n\\(\\bar{x_2}\\) is the mean of condition 2\n\\(\\bar{x_3}\\) is the mean of the result you get when you subtract condition 2 from condition 1 for each participant, i.e. \\(mean(x_1-x_2)\\).\n\\[\n\\sigma_{pooled}  = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}} OR \\frac{\\sum(x_1 - x_2)^2}{N-1}\n\\] One way effectively gets the average of the standard deviations of condition and 1. The second way gets the standard deviation of the differences between conditions 1 and 2. Both give you the same outcome.\n\\(N\\) is the number of participants\n\nYou can rewrite the above formula to compare \\(\\bar{x_3}\\) to \\(\\mu\\), as we know \\(\\mu\\) is zero, which would make this formula (effectively) identical to the one above for one-sample t-tests:\n\\[\n\\frac{\\bar{x_3} - \\mu}{\\sigma_{pooled}/\\sqrt{N}}\n\\]"
  },
  {
    "objectID": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "href": "GeneralLinearModels/TTests.html#independent-samples-t-tests",
    "title": "T-Tests(incomplete)",
    "section": "Independent Samples t-tests",
    "text": "Independent Samples t-tests\n\nGLM approach\nFor an independent samples t-test we can create a simple model based on the means of the two groups. You can either dummy or effect code the groups, so we’ll do both to look at how the output is slightly different each way. We’ll use the gapminder data to see if there are differences in life expectancies between the Americas and Europe in 2007 to illustrate these:\n\ngapminder_2007_Am_Eu <- subset(\n  gapminder,   # the data set\n  year == 2007 & continent == \"Americas\" | \n  year == 2007 & continent == \"Europe\"\n)\n\n\nDummy coding\nOne way to make a model for a t-test is to have a variable that is 1 for one level, and 0 for the other level (note that this gets more complicated if you are going an ANOVA with 3 or more levels). Let’s create a new variable for continent that is 1 if the country is in the Americas, and 0 if it’s not:\n\ngapminder_2007_Am_Eu$americas_dummy = ifelse(gapminder_2007_Am_Eu$continent == \"Americas\", 1,0)\nrmarkdown::paged_table(gapminder_2007_Am_Eu)\n\n\n\n  \n\n\n\nNow that we have added our dummy code, we can write a model for what we expect life expectancy to be for each country:\n\\[\nlifeExp = \\beta_{americas}*mean(lifeExp_{americas}) + \\beta_{europe}*mean(lifeExp_{europe}) + e\n\\]\n\n\nEffect coding"
  },
  {
    "objectID": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "href": "statsBasics/statsBasics.html#what-is-power-and-what-is-the-beta-value",
    "title": "Statistics Basics (incomplete)",
    "section": "What is power (and what is the beta value)",
    "text": "What is power (and what is the beta value)"
  },
  {
    "objectID": "jast_setup.html",
    "href": "jast_setup.html",
    "title": "Just Another Statistics Textbook",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  },
  {
    "objectID": "advancedR/fancyFigures.html",
    "href": "advancedR/fancyFigures.html",
    "title": "Fancy Figures",
    "section": "",
    "text": "# add google fonts\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\n\n\n# also helps ggplot for macos computers with the problem \"no font could be found for ...\"\nfont_add(\"Arial\", \"/Library/Fonts/Arial.ttf\")  # Use the actual file path\n\nshowtext_auto()"
  }
]