---
title: "Probability (R)"
editor: visual
---

This concept is somewhat foundational to almost all the other content in this website, but is often overlooked in textbooks. An advantage about understanding it is that will give you a more complete understanding later when we go through concepts such as **probability distributions** such as **binomial** and **normal distributions.**

## **Probability of single outcomes**

In informal language we might talk about there being a 50% chance of something happening, such as a 50% chance of getting "heads" when flipping a coin. At the risk of telling you what you already know, if there's a 50% chance of getting "heads", then we assume there is a 50% chance of getting "tails", because the likelihood of all outcomes put together must equal 100% or else it suggests there's another outcome you haven't considered. For example, if a flipped coin lands on heads 45% of the time, and on tails 45% of the time, then (45% + 45% =) 90% of the time it will land on heads or tails, which means 10% of the time it will do one or more other things (e.g. land on neither side). **Going forward we will assume we're using coins that can only land on heads or tails.**

Within statistics we tend to use decimalised numbers rather than percentages, so a 10% chance is written as .1, a 50% chance is written as .5 and 100% chance is written as 1. **This is important as when you have considered all possible outcomes you should be able to add their likelihoods up to make 1.**

## **Probability of combinations of outcomes**

Imagine that you want to predict the likelihood of flipping a coin on heads or tails a certain number of times in a row. The outcome of each coin flip is **binary**, i.e. there is only 1 possible outcome out of 2 options. If this isn't a biased coin, we can calculate some basic expectations about what will happen after 2 flips of the coin:

-   Each flip of the coin has a 0.5 chance of landing on Heads.

-   If we wanted to calculate the likelihood of 2 flips of heads, there's a 0.5 chance we get the first heads flip, and then another 0.5 chance we'll get the second head flip. To summarise this, we can multiply both 0.5 chances together to get 0.25.

-   To summarise the likelihood of all combinations we could make the following table:

|                               |                                              |                           |
|----------------------|-------------------------------|-------------------|
| **First flip and likelihood** | **Second flip and likelihood**               | **Overall likelihood**    |
| Heads (.5)                    | Heads (.5)                                   | 0.5 \* 0.5 = .25          |
| Heads (.5)                    | Tails (.5)                                   | 0.5 \* 0.5 = .25          |
| Tails (.5)                    | Heads (.5)                                   | 0.5 \* 0.5 = .25          |
| Tails (.5)                    | Tails (.5)                                   | 0.5 \* 0.5 = .25          |
|                               | **Likelihood of any of the above happening** | .25 + .25 + .25 + .25 = 1 |

To achieve what you have above, you need to consider every combination of possible outcomes and calculate their likelihood. A useful quality check is to make sure that when you add the overall likelihoods together you get 1, otherwise...

-   If you have less than 1 it suggests you have overlooked an outcome

-   If you have more than 1 it suggests you have either

    -   Overestimated the likelihood of a specific outcome

    -   Treated overlapping outcomes as if they are **mutually exclusive**

### Mutually exclusive outcomes

In the above example each of the four combinations are mutually exclusive, as they are specific and distinct. It is impossible that your flip of your coins was both:

-   heads and then tails

-   heads and then heads

Even though the first flip is heads in both outcomes, the second flip is distinct and so the two flips could not be both outcomes as they are exclusive.

However, there are some scenarios in which it's less clear whether two outcomes are mutually exclusive. The chance of rolling any side of a die is 1 in 6. The chance of rolling a certain number and above can be calculated by counting the number of **valid** sides there are and then dividing by 6. For example, the chance of rolling 3 and above is 4 in 6 because there are 4 valid sides (3,4,5 and 6).

Imagine that you are rolling dice twice in a row - what is the likelihood that one role will be at least 3 and the other will be at least 4? Let's start with **an incorrect** answer:

```{r}
dice_outcomes <- data.frame(
  Description <- c(
    "First is 3 or more, second is 4 or more", # valid outcome
    "First is 4 or more, second is 3 or more", # valid outcome
    "First is 3 or more, second is 3 or LESS", # invalid outcome
    "First is 4 or more, second is 2 or LESS", # invalid outcome
    "First is 3 or LESS, second is 4 or more", # invalid outcome
    "First is 2 or LESS, second is 3 or more", # invalid outcome
    "First is 2 or LESS, second is 2 or LESS"  # invalid outcome
  ),
  likelihood = c(
    (4/6) * (3/6), # "First is 3 or more, second is 4 or more", # valid outcome
    (3/6) * (4/6), # "First is 4 or more, second is 3 or more", # valid outcome
    (3/6) * (3/6), # "First is 3 or more, second is 3 or LESS", # invalid outcome
    (3/6) * (2/6), # "First is 4 or more, second is 2 or LESS", # invalid outcome
    (3/6) * (3/6), # "First is 3 or LESS, second is 4 or more", # invalid outcome
    (2/6) * (4/6), # "First is 2 or LESS, second is 3 or more", # invalid outcome
    (2/6) * (2/6)  # "First is 2 or LESS, second is 2 or LESS"  # invalid outcome
  )
)
knitr::kable(dice_outcomes)
sum(dice_outcomes$likelihood)
```

When we add all the outcome likelihoods together we get more than 1, which makes me (and maybe you) sad. This reflects us (or me, you didn't ask me to make the above table) treating some outcomes as if they are exclusive when they are not. For example, the two **valid** outcomes have overlap as they both include the likelihoods that both dice are of 4 or more. This has inflated the likelihood of the **valid outcomes** when you add these outcomes together, as you have doubled up on the overlap. To visualise this:

```{r}
#| label: fig-prob-dice-outcome-likelihoods
#| fig-cap: a summary of the overlap between 2 outcomes. The likelihood of any particular dice roll is 1 out of 36 (see how there are 36 combinations in the above grid), and so you could calculate the likelihood of one dice being at least 3 and the other being at least 4 by making a grid like above and then calculating what **proportion of the area** is covered by valid rolls. If half of your rolls are valid, then half of the area would be covered. In our case, 15 out of 36 possible combinations is valid, and so 15/36 of the area in the above grid is your likelihood. **However**, this figure also visualises how mathematical shortcuts described above of the likelihood of getting an outcome can be incorrect if you make false assumptions about whether outcomes are mutually exclusive. As we can see above, the likelihood of dice 1 being 3 or above and dice 2 being 4 or above (blue) is NOT exclusive from the likelihood of dice 1 being 3 and above and dice being 3 and above (red), which is why the purple squares reflect the overlap between these outcomes. 

library(ggplot2)

ggplot() +
  geom_rect(
    mapping = aes(
      xmin = 2.5,
      xmax = 6.5,
      ymin = 3.5,
      ymax = 6.5
    ),
    fill = "blue",
    alpha = .5
  ) +
  geom_rect(
    mapping = aes(
      xmin = 3.5,
      xmax = 6.5,
      ymin = 2.5,
      ymax = 6.5
    ),
    fill = "red",
    alpha = .5
  ) +
  theme_bw() +
  theme(
    plot.background = element_blank(),
    #panel.grid.minor = element_line(color = "black", linewidth =  2),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  ) + 
  coord_fixed() +
  xlab("First roll") +
  ylab("Second roll") + 
  geom_vline(
    xintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),
    color="white"
  ) + 
  geom_hline(
    yintercept =c(0.5,1.5,2.5,3.5,4.5, 5.5),
    color="white"
  ) +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) +
  scale_y_continuous(breaks = seq(0, 6, by = 1)) 


```

In the above visualisation of all possible dice role combinations, the proportion of the area covered reflects the likelihood of each valid combination highlighted: "First is 3 or more, second is 4 or more" in blue and "First is 4 or more, second is 3 or more" in red. As you can see, there is a lot of overlap (purple/pink), reflecting that these were not mutually exclusive possibilities, and so adding them together will inflate the estimated likelihood of either of them happening.

An elegant solution is to subtract the likelihood associated with repetition from your original calculation:

```{r}
(4/6) * (3/6) + # "First is 3 or more, second is 4 or more", # valid outcome
(3/6) * (4/6) - # "First is 4 or more, second is 3 or more", # valid outcome
(3/6) * 3/6     # overlap between the two valid outcomes above 
```

To confirm this is correct, we can use R to count how many times out of 36 the dice will be valid:

```{r}
valid_dice <- matrix(FALSE,nrow = 6,ncol = 6)
for(i in 1:6){
  for(j in 1:6){
    if(i >= 3 & j >= 4){
      valid_dice[i,j] = TRUE
    }
    if(i >= 4 & j >= 3){
      valid_dice[i,j] = TRUE
    }
  }
}
sum(valid_dice)/ # number of valid roll combinations
36               # total number of roll combinations

```

We get the same output.

## The likelihood of something **NOT** happening is often a useful shortcut

As shown above, calculating of the probabilities can be done in more and less reliable and elegant ways. Generally speaking, an elegant calculation is less likely to result in error. One common example of how to be more elegant is how we address the chance of avoiding an undesirable outcome, such as a [**false-positive**](../statsBasics/statsBasics.html). If we ran three tests on random data, and accepted an $\alpha$ threshold of .05 (i.e. a 5% chance that we would incorrectly identify an effect), then there are (at least) 2 ways you could calculate the likelihood of at least 1 false positive. The slower way:

$$
fp_1 + fp_2 + fp_3  + fp_{1,2} + fp_{1,3} + fp_{2,3} + fp_{1,2,3}
$$

-   $fp_1$ refers to a false positive **only** for test 1

-   $fp_2$ refers to a false positive **only** for test 2

-   $fp_3$ refers to a false positive **only** for test 3

-   $fp_{1,2}$ refers to a false positive **only** for tests 1 and 2

-   $fp_{1,3}$ refers to a false positive **only** for tests 1 and 3

-   $fp_{2,3}$ refers to a false positive **only** for tests 2 and 3

-   $fp_{1,2,3}$ refers to a false positive for all three tests

```{r}
at_least_1_fp = 
.05 * .95 * .95 + # false positive only for test 1
.95 * .05 * .95 + # false positive only for test 2
.95 * .95 * .05 + # false positive only for test 3
.05 * .05 * .95 + # false positive only for test 1 and 2
.05 * .95 * .05 + # false positive only for test 1 and 3
.95 * .05 * .05 + # false positive only for test 2 and 3
.05 * .05 * .05   # false positive only for test all tests
at_least_1_fp
```

To check this is correct, when we add the above number to the likelihood of 0 false-positives we should get 1:

```{r}
.95 * .95 * .95 + # zero false positives
  at_least_1_fp
```

But, we could simply subtract the likelihood of zero false positives from 1:

```{r}
1 - (.95 * .95 * .95)
```

and get the same result.

::: callout-note
## Inconsistent p-value thresholds

Note that it's easier to do simple calculations like above if the p-value thresholds are consistent (e.g. .05 in this case), whereas inconsistent p-value thresholds between outcomes can lead to false assumptions about **mutual exclusivity** as described above.
:::

{{< include probabilityQuestions.qmd >}}
